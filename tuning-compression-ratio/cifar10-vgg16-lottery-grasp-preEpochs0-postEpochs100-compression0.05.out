Namespace(dataset='cifar10', model='vgg16', model_class='lottery', dense_classifier=False, pretrained=False, optimizer='adam', train_batch_size=256, test_batch_size=256, pre_epochs=0, post_epochs=100, lr=0.001, lr_drops=[], lr_drop_rate=0.1, weight_decay=0.0, pruner='grasp', compression=0.05, quantization=False, prune_epochs=1, compression_schedule='exponential', mask_scope='global', prune_dataset_ratio=10, prune_batch_size=256, prune_bias=False, prune_batchnorm=False, prune_residual=False, prune_train_mode=False, reinitialize=False, shuffle=False, invert=False, pruner_list=[], prune_epoch_list=[], compression_list=[], level_list=[], experiment='singleshot', expid='cifar10-vgg16-lottery-grasp-preEpochs0-postEpochs100-compression0.05', result_dir='Results/data', gpu=0, workers=4, no_cuda=False, seed=1, verbose=False)
Loading cifar10 dataset.
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with grasp for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:35<00:00, 35.65s/it]100%|██████████| 1/1 [00:35<00:00, 35.65s/it]
Post-Training for 100 epochs.
  0%|          | 0/100 [00:00<?, ?it/s]Time:  3.817999736405909
  1%|          | 1/100 [00:54<1:30:36, 54.92s/it]Time:  3.5781423170119524
  2%|▏         | 2/100 [01:50<1:30:42, 55.53s/it]Time:  3.844956492073834
  3%|▎         | 3/100 [02:47<1:30:21, 55.89s/it]Time:  3.931658868677914
  4%|▍         | 4/100 [03:42<1:29:22, 55.85s/it]Time:  3.9426507158204913
  5%|▌         | 5/100 [04:38<1:28:12, 55.71s/it]Time:  4.43818065430969
  6%|▌         | 6/100 [05:34<1:27:12, 55.66s/it]Time:  4.259239874780178
  7%|▋         | 7/100 [06:29<1:26:07, 55.57s/it]Time:  4.260436162352562
  8%|▊         | 8/100 [07:24<1:24:49, 55.32s/it]Time:  4.654330179095268
  9%|▉         | 9/100 [08:19<1:23:47, 55.25s/it]Time:  4.429064110852778
 10%|█         | 10/100 [09:14<1:22:40, 55.12s/it]Time:  4.285586750134826
 11%|█         | 11/100 [10:09<1:21:43, 55.09s/it]Time:  4.712289379909635
 12%|█▏        | 12/100 [11:04<1:21:01, 55.25s/it]Time:  4.437905321829021
 13%|█▎        | 13/100 [11:59<1:20:03, 55.22s/it]Time:  4.4012431493029
 14%|█▍        | 14/100 [12:53<1:18:33, 54.81s/it]Time:  4.469382248818874
 15%|█▌        | 15/100 [13:48<1:17:36, 54.79s/it]Time:  4.358681286685169
 16%|█▌        | 16/100 [14:43<1:16:46, 54.85s/it]Time:  4.389631487429142
 17%|█▋        | 17/100 [15:38<1:15:56, 54.90s/it]Time:  4.2590118162333965
 18%|█▊        | 18/100 [16:33<1:15:14, 55.05s/it]Time:  3.265989052131772
 19%|█▉        | 19/100 [17:29<1:14:27, 55.16s/it]Time:  3.719934237189591
 20%|██        | 20/100 [18:25<1:14:05, 55.57s/it]Time:  3.968093647621572
 21%|██        | 21/100 [19:20<1:12:53, 55.37s/it]Time:  4.018000637181103
 22%|██▏       | 22/100 [20:15<1:11:49, 55.25s/it]Time:  4.148675073869526
 23%|██▎       | 23/100 [21:11<1:11:07, 55.42s/it]Time:  3.911926133558154
 24%|██▍       | 24/100 [22:06<1:10:05, 55.34s/it]Time:  4.231389427557588
 25%|██▌       | 25/100 [23:03<1:09:34, 55.66s/it]Time:  3.861084006726742
 26%|██▌       | 26/100 [23:58<1:08:44, 55.74s/it]Time:  3.373353410512209
 27%|██▋       | 27/100 [24:53<1:07:28, 55.46s/it]Time:  4.089747856371105
 28%|██▊       | 28/100 [25:48<1:06:11, 55.17s/it]Time:  4.57936305552721
 29%|██▉       | 29/100 [26:41<1:04:42, 54.69s/it]Time:  4.711760469712317
 30%|███       | 30/100 [27:37<1:04:17, 55.11s/it]Time:  4.541779565624893
 31%|███       | 31/100 [28:33<1:03:40, 55.37s/it]Time:  4.49321630038321
 32%|███▏      | 32/100 [29:28<1:02:32, 55.19s/it]Time:  4.458153136074543
 33%|███▎      | 33/100 [30:24<1:01:40, 55.23s/it]Time:  4.5436610244214535
 34%|███▍      | 34/100 [31:18<1:00:37, 55.12s/it]Time:  4.627453595399857
 35%|███▌      | 35/100 [32:14<59:43, 55.14s/it]  Time:  4.520496767945588
 36%|███▌      | 36/100 [33:09<58:55, 55.24s/it]Time:  4.704786106944084
 37%|███▋      | 37/100 [34:05<58:04, 55.32s/it]Time:  4.50748660042882
 38%|███▊      | 38/100 [35:00<57:04, 55.23s/it]Time:  4.590756145305932
 39%|███▉      | 39/100 [35:54<56:01, 55.11s/it]Time:  4.528690142557025
 40%|████      | 40/100 [36:49<55:01, 55.02s/it]Time:  4.530194808728993
 41%|████      | 41/100 [37:45<54:13, 55.14s/it]Time:  4.499898951500654
 42%|████▏     | 42/100 [38:39<53:12, 55.05s/it]Time:  4.4870146084576845
 43%|████▎     | 43/100 [39:34<52:12, 54.95s/it]Time:  4.568650905042887
 44%|████▍     | 44/100 [40:29<51:15, 54.92s/it]Time:  4.400426932610571
 45%|████▌     | 45/100 [41:24<50:16, 54.84s/it]Time:  4.508788159117103
 46%|████▌     | 46/100 [42:19<49:21, 54.84s/it]Time:  4.315455485135317
 47%|████▋     | 47/100 [43:13<48:24, 54.80s/it]Time:  4.638661723583937
 48%|████▊     | 48/100 [44:08<47:36, 54.93s/it]Time:  4.837700386531651
 49%|████▉     | 49/100 [45:04<46:44, 54.98s/it]Time:  4.7501277811825275
 50%|█████     | 50/100 [45:58<45:47, 54.96s/it]Time:  4.777657633647323
 51%|█████     | 51/100 [46:53<44:49, 54.89s/it]Time:  4.971314661204815
 52%|█████▏    | 52/100 [47:48<43:59, 54.98s/it]Time:  4.811503174714744
 53%|█████▎    | 53/100 [48:43<43:02, 54.94s/it]Time:  4.9137493865564466
 54%|█████▍    | 54/100 [49:38<42:03, 54.85s/it]Time:  4.76515450142324
 55%|█████▌    | 55/100 [50:32<40:54, 54.54s/it]Time:  4.786659167148173
 56%|█████▌    | 56/100 [51:26<40:01, 54.59s/it]Time:  5.036170874722302
 57%|█████▋    | 57/100 [52:22<39:15, 54.77s/it]Time:  5.0082042729482055
 58%|█████▊    | 58/100 [53:17<38:24, 54.88s/it]Time:  5.028965633362532
 59%|█████▉    | 59/100 [54:12<37:32, 54.94s/it]Time:  4.9956264058128
 60%|██████    | 60/100 [55:07<36:37, 54.94s/it]Time:  5.048878281377256
 61%|██████    | 61/100 [56:01<35:35, 54.76s/it]Time:  5.015607089735568
 62%|██████▏   | 62/100 [56:56<34:39, 54.71s/it]Time:  5.063412673771381
 63%|██████▎   | 63/100 [57:50<33:44, 54.72s/it]Time:  5.0197687447071075
 64%|██████▍   | 64/100 [58:45<32:52, 54.81s/it]Time:  4.994751981459558
 65%|██████▌   | 65/100 [59:40<31:58, 54.81s/it]Time:  4.988556672818959
 66%|██████▌   | 66/100 [1:00:35<31:03, 54.81s/it]Time:  4.978635199368
 67%|██████▋   | 67/100 [1:01:29<30:04, 54.69s/it]Time:  5.027272129431367
 68%|██████▊   | 68/100 [1:02:24<29:08, 54.65s/it]Time:  5.098264171741903
 69%|██████▉   | 69/100 [1:03:19<28:15, 54.70s/it]Time:  5.185343011282384
 70%|███████   | 70/100 [1:04:14<27:24, 54.80s/it]Time:  5.076263129711151
 71%|███████   | 71/100 [1:05:09<26:27, 54.75s/it]Time:  4.874322037212551
 72%|███████▏  | 72/100 [1:06:03<25:33, 54.77s/it]Time:  4.876462302170694
 73%|███████▎  | 73/100 [1:06:58<24:39, 54.78s/it]Time:  4.795146131888032
 74%|███████▍  | 74/100 [1:07:53<23:41, 54.69s/it]Time:  4.836047064512968
 75%|███████▌  | 75/100 [1:08:48<22:49, 54.77s/it]Time:  4.732132997363806
 76%|███████▌  | 76/100 [1:09:42<21:55, 54.80s/it]Time:  4.665646151639521
 77%|███████▋  | 77/100 [1:10:37<21:01, 54.83s/it]Time:  4.815489135682583
 78%|███████▊  | 78/100 [1:11:33<20:09, 54.96s/it]Time:  4.664423746988177
 79%|███████▉  | 79/100 [1:12:28<19:14, 54.99s/it]Time:  4.847150353714824
 80%|████████  | 80/100 [1:13:23<18:20, 55.03s/it]Time:  4.780756575986743
 81%|████████  | 81/100 [1:14:18<17:26, 55.07s/it]Time:  4.81064491905272
 82%|████████▏ | 82/100 [1:15:13<16:32, 55.12s/it]Time:  4.837888254784048
 83%|████████▎ | 83/100 [1:16:08<15:35, 55.03s/it]Time:  4.790184307843447
 84%|████████▍ | 84/100 [1:17:03<14:41, 55.08s/it]Time:  4.740523107349873
 85%|████████▌ | 85/100 [1:17:58<13:45, 55.03s/it]Time:  4.854749771766365
 86%|████████▌ | 86/100 [1:18:53<12:50, 55.03s/it]Time:  4.748416917398572
 87%|████████▋ | 87/100 [1:19:48<11:54, 54.94s/it]Time:  4.7054248675704
 88%|████████▊ | 88/100 [1:20:43<11:00, 55.04s/it]Time:  4.666891159489751
 89%|████████▉ | 89/100 [1:21:38<10:04, 54.97s/it]Time:  4.781614957377315
 90%|█████████ | 90/100 [1:22:33<09:09, 55.00s/it]Time:  4.743350327946246
 91%|█████████ | 91/100 [1:23:28<08:15, 55.04s/it]Time:  4.713500436395407
 92%|█████████▏| 92/100 [1:24:23<07:20, 55.05s/it]Time:  4.736768753267825
 93%|█████████▎| 93/100 [1:25:18<06:25, 55.04s/it]Time:  4.772708153352141
 94%|█████████▍| 94/100 [1:26:13<05:29, 54.96s/it]Time:  4.708308510482311
 95%|█████████▌| 95/100 [1:27:08<04:34, 54.99s/it]Time:  4.695091663859785
 96%|█████████▌| 96/100 [1:28:03<03:39, 54.95s/it]Time:  4.661825552582741
 97%|█████████▋| 97/100 [1:28:58<02:44, 54.91s/it]Time:  4.738964919000864
 98%|█████████▊| 98/100 [1:29:53<01:49, 54.86s/it]Time:  3.8106889072805643
 99%|█████████▉| 99/100 [1:30:40<00:52, 52.67s/it]Time:  1.2588434247300029
100%|██████████| 100/100 [1:31:10<00:00, 45.72s/it]100%|██████████| 100/100 [1:31:10<00:00, 54.70s/it]
Post-training time: 5474.09 seconds
GPU memory allocated: 374.76 MB, peak: 999.48 MB
Train results:
                 train_loss     test_loss  top1_accuracy  top5_accuracy   test_time
Init.      0           NaN  2.417717e+00          11.73          50.17  126.390530
Pre-Prune  0           NaN  2.417717e+00          11.73          50.17  126.390530
Post-Prune 0           NaN  1.122388e+10          10.00          49.52    4.004864
Final      100    1.360485  1.409259e+00          50.00          92.48    1.258843
Prune results:
             module   param  sparsity     size             shape     flops    score mean  score variance  score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.578704     1728     (64, 3, 3, 3)   1769472  4.450377e-05    8.434830e-08   0.076903    1.499241e-04        6.385166e-08       0.259069      True
1    layers.0.conv    bias  1.000000       64             (64,)     65536  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
2    layers.1.conv  weight  0.630371    36864    (64, 64, 3, 3)  37748736  2.075448e-06    4.122782e-09   0.076509    2.485491e-05        3.509323e-09       0.916251      True
3    layers.1.conv    bias  1.000000       64             (64,)     65536  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
4    layers.3.conv  weight  0.699327    73728   (128, 64, 3, 3)  18874368  1.036265e-06    1.686676e-09   0.076402    1.571875e-05        1.440670e-09       1.158912      True
5    layers.3.conv    bias  1.000000      128            (128,)     32768  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
6    layers.4.conv  weight  0.777486   147456  (128, 128, 3, 3)  37748736  5.169904e-07    5.230480e-10   0.076233    7.786413e-06        4.626871e-10       1.148153      True
7    layers.4.conv    bias  1.000000      128            (128,)     32768  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
8    layers.6.conv  weight  0.806987   294912  (256, 128, 3, 3)  18874368  2.581017e-07    2.053401e-10   0.076117    4.942224e-06        1.809812e-10       1.457521      True
9    layers.6.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
10   layers.7.conv  weight  0.851437   589824  (256, 256, 3, 3)  37748736  1.255479e-07    5.263317e-11   0.074051    2.530799e-06        4.624397e-11       1.492726      True
11   layers.7.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
12   layers.8.conv  weight  0.844347   589824  (256, 256, 3, 3)  37748736  1.240258e-07    3.422636e-11   0.073153    2.217017e-06        2.932658e-11       1.307650      True
13   layers.8.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
14  layers.10.conv  weight  0.863741  1179648  (512, 256, 3, 3)  18874368  6.157174e-08    1.396250e-11   0.072633    1.547427e-06        1.157176e-11       1.825420      True
15  layers.10.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
16  layers.11.conv  weight  0.908128  2359296  (512, 512, 3, 3)  37748736  2.846881e-08    4.243501e-12   0.067166    8.836322e-07        3.463504e-12       2.084750      True
17  layers.11.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
18  layers.12.conv  weight  0.901108  2359296  (512, 512, 3, 3)  37748736  2.840790e-08    3.657214e-12   0.067023    8.970007e-07        2.853411e-12       2.116290      True
19  layers.12.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
20  layers.14.conv  weight  0.902061  2359296  (512, 512, 3, 3)   9437184  2.694333e-08    3.812698e-12   0.063567    8.725643e-07        3.052054e-12       2.058637      True
21  layers.14.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
22  layers.15.conv  weight  0.909092  2359296  (512, 512, 3, 3)   9437184  2.711094e-08    4.234986e-12   0.063963    8.358488e-07        3.537079e-12       1.972015      True
23  layers.15.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
24  layers.16.conv  weight  0.899855  2359296  (512, 512, 3, 3)   9437184  2.642919e-08    5.443435e-12   0.062354    9.434237e-07        4.554085e-12       2.225816      True
25  layers.16.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
26              fc  weight  0.610742     5120         (10, 512)      5120  1.443845e-05    5.200127e-09   0.073925    3.773054e-05        3.985002e-09       0.193180      True
27              fc    bias  1.000000       10             (10,)        10  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
Parameter Sparsity: 13119512/14719818 (0.8913)
FLOP Sparsity: 257076101/313478154 (0.8201)
Saving results.
