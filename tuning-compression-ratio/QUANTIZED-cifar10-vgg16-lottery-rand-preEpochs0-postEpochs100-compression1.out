Namespace(dataset='cifar10', model='vgg16', model_class='lottery', dense_classifier=False, pretrained=False, optimizer='adam', train_batch_size=256, test_batch_size=256, pre_epochs=0, post_epochs=100, lr=0.001, lr_drops=[], lr_drop_rate=0.1, weight_decay=0.0, pruner='rand', compression=1.0, quantization=True, prune_epochs=1, compression_schedule='exponential', mask_scope='global', prune_dataset_ratio=10, prune_batch_size=256, prune_bias=False, prune_batchnorm=False, prune_residual=False, prune_train_mode=False, reinitialize=False, shuffle=False, invert=False, pruner_list=[], prune_epoch_list=[], compression_list=[], level_list=[], experiment='singleshot', expid='QUANTIZED-cifar10-vgg16-lottery-rand-preEpochs0-postEpochs100-compression1', result_dir='Results/data', gpu=0, workers=4, no_cuda=False, seed=1, verbose=False)
Loading cifar10 dataset.
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with rand for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.74s/it]100%|██████████| 1/1 [00:02<00:00,  2.74s/it]
Post-Training for 100 epochs.
  0%|          | 0/100 [00:00<?, ?it/s]Time:  3.833677956950851
  1%|          | 1/100 [01:01<1:42:11, 61.94s/it]Time:  3.73980404692702
  2%|▏         | 2/100 [01:36<1:14:41, 45.73s/it]Time:  3.697984894970432
  3%|▎         | 3/100 [02:10<1:05:44, 40.66s/it]Time:  3.7694716239348054
  4%|▍         | 4/100 [02:44<1:00:51, 38.03s/it]Time:  3.8518891170388088
  5%|▌         | 5/100 [03:18<57:36, 36.38s/it]  Time:  4.100431593018584
  6%|▌         | 6/100 [03:51<55:29, 35.42s/it]Time:  4.079410684993491
  7%|▋         | 7/100 [04:25<53:52, 34.76s/it]Time:  4.166325042024255
  8%|▊         | 8/100 [04:58<52:25, 34.19s/it]Time:  4.282690817024559
  9%|▉         | 9/100 [05:30<50:56, 33.58s/it]Time:  4.49706217891071
 10%|█         | 10/100 [06:04<50:35, 33.72s/it]Time:  4.4362571380333975
 11%|█         | 11/100 [06:35<48:56, 33.00s/it]Time:  4.45683685305994
 12%|█▏        | 12/100 [07:08<48:14, 32.89s/it]Time:  4.516875314991921
 13%|█▎        | 13/100 [07:40<47:20, 32.65s/it]Time:  4.463100001914427
 14%|█▍        | 14/100 [08:12<46:37, 32.53s/it]Time:  4.473332101944834
 15%|█▌        | 15/100 [08:45<46:00, 32.47s/it]Time:  4.483092486043461
 16%|█▌        | 16/100 [09:16<44:59, 32.14s/it]Time:  4.4776661060750484
 17%|█▋        | 17/100 [09:48<44:30, 32.17s/it]Time:  4.463134494028054
 18%|█▊        | 18/100 [10:21<44:06, 32.27s/it]Time:  4.442413281998597
 19%|█▉        | 19/100 [10:53<43:33, 32.27s/it]Time:  4.445367280044593
 20%|██        | 20/100 [11:25<43:00, 32.25s/it]Time:  4.435247425921261
 21%|██        | 21/100 [11:57<42:19, 32.15s/it]Time:  4.453391712042503
 22%|██▏       | 22/100 [12:29<41:45, 32.12s/it]Time:  4.438469655928202
 23%|██▎       | 23/100 [13:01<41:09, 32.07s/it]Time:  4.469334376975894
 24%|██▍       | 24/100 [13:33<40:24, 31.91s/it]Time:  4.364670887007378
 25%|██▌       | 25/100 [14:04<39:36, 31.69s/it]Time:  4.397454733029008
 26%|██▌       | 26/100 [14:36<39:08, 31.74s/it]Time:  4.493335783015937
 27%|██▋       | 27/100 [15:09<39:04, 32.11s/it]Time:  4.0295045339735225
 28%|██▊       | 28/100 [15:40<38:18, 31.92s/it]Time:  4.681413358077407
 29%|██▉       | 29/100 [16:12<37:50, 31.97s/it]Time:  4.088473135023378
 30%|███       | 30/100 [16:44<37:16, 31.95s/it]Time:  3.2899802339961752
 31%|███       | 31/100 [17:15<36:22, 31.64s/it]Time:  3.863586775958538
 32%|███▏      | 32/100 [17:50<36:50, 32.50s/it]Time:  3.7434022730449215
 33%|███▎      | 33/100 [18:23<36:32, 32.72s/it]Time:  4.061181148979813
 34%|███▍      | 34/100 [19:01<37:40, 34.24s/it]Time:  4.130442455993034
 35%|███▌      | 35/100 [19:33<36:26, 33.64s/it]Time:  3.981244678958319
 36%|███▌      | 36/100 [20:10<36:57, 34.65s/it]Time:  4.201992850052193
 37%|███▋      | 37/100 [20:44<36:04, 34.36s/it]Time:  3.944578462978825
 38%|███▊      | 38/100 [21:18<35:27, 34.32s/it]Time:  3.124209885019809
 39%|███▉      | 39/100 [21:51<34:25, 33.86s/it]Time:  3.2021545940078795
 40%|████      | 40/100 [22:25<33:53, 33.89s/it]Time:  3.6681190910749137
 41%|████      | 41/100 [22:59<33:28, 34.05s/it]Time:  3.663321148022078
 42%|████▏     | 42/100 [23:32<32:32, 33.66s/it]Time:  3.852238841005601
 43%|████▎     | 43/100 [24:05<31:50, 33.52s/it]Time:  3.998007597052492
 44%|████▍     | 44/100 [24:39<31:17, 33.52s/it]Time:  3.8979730519931763
 45%|████▌     | 45/100 [25:11<30:33, 33.34s/it]Time:  4.147670642938465
 46%|████▌     | 46/100 [25:45<29:56, 33.26s/it]Time:  4.2126789659960195
 47%|████▋     | 47/100 [26:18<29:25, 33.32s/it]Time:  4.21008328103926
 48%|████▊     | 48/100 [26:51<28:48, 33.25s/it]Time:  4.01389724004548
 49%|████▉     | 49/100 [27:25<28:33, 33.59s/it]Time:  4.167264068964869
 50%|█████     | 50/100 [27:59<27:53, 33.47s/it]Time:  4.193081838078797
 51%|█████     | 51/100 [28:32<27:14, 33.35s/it]Time:  4.212541896966286
 52%|█████▏    | 52/100 [29:05<26:45, 33.45s/it]Time:  4.232373801060021
 53%|█████▎    | 53/100 [29:39<26:14, 33.50s/it]Time:  4.2225027199601755
 54%|█████▍    | 54/100 [30:12<25:35, 33.37s/it]Time:  4.260154435993172
 55%|█████▌    | 55/100 [30:45<24:59, 33.33s/it]Time:  4.253116903011687
 56%|█████▌    | 56/100 [31:19<24:32, 33.46s/it]Time:  4.166953923995607
 57%|█████▋    | 57/100 [31:52<23:53, 33.33s/it]Time:  4.095970485941507
 58%|█████▊    | 58/100 [32:25<23:15, 33.22s/it]Time:  4.125476491055451
 59%|█████▉    | 59/100 [32:58<22:38, 33.14s/it]Time:  4.09034810308367
 60%|██████    | 60/100 [33:31<22:04, 33.11s/it]Time:  4.191204895963892
 61%|██████    | 61/100 [34:04<21:33, 33.16s/it]Time:  4.309586353017949
 62%|██████▏   | 62/100 [34:37<20:58, 33.13s/it]Time:  4.245791397057474
 63%|██████▎   | 63/100 [35:11<20:27, 33.16s/it]Time:  4.1780596929602325
 64%|██████▍   | 64/100 [35:44<20:00, 33.35s/it]Time:  4.083017405006103
 65%|██████▌   | 65/100 [36:19<19:37, 33.65s/it]Time:  4.0024847749155015
 66%|██████▌   | 66/100 [36:53<19:06, 33.72s/it]Time:  4.0447037880076095
 67%|██████▋   | 67/100 [37:25<18:21, 33.39s/it]Time:  4.22180674795527
 68%|██████▊   | 68/100 [37:57<17:35, 32.99s/it]Time:  4.392742698080838
 69%|██████▉   | 69/100 [38:29<16:53, 32.69s/it]Time:  4.400819397997111
 70%|███████   | 70/100 [39:02<16:21, 32.70s/it]Time:  4.361928910948336
 71%|███████   | 71/100 [39:37<16:08, 33.40s/it]Time:  4.120749493013136
 72%|███████▏  | 72/100 [40:09<15:26, 33.08s/it]Time:  4.356424408964813
 73%|███████▎  | 73/100 [40:41<14:42, 32.70s/it]Time:  4.383947778958827
 74%|███████▍  | 74/100 [41:12<13:55, 32.14s/it]Time:  4.354661545017734
 75%|███████▌  | 75/100 [41:44<13:23, 32.13s/it]Time:  4.40760224708356
 76%|███████▌  | 76/100 [42:16<12:52, 32.17s/it]Time:  4.363708300981671
 77%|███████▋  | 77/100 [42:49<12:22, 32.27s/it]Time:  4.384546289918944
 78%|███████▊  | 78/100 [43:21<11:48, 32.21s/it]Time:  4.36681515397504
 79%|███████▉  | 79/100 [43:53<11:17, 32.27s/it]Time:  4.348456206964329
 80%|████████  | 80/100 [44:27<10:50, 32.54s/it]Time:  4.394830368924886
 81%|████████  | 81/100 [44:59<10:16, 32.43s/it]Time:  4.37099396600388
 82%|████████▏ | 82/100 [45:28<09:23, 31.33s/it]Time:  4.390769130084664
 83%|████████▎ | 83/100 [45:58<08:49, 31.14s/it]Time:  4.366474037989974
 84%|████████▍ | 84/100 [46:29<08:17, 31.11s/it]Time:  4.363794983946718
 85%|████████▌ | 85/100 [47:04<08:04, 32.30s/it]Time:  1.6343052430311218
 86%|████████▌ | 86/100 [47:29<06:59, 29.97s/it]Time:  2.960293445037678
 87%|████████▋ | 87/100 [48:03<06:45, 31.22s/it]Time:  3.7117130770348012
 88%|████████▊ | 88/100 [48:38<06:27, 32.26s/it]Time:  3.829127465025522
 89%|████████▉ | 89/100 [49:13<06:04, 33.13s/it]Time:  3.6932338339975104
 90%|█████████ | 90/100 [49:47<05:32, 33.30s/it]Time:  3.511752545950003
 91%|█████████ | 91/100 [50:21<05:01, 33.53s/it]Time:  3.472050739102997
 92%|█████████▏| 92/100 [50:55<04:30, 33.85s/it]Time:  3.6541918489383534
 93%|█████████▎| 93/100 [51:31<04:00, 34.36s/it]Time:  3.993123886990361
 94%|█████████▍| 94/100 [52:06<03:27, 34.64s/it]Time:  3.964137748000212
 95%|█████████▌| 95/100 [52:40<02:52, 34.54s/it]Time:  3.658044124022126
 96%|█████████▌| 96/100 [53:14<02:17, 34.26s/it]Time:  3.8107673230115324
 97%|█████████▋| 97/100 [53:49<01:43, 34.56s/it]Time:  4.081653576926328
 98%|█████████▊| 98/100 [54:27<01:11, 35.62s/it]Time:  2.462244975962676
 99%|█████████▉| 99/100 [54:59<00:34, 34.51s/it]Time:  2.2828318759566173
100%|██████████| 100/100 [55:17<00:00, 29.53s/it]100%|██████████| 100/100 [55:17<00:00, 33.18s/it]
Post-training time: 3321.46 seconds
GPU memory allocated: 376.33 MB, peak: 876.67 MB
Train results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy  test_time
Init.      0           NaN   2.417717          11.73          50.17  45.442881
Pre-Prune  0           NaN   2.417717          11.73          50.17  45.442881
Post-Prune 0           NaN   2.302585           9.99          50.04   3.791977
Final      100    2.302695   2.302588          10.00          50.00   2.282832
Prune results:
             module   param  sparsity     size             shape     flops  score mean  score variance    score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.100694     1728     (64, 3, 3, 3)   1769472   -0.027050        1.005513   -46.742134        0.805873            0.356814   1.392548e+03      True
1    layers.0.conv    bias  1.000000       64             (64,)     65536    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
2    layers.1.conv  weight  0.100423    36864    (64, 64, 3, 3)  37748736    0.000872        0.996963    32.139240        0.796522            0.362516   2.936300e+04      True
3    layers.1.conv    bias  1.000000       64             (64,)     65536    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
4    layers.3.conv  weight  0.099501    73728   (128, 64, 3, 3)  18874368    0.000958        0.993901    70.645149        0.796130            0.360079   5.869704e+04      True
5    layers.3.conv    bias  1.000000      128            (128,)     32768    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
6    layers.4.conv  weight  0.099413   147456  (128, 128, 3, 3)  37748736    0.001246        0.994699   183.733383        0.795026            0.362635   1.172313e+05      True
7    layers.4.conv    bias  1.000000      128            (128,)     32768    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
8    layers.6.conv  weight  0.099294   294912  (256, 128, 3, 3)  18874368   -0.001563        1.001676  -460.913849        0.798645            0.363844   2.355301e+05      True
9    layers.6.conv    bias  1.000000      256            (256,)     16384    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
10   layers.7.conv  weight  0.099864   589824  (256, 256, 3, 3)  37748736    0.000378        0.999599   222.715530        0.797776            0.363152   4.705474e+05      True
11   layers.7.conv    bias  1.000000      256            (256,)     16384    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
12   layers.8.conv  weight  0.099543   589824  (256, 256, 3, 3)  37748736    0.000403        0.998166   237.463287        0.796769            0.363324   4.699538e+05      True
13   layers.8.conv    bias  1.000000      256            (256,)     16384    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
14  layers.10.conv  weight  0.099947  1179648  (512, 256, 3, 3)  18874368    0.000511        0.999746   602.936768        0.797616            0.363555   9.409056e+05      True
15  layers.10.conv    bias  1.000000      512            (512,)      8192    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
16  layers.11.conv  weight  0.100168  2359296  (512, 512, 3, 3)  37748736   -0.000190        1.001348  -448.645966        0.798652            0.363503   1.884256e+06      True
17  layers.11.conv    bias  1.000000      512            (512,)      8192    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
18  layers.12.conv  weight  0.099864  2359296  (512, 512, 3, 3)  37748736    0.000249        0.999722   588.484009        0.797592            0.363569   1.881756e+06      True
19  layers.12.conv    bias  1.000000      512            (512,)      8192    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
20  layers.14.conv  weight  0.100023  2359296  (512, 512, 3, 3)   9437184   -0.000339        0.999254  -799.927002        0.797659            0.362994   1.881914e+06      True
21  layers.14.conv    bias  1.000000      512            (512,)      2048    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
22  layers.15.conv  weight  0.100098  2359296  (512, 512, 3, 3)   9437184    0.000708        1.000298  1670.380615        0.797886            0.363677   1.882448e+06      True
23  layers.15.conv    bias  1.000000      512            (512,)      2048    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
24  layers.16.conv  weight  0.100165  2359296  (512, 512, 3, 3)   9437184    0.000980        0.999300  2311.431885        0.797918            0.362629   1.882524e+06      True
25  layers.16.conv    bias  1.000000      512            (512,)      2048    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
26              fc  weight  0.096289     5120         (10, 512)      5120    0.007839        0.966788    40.134628        0.782462            0.354602   4.006208e+03      True
27              fc    bias  1.000000       10             (10,)        10    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
Parameter Sparsity: 1475793/14719818 (0.1003)
FLOP Sparsity: 31549407/313478154 (0.1006)
Saving results.
