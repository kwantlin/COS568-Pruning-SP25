Namespace(dataset='cifar10', model='vgg16', model_class='lottery', dense_classifier=False, pretrained=False, optimizer='adam', train_batch_size=256, test_batch_size=256, pre_epochs=0, post_epochs=100, lr=0.001, lr_drops=[], lr_drop_rate=0.1, weight_decay=0.0, pruner='rand', compression=0.1, quantization=True, prune_epochs=1, compression_schedule='exponential', mask_scope='global', prune_dataset_ratio=10, prune_batch_size=256, prune_bias=False, prune_batchnorm=False, prune_residual=False, prune_train_mode=False, reinitialize=False, shuffle=False, invert=False, pruner_list=[], prune_epoch_list=[], compression_list=[], level_list=[], experiment='singleshot', expid='QUANTIZED-cifar10-vgg16-lottery-rand-preEpochs0-postEpochs100-compression0.1', result_dir='Results/data', gpu=0, workers=4, no_cuda=False, seed=1, verbose=False)
Loading cifar10 dataset.
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with rand for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.74s/it]100%|██████████| 1/1 [00:02<00:00,  2.74s/it]
Post-Training for 100 epochs.
  0%|          | 0/100 [00:00<?, ?it/s]Time:  3.837036448996514
  1%|          | 1/100 [01:01<1:41:22, 61.44s/it]Time:  3.696664755931124
  2%|▏         | 2/100 [01:34<1:13:31, 45.01s/it]Time:  3.5866574990795925
  3%|▎         | 3/100 [02:07<1:03:48, 39.46s/it]Time:  4.520018826937303
  4%|▍         | 4/100 [02:39<58:16, 36.42s/it]  Time:  4.5413517479319125
  5%|▌         | 5/100 [03:09<53:53, 34.04s/it]Time:  4.518097207066603
  6%|▌         | 6/100 [03:40<51:37, 32.96s/it]Time:  4.541882408084348
  7%|▋         | 7/100 [04:11<50:02, 32.28s/it]Time:  4.533266141079366
  8%|▊         | 8/100 [04:42<48:57, 31.93s/it]Time:  4.532078373013064
  9%|▉         | 9/100 [05:14<48:36, 32.05s/it]Time:  4.575584416044876
 10%|█         | 10/100 [05:46<48:01, 32.02s/it]Time:  4.532611493952572
 11%|█         | 11/100 [06:17<47:02, 31.72s/it]Time:  4.540347573929466
 12%|█▏        | 12/100 [06:49<46:41, 31.84s/it]Time:  4.528086495003663
 13%|█▎        | 13/100 [07:21<46:06, 31.80s/it]Time:  4.530312844901346
 14%|█▍        | 14/100 [07:54<46:03, 32.14s/it]Time:  4.484450759948231
 15%|█▌        | 15/100 [08:26<45:43, 32.28s/it]Time:  4.504158049006946
 16%|█▌        | 16/100 [09:00<45:50, 32.74s/it]Time:  4.537974135950208
 17%|█▋        | 17/100 [09:32<44:56, 32.49s/it]Time:  4.508389616035856
 18%|█▊        | 18/100 [10:05<44:27, 32.53s/it]Time:  4.48695338703692
 19%|█▉        | 19/100 [10:37<43:56, 32.55s/it]Time:  4.577371992985718
 20%|██        | 20/100 [11:10<43:34, 32.68s/it]Time:  4.575360462069511
 21%|██        | 21/100 [11:44<43:12, 32.82s/it]Time:  4.458802497014403
 22%|██▏       | 22/100 [12:17<42:44, 32.88s/it]Time:  4.065565008088015
 23%|██▎       | 23/100 [12:49<42:06, 32.82s/it]Time:  3.7593296649865806
 24%|██▍       | 24/100 [13:23<41:50, 33.03s/it]Time:  4.330798601033166
 25%|██▌       | 25/100 [13:58<42:15, 33.81s/it]Time:  4.330497181974351
 26%|██▌       | 26/100 [14:31<41:11, 33.40s/it]Time:  4.361092965002172
 27%|██▋       | 27/100 [15:04<40:22, 33.19s/it]Time:  4.015456147026271
 28%|██▊       | 28/100 [15:36<39:24, 32.84s/it]Time:  3.96183667704463
 29%|██▉       | 29/100 [16:08<38:33, 32.59s/it]Time:  3.9777920229826123
 30%|███       | 30/100 [16:41<38:15, 32.79s/it]Time:  3.7276091020321473
 31%|███       | 31/100 [17:14<37:48, 32.88s/it]Time:  4.04081135801971
 32%|███▏      | 32/100 [17:49<38:10, 33.69s/it]Time:  3.953570654965006
 33%|███▎      | 33/100 [18:23<37:33, 33.63s/it]Time:  4.119025209103711
 34%|███▍      | 34/100 [19:01<38:24, 34.92s/it]Time:  4.027067657909356
 35%|███▌      | 35/100 [19:33<36:55, 34.08s/it]Time:  3.8375427999999374
 36%|███▌      | 36/100 [20:10<37:17, 34.96s/it]Time:  4.132087434991263
 37%|███▋      | 37/100 [20:44<36:18, 34.57s/it]Time:  3.888952363980934
 38%|███▊      | 38/100 [21:17<35:19, 34.18s/it]Time:  4.042433158960193
 39%|███▉      | 39/100 [21:49<34:11, 33.64s/it]Time:  4.274892715970054
 40%|████      | 40/100 [22:22<33:12, 33.21s/it]Time:  4.353180496953428
 41%|████      | 41/100 [22:53<32:10, 32.73s/it]Time:  4.384049816988409
 42%|████▏     | 42/100 [23:27<31:50, 32.93s/it]Time:  4.350404427968897
 43%|████▎     | 43/100 [23:58<30:48, 32.42s/it]Time:  4.3578681600047275
 44%|████▍     | 44/100 [24:29<29:53, 32.03s/it]Time:  4.3374829379608855
 45%|████▌     | 45/100 [25:02<29:34, 32.26s/it]Time:  4.349943189998157
 46%|████▌     | 46/100 [25:35<29:24, 32.67s/it]Time:  4.314289222937077
 47%|████▋     | 47/100 [26:07<28:36, 32.39s/it]Time:  4.267425167025067
 48%|████▊     | 48/100 [26:40<28:08, 32.46s/it]Time:  4.279109620023519
 49%|████▉     | 49/100 [27:12<27:26, 32.29s/it]Time:  4.342037897906266
 50%|█████     | 50/100 [27:41<26:05, 31.30s/it]Time:  4.354787392891012
 51%|█████     | 51/100 [28:13<25:46, 31.56s/it]Time:  4.348461916903034
 52%|█████▏    | 52/100 [28:45<25:25, 31.78s/it]Time:  4.386415067012422
 53%|█████▎    | 53/100 [29:17<24:59, 31.90s/it]Time:  4.342185613000765
 54%|█████▍    | 54/100 [29:49<24:26, 31.87s/it]Time:  4.3481908780522645
 55%|█████▌    | 55/100 [30:21<23:59, 31.99s/it]Time:  4.312953944900073
 56%|█████▌    | 56/100 [30:53<23:28, 32.02s/it]Time:  4.199874050100334
 57%|█████▋    | 57/100 [31:25<22:54, 31.96s/it]Time:  4.272469830932096
 58%|█████▊    | 58/100 [31:57<22:26, 32.05s/it]Time:  4.331631515990011
 59%|█████▉    | 59/100 [32:30<22:04, 32.31s/it]Time:  4.2773065499495715
 60%|██████    | 60/100 [33:03<21:38, 32.46s/it]Time:  4.245717178913765
 61%|██████    | 61/100 [33:36<21:08, 32.54s/it]Time:  4.291686553042382
 62%|██████▏   | 62/100 [34:09<20:41, 32.68s/it]Time:  4.266955323051661
 63%|██████▎   | 63/100 [34:42<20:12, 32.76s/it]Time:  3.5509484839858487
 64%|██████▍   | 64/100 [35:15<19:38, 32.74s/it]Time:  2.569370545912534
 65%|██████▌   | 65/100 [35:46<18:53, 32.38s/it]Time:  3.3519728630781174
 66%|██████▌   | 66/100 [36:20<18:33, 32.74s/it]Time:  3.6453589579323307
 67%|██████▋   | 67/100 [36:54<18:14, 33.17s/it]Time:  3.5140068220207468
 68%|██████▊   | 68/100 [37:27<17:44, 33.27s/it]Time:  3.5545711299637333
 69%|██████▉   | 69/100 [38:01<17:11, 33.29s/it]Time:  3.8879120720084757
 70%|███████   | 70/100 [38:34<16:39, 33.32s/it]Time:  3.803168927086517
 71%|███████   | 71/100 [39:07<16:04, 33.27s/it]Time:  2.483888935064897
 72%|███████▏  | 72/100 [39:39<15:18, 32.81s/it]Time:  3.1623420390533283
 73%|███████▎  | 73/100 [40:12<14:50, 32.96s/it]Time:  3.751749553019181
 74%|███████▍  | 74/100 [40:47<14:28, 33.41s/it]Time:  4.005278998985887
 75%|███████▌  | 75/100 [41:19<13:49, 33.18s/it]Time:  4.0602752569830045
 76%|███████▌  | 76/100 [41:52<13:14, 33.09s/it]Time:  4.113929111044854
 77%|███████▋  | 77/100 [42:25<12:41, 33.10s/it]Time:  4.111849854001775
 78%|███████▊  | 78/100 [42:59<12:11, 33.25s/it]Time:  3.9342137230560184
 79%|███████▉  | 79/100 [43:32<11:33, 33.04s/it]Time:  3.640238226042129
 80%|████████  | 80/100 [44:05<11:04, 33.21s/it]Time:  4.038944364991039
 81%|████████  | 81/100 [44:38<10:29, 33.14s/it]Time:  4.077337005059235
 82%|████████▏ | 82/100 [45:12<09:59, 33.30s/it]Time:  4.033710204996169
 83%|████████▎ | 83/100 [45:47<09:33, 33.76s/it]Time:  3.6311328950105235
 84%|████████▍ | 84/100 [46:20<08:59, 33.70s/it]Time:  3.51870609796606
 85%|████████▌ | 85/100 [46:53<08:22, 33.53s/it]Time:  3.5247699819738045
 86%|████████▌ | 86/100 [47:28<07:53, 33.82s/it]Time:  3.344595075934194
 87%|████████▋ | 87/100 [48:02<07:22, 34.01s/it]Time:  3.7503974569262937
 88%|████████▊ | 88/100 [48:38<06:53, 34.46s/it]Time:  3.8961886739125475
 89%|████████▉ | 89/100 [49:13<06:21, 34.66s/it]Time:  3.7184908379567787
 90%|█████████ | 90/100 [49:47<05:45, 34.60s/it]Time:  3.175377041916363
 91%|█████████ | 91/100 [50:21<05:09, 34.33s/it]Time:  3.4861940260743722
 92%|█████████▏| 92/100 [50:55<04:34, 34.32s/it]Time:  3.5493269589496776
 93%|█████████▎| 93/100 [51:31<04:02, 34.67s/it]Time:  3.9934161639539525
 94%|█████████▍| 94/100 [52:06<03:29, 34.91s/it]Time:  3.734153565019369
 95%|█████████▌| 95/100 [52:40<02:53, 34.66s/it]Time:  3.6903038850286976
 96%|█████████▌| 96/100 [53:14<02:17, 34.42s/it]Time:  3.8014468930196017
 97%|█████████▋| 97/100 [53:49<01:43, 34.60s/it]Time:  4.019990905071609
 98%|█████████▊| 98/100 [54:27<01:11, 35.66s/it]Time:  2.8245347880292684
 99%|█████████▉| 99/100 [55:00<00:34, 34.67s/it]Time:  1.4439103220356628
100%|██████████| 100/100 [55:18<00:00, 29.76s/it]100%|██████████| 100/100 [55:18<00:00, 33.19s/it]
Post-training time: 3322.32 seconds
GPU memory allocated: 376.33 MB, peak: 876.67 MB
Train results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy  test_time
Init.      0           NaN   2.417717          11.73          50.17  45.440055
Pre-Prune  0           NaN   2.417717          11.73          50.17  45.440055
Post-Prune 0           NaN   2.306984           9.79          48.42   3.798972
Final      100    0.082698   0.616105          87.95          99.12   1.443910
Prune results:
             module   param  sparsity     size             shape     flops  score mean  score variance    score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.780671     1728     (64, 3, 3, 3)   1769472   -0.027050        1.005513   -46.742134        0.805873            0.356814   1.392548e+03      True
1    layers.0.conv    bias  1.000000       64             (64,)     65536    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
2    layers.1.conv  weight  0.795085    36864    (64, 64, 3, 3)  37748736    0.000872        0.996963    32.139240        0.796522            0.362516   2.936300e+04      True
3    layers.1.conv    bias  1.000000       64             (64,)     65536    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
4    layers.3.conv  weight  0.793850    73728   (128, 64, 3, 3)  18874368    0.000958        0.993901    70.645149        0.796130            0.360079   5.869704e+04      True
5    layers.3.conv    bias  1.000000      128            (128,)     32768    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
6    layers.4.conv  weight  0.795390   147456  (128, 128, 3, 3)  37748736    0.001246        0.994699   183.733383        0.795026            0.362635   1.172313e+05      True
7    layers.4.conv    bias  1.000000      128            (128,)     32768    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
8    layers.6.conv  weight  0.793606   294912  (256, 128, 3, 3)  18874368   -0.001563        1.001676  -460.913849        0.798645            0.363844   2.355301e+05      True
9    layers.6.conv    bias  1.000000      256            (256,)     16384    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
10   layers.7.conv  weight  0.794293   589824  (256, 256, 3, 3)  37748736    0.000378        0.999599   222.715530        0.797776            0.363152   4.705474e+05      True
11   layers.7.conv    bias  1.000000      256            (256,)     16384    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
12   layers.8.conv  weight  0.794837   589824  (256, 256, 3, 3)  37748736    0.000403        0.998166   237.463287        0.796769            0.363324   4.699538e+05      True
13   layers.8.conv    bias  1.000000      256            (256,)     16384    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
14  layers.10.conv  weight  0.794458  1179648  (512, 256, 3, 3)  18874368    0.000511        0.999746   602.936768        0.797616            0.363555   9.409056e+05      True
15  layers.10.conv    bias  1.000000      512            (512,)      8192    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
16  layers.11.conv  weight  0.794004  2359296  (512, 512, 3, 3)  37748736   -0.000190        1.001348  -448.645966        0.798652            0.363503   1.884256e+06      True
17  layers.11.conv    bias  1.000000      512            (512,)      8192    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
18  layers.12.conv  weight  0.794372  2359296  (512, 512, 3, 3)  37748736    0.000249        0.999722   588.484009        0.797592            0.363569   1.881756e+06      True
19  layers.12.conv    bias  1.000000      512            (512,)      8192    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
20  layers.14.conv  weight  0.793964  2359296  (512, 512, 3, 3)   9437184   -0.000339        0.999254  -799.927002        0.797659            0.362994   1.881914e+06      True
21  layers.14.conv    bias  1.000000      512            (512,)      2048    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
22  layers.15.conv  weight  0.794540  2359296  (512, 512, 3, 3)   9437184    0.000708        1.000298  1670.380615        0.797886            0.363677   1.882448e+06      True
23  layers.15.conv    bias  1.000000      512            (512,)      2048    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
24  layers.16.conv  weight  0.794592  2359296  (512, 512, 3, 3)   9437184    0.000980        0.999300  2311.431885        0.797918            0.362629   1.882524e+06      True
25  layers.16.conv    bias  1.000000      512            (512,)      2048    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
26              fc  weight  0.805273     5120         (10, 512)      5120    0.007839        0.966788    40.134628        0.782462            0.354602   4.006208e+03      True
27              fc    bias  1.000000       10             (10,)        10    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
Parameter Sparsity: 11693237/14719818 (0.7944)
FLOP Sparsity: 249094053/313478154 (0.7946)
Saving results.
