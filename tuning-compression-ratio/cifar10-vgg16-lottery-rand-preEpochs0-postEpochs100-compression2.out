Namespace(dataset='cifar10', model='vgg16', model_class='lottery', dense_classifier=False, pretrained=False, optimizer='adam', train_batch_size=256, test_batch_size=256, pre_epochs=0, post_epochs=100, lr=0.001, lr_drops=[], lr_drop_rate=0.1, weight_decay=0.0, pruner='rand', compression=2.0, quantization=False, prune_epochs=1, compression_schedule='exponential', mask_scope='global', prune_dataset_ratio=10, prune_batch_size=256, prune_bias=False, prune_batchnorm=False, prune_residual=False, prune_train_mode=False, reinitialize=False, shuffle=False, invert=False, pruner_list=[], prune_epoch_list=[], compression_list=[], level_list=[], experiment='singleshot', expid='cifar10-vgg16-lottery-rand-preEpochs0-postEpochs100-compression2', result_dir='Results/data', gpu=0, workers=4, no_cuda=False, seed=1, verbose=False)
Loading cifar10 dataset.
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with rand for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:09<00:00,  9.63s/it]100%|██████████| 1/1 [00:09<00:00,  9.63s/it]
Post-Training for 100 epochs.
  0%|          | 0/100 [00:00<?, ?it/s]Time:  3.8842508736997843
  1%|          | 1/100 [01:17<2:07:57, 77.55s/it]Time:  3.851661621592939
  2%|▏         | 2/100 [02:12<1:44:46, 64.15s/it]Time:  3.8254016134887934
  3%|▎         | 3/100 [03:08<1:37:27, 60.29s/it]Time:  3.4512536134570837
  4%|▍         | 4/100 [04:03<1:33:24, 58.39s/it]Time:  3.4526939960196614
  5%|▌         | 5/100 [04:58<1:30:17, 57.02s/it]Time:  3.7372516179457307
  6%|▌         | 6/100 [05:52<1:28:03, 56.21s/it]Time:  3.8801996605470777
  7%|▋         | 7/100 [06:46<1:26:07, 55.57s/it]Time:  3.3954904060810804
  8%|▊         | 8/100 [07:41<1:24:50, 55.33s/it]Time:  3.6693762578070164
  9%|▉         | 9/100 [08:35<1:23:20, 54.95s/it]Time:  4.409037622623146
 10%|█         | 10/100 [09:31<1:22:45, 55.18s/it]Time:  4.668705643154681
 11%|█         | 11/100 [10:26<1:21:40, 55.06s/it]Time:  4.622774071060121
 12%|█▏        | 12/100 [11:20<1:20:22, 54.81s/it]Time:  4.691954595036805
 13%|█▎        | 13/100 [12:15<1:19:27, 54.80s/it]Time:  4.723128279671073
 14%|█▍        | 14/100 [13:14<1:20:36, 56.24s/it]Time:  4.599808035418391
 15%|█▌        | 15/100 [14:09<1:18:56, 55.73s/it]Time:  4.676580901257694
 16%|█▌        | 16/100 [15:03<1:17:29, 55.35s/it]Time:  4.5930687338113785
 17%|█▋        | 17/100 [15:58<1:16:11, 55.07s/it]Time:  4.652235844172537
 18%|█▊        | 18/100 [16:52<1:14:50, 54.77s/it]Time:  4.7084467662498355
 19%|█▉        | 19/100 [17:45<1:13:23, 54.37s/it]Time:  4.859578552655876
 20%|██        | 20/100 [18:40<1:12:42, 54.53s/it]Time:  4.811362248845398
 21%|██        | 21/100 [19:34<1:11:20, 54.18s/it]Time:  4.772250664420426
 22%|██▏       | 22/100 [20:27<1:10:14, 54.03s/it]Time:  4.779298827983439
 23%|██▎       | 23/100 [21:22<1:09:37, 54.26s/it]Time:  4.673723773099482
 24%|██▍       | 24/100 [22:18<1:09:12, 54.63s/it]Time:  4.74533124640584
 25%|██▌       | 25/100 [23:11<1:07:44, 54.19s/it]Time:  4.800263871438801
 26%|██▌       | 26/100 [24:11<1:08:57, 55.92s/it]Time:  4.488419023342431
 27%|██▋       | 27/100 [25:06<1:07:55, 55.82s/it]Time:  4.416184883564711
 28%|██▊       | 28/100 [26:02<1:06:45, 55.63s/it]Time:  4.539684104733169
 29%|██▉       | 29/100 [26:55<1:05:00, 54.94s/it]Time:  4.757305596955121
 30%|███       | 30/100 [27:46<1:02:52, 53.89s/it]Time:  4.736829359084368
 31%|███       | 31/100 [28:40<1:02:02, 53.94s/it]Time:  4.723951010033488
 32%|███▏      | 32/100 [29:35<1:01:13, 54.02s/it]Time:  4.685230936855078
 33%|███▎      | 33/100 [30:29<1:00:20, 54.04s/it]Time:  4.687939683906734
 34%|███▍      | 34/100 [31:23<59:25, 54.03s/it]  Time:  4.428225541487336
 35%|███▌      | 35/100 [32:18<58:49, 54.30s/it]Time:  4.5153200738132
 36%|███▌      | 36/100 [33:12<57:51, 54.24s/it]Time:  4.531193317845464
 37%|███▋      | 37/100 [34:05<56:41, 53.99s/it]Time:  4.540249014273286
 38%|███▊      | 38/100 [34:59<55:37, 53.84s/it]Time:  4.596032599918544
 39%|███▉      | 39/100 [35:52<54:42, 53.81s/it]Time:  4.687910542823374
 40%|████      | 40/100 [36:46<53:52, 53.88s/it]Time:  4.565026030875742
 41%|████      | 41/100 [37:40<52:53, 53.79s/it]Time:  4.588172606192529
 42%|████▏     | 42/100 [38:33<51:50, 53.63s/it]Time:  4.6016079261898994
 43%|████▎     | 43/100 [39:27<51:02, 53.72s/it]Time:  4.527978713624179
 44%|████▍     | 44/100 [40:21<50:09, 53.74s/it]Time:  4.635857601650059
 45%|████▌     | 45/100 [41:15<49:20, 53.82s/it]Time:  4.533992053940892
 46%|████▌     | 46/100 [42:09<48:24, 53.79s/it]Time:  4.460438900627196
 47%|████▋     | 47/100 [43:02<47:29, 53.77s/it]Time:  4.5344875026494265
 48%|████▊     | 48/100 [43:56<46:38, 53.82s/it]Time:  4.533752939663827
 49%|████▉     | 49/100 [44:50<45:42, 53.78s/it]Time:  4.478820581920445
 50%|█████     | 50/100 [45:44<44:46, 53.74s/it]Time:  4.593093633651733
 51%|█████     | 51/100 [46:38<44:01, 53.90s/it]Time:  4.5432459358125925
 52%|█████▏    | 52/100 [47:32<43:09, 53.96s/it]Time:  4.563736439682543
 53%|█████▎    | 53/100 [48:26<42:15, 53.95s/it]Time:  4.568644496612251
 54%|█████▍    | 54/100 [49:20<41:19, 53.91s/it]Time:  4.533702197484672
 55%|█████▌    | 55/100 [50:14<40:25, 53.90s/it]Time:  4.635767704807222
 56%|█████▌    | 56/100 [51:08<39:31, 53.91s/it]Time:  4.498060000129044
 57%|█████▋    | 57/100 [52:01<38:32, 53.79s/it]Time:  4.545368901453912
 58%|█████▊    | 58/100 [52:55<37:39, 53.79s/it]Time:  4.5020946795120835
 59%|█████▉    | 59/100 [53:48<36:41, 53.69s/it]Time:  4.41867879871279
 60%|██████    | 60/100 [54:42<35:48, 53.71s/it]Time:  4.478769375011325
 61%|██████    | 61/100 [55:36<34:52, 53.64s/it]Time:  4.54503383859992
 62%|██████▏   | 62/100 [56:29<33:59, 53.68s/it]Time:  4.593855478800833
 63%|██████▎   | 63/100 [57:23<33:11, 53.81s/it]Time:  4.62781726475805
 64%|██████▍   | 64/100 [58:17<32:16, 53.79s/it]Time:  4.563003547489643
 65%|██████▌   | 65/100 [59:12<31:32, 54.08s/it]Time:  4.995257300324738
 66%|██████▌   | 66/100 [1:00:07<30:46, 54.31s/it]Time:  4.764902087859809
 67%|██████▋   | 67/100 [1:01:01<29:49, 54.24s/it]Time:  4.696089301258326
 68%|██████▊   | 68/100 [1:01:55<28:55, 54.23s/it]Time:  4.637477615848184
 69%|██████▉   | 69/100 [1:02:49<27:57, 54.11s/it]Time:  4.630762284621596
 70%|███████   | 70/100 [1:03:43<27:00, 54.00s/it]Time:  4.708217267878354
 71%|███████   | 71/100 [1:04:37<26:07, 54.04s/it]Time:  4.6937665324658155
 72%|███████▏  | 72/100 [1:05:31<25:12, 54.02s/it]Time:  4.708084976300597
 73%|███████▎  | 73/100 [1:06:25<24:17, 53.99s/it]Time:  4.743777911178768
 74%|███████▍  | 74/100 [1:07:19<23:23, 54.00s/it]Time:  4.722917819395661
 75%|███████▌  | 75/100 [1:08:13<22:28, 53.95s/it]Time:  4.803470656275749
 76%|███████▌  | 76/100 [1:09:06<21:32, 53.85s/it]Time:  4.782250848598778
 77%|███████▋  | 77/100 [1:10:00<20:38, 53.84s/it]Time:  4.7894446942955256
 78%|███████▊  | 78/100 [1:10:53<19:42, 53.73s/it]Time:  4.849333440884948
 79%|███████▉  | 79/100 [1:11:47<18:48, 53.73s/it]Time:  5.065253864042461
 80%|████████  | 80/100 [1:12:41<17:55, 53.79s/it]Time:  5.0618999525904655
 81%|████████  | 81/100 [1:13:35<17:04, 53.94s/it]Time:  4.996060097590089
 82%|████████▏ | 82/100 [1:14:29<16:11, 53.97s/it]Time:  4.992208551615477
 83%|████████▎ | 83/100 [1:15:23<15:17, 53.97s/it]Time:  4.948807450942695
 84%|████████▍ | 84/100 [1:16:17<14:22, 53.88s/it]Time:  4.9987240405753255
 85%|████████▌ | 85/100 [1:17:11<13:28, 53.89s/it]Time:  5.025839911773801
 86%|████████▌ | 86/100 [1:18:05<12:33, 53.84s/it]Time:  5.058108182623982
 87%|████████▋ | 87/100 [1:18:59<11:39, 53.85s/it]Time:  4.96851207036525
 88%|████████▊ | 88/100 [1:19:52<10:45, 53.82s/it]Time:  4.962863992899656
 89%|████████▉ | 89/100 [1:20:46<09:51, 53.75s/it]Time:  5.007545379921794
 90%|█████████ | 90/100 [1:21:40<08:57, 53.71s/it]Time:  4.989391587674618
 91%|█████████ | 91/100 [1:22:33<08:02, 53.61s/it]Time:  4.977789793163538
 92%|█████████▏| 92/100 [1:23:27<07:09, 53.69s/it]Time:  4.9871284225955606
 93%|█████████▎| 93/100 [1:24:19<06:13, 53.35s/it]Time:  4.987766359001398
 94%|█████████▍| 94/100 [1:25:13<05:19, 53.30s/it]Time:  4.990349862724543
 95%|█████████▌| 95/100 [1:26:06<04:26, 53.28s/it]Time:  5.0072115967050195
 96%|█████████▌| 96/100 [1:26:59<03:33, 53.37s/it]Time:  4.994723679497838
 97%|█████████▋| 97/100 [1:27:53<02:40, 53.54s/it]Time:  5.035350311547518
 98%|█████████▊| 98/100 [1:28:47<01:47, 53.55s/it]Time:  4.11572286952287
 99%|█████████▉| 99/100 [1:29:39<00:53, 53.13s/it]Time:  1.5933487704023719
100%|██████████| 100/100 [1:30:14<00:00, 47.59s/it]100%|██████████| 100/100 [1:30:14<00:00, 54.14s/it]
Post-training time: 5414.89 seconds
GPU memory allocated: 375.42 MB, peak: 999.49 MB
Train results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy   test_time
Init.      0           NaN   2.417717          11.73          50.17  126.420388
Pre-Prune  0           NaN   2.417717          11.73          50.17  126.420388
Post-Prune 0           NaN   2.302585          10.45          49.06    0.714338
Final      100    2.302679   2.302588          10.00          50.00    1.593349
Prune results:
             module   param  sparsity     size             shape     flops  score mean  score variance    score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.011574     1728     (64, 3, 3, 3)   1769472   -0.027050        1.005513   -46.742134        0.805873            0.356814   1.392548e+03      True
1    layers.0.conv    bias  1.000000       64             (64,)     65536    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
2    layers.1.conv  weight  0.009494    36864    (64, 64, 3, 3)  37748736    0.000872        0.996963    32.139240        0.796522            0.362516   2.936300e+04      True
3    layers.1.conv    bias  1.000000       64             (64,)     65536    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
4    layers.3.conv  weight  0.010335    73728   (128, 64, 3, 3)  18874368    0.000958        0.993901    70.645149        0.796130            0.360079   5.869704e+04      True
5    layers.3.conv    bias  1.000000      128            (128,)     32768    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
6    layers.4.conv  weight  0.009881   147456  (128, 128, 3, 3)  37748736    0.001246        0.994699   183.733383        0.795026            0.362635   1.172313e+05      True
7    layers.4.conv    bias  1.000000      128            (128,)     32768    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
8    layers.6.conv  weight  0.010067   294912  (256, 128, 3, 3)  18874368   -0.001563        1.001676  -460.913849        0.798645            0.363844   2.355301e+05      True
9    layers.6.conv    bias  1.000000      256            (256,)     16384    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
10   layers.7.conv  weight  0.009972   589824  (256, 256, 3, 3)  37748736    0.000378        0.999599   222.715530        0.797776            0.363152   4.705474e+05      True
11   layers.7.conv    bias  1.000000      256            (256,)     16384    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
12   layers.8.conv  weight  0.010044   589824  (256, 256, 3, 3)  37748736    0.000403        0.998166   237.463287        0.796769            0.363324   4.699538e+05      True
13   layers.8.conv    bias  1.000000      256            (256,)     16384    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
14  layers.10.conv  weight  0.009916  1179648  (512, 256, 3, 3)  18874368    0.000511        0.999746   602.936768        0.797616            0.363555   9.409056e+05      True
15  layers.10.conv    bias  1.000000      512            (512,)      8192    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
16  layers.11.conv  weight  0.009981  2359296  (512, 512, 3, 3)  37748736   -0.000190        1.001348  -448.645966        0.798652            0.363503   1.884256e+06      True
17  layers.11.conv    bias  1.000000      512            (512,)      8192    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
18  layers.12.conv  weight  0.010007  2359296  (512, 512, 3, 3)  37748736    0.000249        0.999722   588.484009        0.797592            0.363569   1.881756e+06      True
19  layers.12.conv    bias  1.000000      512            (512,)      8192    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
20  layers.14.conv  weight  0.009933  2359296  (512, 512, 3, 3)   9437184   -0.000339        0.999254  -799.927002        0.797659            0.362994   1.881914e+06      True
21  layers.14.conv    bias  1.000000      512            (512,)      2048    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
22  layers.15.conv  weight  0.010127  2359296  (512, 512, 3, 3)   9437184    0.000708        1.000298  1670.380615        0.797886            0.363677   1.882448e+06      True
23  layers.15.conv    bias  1.000000      512            (512,)      2048    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
24  layers.16.conv  weight  0.009989  2359296  (512, 512, 3, 3)   9437184    0.000980        0.999300  2311.431885        0.797918            0.362629   1.882524e+06      True
25  layers.16.conv    bias  1.000000      512            (512,)      2048    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
26              fc  weight  0.008008     5120         (10, 512)      5120    0.007839        0.966788    40.134628        0.782462            0.354602   4.006208e+03      True
27              fc    bias  1.000000       10             (10,)        10    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
Parameter Sparsity: 151390/14719818 (0.0103)
FLOP Sparsity: 3394350/313478154 (0.0108)
Saving results.
