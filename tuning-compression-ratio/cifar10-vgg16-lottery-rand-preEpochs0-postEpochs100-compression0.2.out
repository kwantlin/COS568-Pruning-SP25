Namespace(dataset='cifar10', model='vgg16', model_class='lottery', dense_classifier=False, pretrained=False, optimizer='adam', train_batch_size=256, test_batch_size=256, pre_epochs=0, post_epochs=100, lr=0.001, lr_drops=[], lr_drop_rate=0.1, weight_decay=0.0, pruner='rand', compression=0.2, quantization=False, prune_epochs=1, compression_schedule='exponential', mask_scope='global', prune_dataset_ratio=10, prune_batch_size=256, prune_bias=False, prune_batchnorm=False, prune_residual=False, prune_train_mode=False, reinitialize=False, shuffle=False, invert=False, pruner_list=[], prune_epoch_list=[], compression_list=[], level_list=[], experiment='singleshot', expid='cifar10-vgg16-lottery-rand-preEpochs0-postEpochs100-compression0.2', result_dir='Results/data', gpu=0, workers=4, no_cuda=False, seed=1, verbose=False)
Loading cifar10 dataset.
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with rand for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:09<00:00,  9.63s/it]100%|██████████| 1/1 [00:09<00:00,  9.63s/it]
Post-Training for 100 epochs.
  0%|          | 0/100 [00:00<?, ?it/s]Time:  3.900415754877031
  1%|          | 1/100 [01:12<1:59:09, 72.22s/it]Time:  3.733479921706021
  2%|▏         | 2/100 [02:06<1:41:11, 61.96s/it]Time:  3.7858175886794925
  3%|▎         | 3/100 [03:02<1:35:06, 58.83s/it]Time:  3.3566398499533534
  4%|▍         | 4/100 [03:56<1:31:35, 57.25s/it]Time:  3.9824991151690483
  5%|▌         | 5/100 [04:52<1:29:28, 56.51s/it]Time:  3.919974146410823
  6%|▌         | 6/100 [05:47<1:28:04, 56.22s/it]Time:  4.157757864333689
  7%|▋         | 7/100 [06:42<1:26:22, 55.73s/it]Time:  3.5781486993655562
  8%|▊         | 8/100 [07:37<1:24:55, 55.39s/it]Time:  4.235605743713677
  9%|▉         | 9/100 [08:31<1:23:26, 55.01s/it]Time:  4.630205965600908
 10%|█         | 10/100 [09:26<1:22:48, 55.20s/it]Time:  4.56160425208509
 11%|█         | 11/100 [10:21<1:21:39, 55.05s/it]Time:  4.405869350768626
 12%|█▏        | 12/100 [11:16<1:20:43, 55.04s/it]Time:  4.430869988165796
 13%|█▎        | 13/100 [12:11<1:19:38, 54.93s/it]Time:  4.351331848651171
 14%|█▍        | 14/100 [13:10<1:20:26, 56.12s/it]Time:  4.530462831258774
 15%|█▌        | 15/100 [14:05<1:18:56, 55.72s/it]Time:  4.485915127210319
 16%|█▌        | 16/100 [14:59<1:17:33, 55.40s/it]Time:  4.432146191596985
 17%|█▋        | 17/100 [15:54<1:16:20, 55.18s/it]Time:  4.485470558516681
 18%|█▊        | 18/100 [16:48<1:14:58, 54.87s/it]Time:  4.487022970803082
 19%|█▉        | 19/100 [17:42<1:13:45, 54.63s/it]Time:  4.252078135497868
 20%|██        | 20/100 [18:36<1:12:41, 54.52s/it]Time:  4.426037382334471
 21%|██        | 21/100 [19:29<1:11:11, 54.07s/it]Time:  4.678909312002361
 22%|██▏       | 22/100 [20:23<1:10:03, 53.90s/it]Time:  4.721693349070847
 23%|██▎       | 23/100 [21:17<1:09:07, 53.87s/it]Time:  4.68027850612998
 24%|██▍       | 24/100 [22:12<1:08:47, 54.30s/it]Time:  4.7531799003481865
 25%|██▌       | 25/100 [23:05<1:07:29, 53.99s/it]Time:  4.730337521992624
 26%|██▌       | 26/100 [24:05<1:08:51, 55.83s/it]Time:  4.488020434975624
 27%|██▋       | 27/100 [25:01<1:07:52, 55.78s/it]Time:  4.39102700445801
 28%|██▊       | 28/100 [25:56<1:06:46, 55.65s/it]Time:  4.505943185649812
 29%|██▉       | 29/100 [26:50<1:04:59, 54.92s/it]Time:  4.704855566844344
 30%|███       | 30/100 [27:41<1:02:46, 53.81s/it]Time:  4.705157787539065
 31%|███       | 31/100 [28:35<1:02:00, 53.92s/it]Time:  4.661035523749888
 32%|███▏      | 32/100 [29:29<1:01:16, 54.07s/it]Time:  4.638665695674717
 33%|███▎      | 33/100 [30:23<1:00:22, 54.07s/it]Time:  4.552977509796619
 34%|███▍      | 34/100 [31:17<59:23, 54.00s/it]  Time:  4.621875622309744
 35%|███▌      | 35/100 [32:11<58:29, 53.99s/it]Time:  4.653886700049043
 36%|███▌      | 36/100 [33:05<57:32, 53.94s/it]Time:  4.797275314107537
 37%|███▋      | 37/100 [33:58<56:18, 53.63s/it]Time:  4.758504445664585
 38%|███▊      | 38/100 [34:52<55:25, 53.63s/it]Time:  4.804961142130196
 39%|███▉      | 39/100 [35:45<54:28, 53.58s/it]Time:  4.7939595729112625
 40%|████      | 40/100 [36:39<53:37, 53.62s/it]Time:  4.785859779454768
 41%|████      | 41/100 [37:32<52:44, 53.63s/it]Time:  4.802194165065885
 42%|████▏     | 42/100 [38:26<51:46, 53.56s/it]Time:  4.834422449581325
 43%|████▎     | 43/100 [39:20<51:01, 53.71s/it]Time:  4.813649701885879
 44%|████▍     | 44/100 [40:14<50:08, 53.73s/it]Time:  4.7884765369817615
 45%|████▌     | 45/100 [41:08<49:20, 53.83s/it]Time:  4.788704787380993
 46%|████▌     | 46/100 [42:02<48:29, 53.88s/it]Time:  4.774702791124582
 47%|████▋     | 47/100 [42:55<47:30, 53.78s/it]Time:  4.7865371303632855
 48%|████▊     | 48/100 [43:49<46:31, 53.69s/it]Time:  4.797676831483841
 49%|████▉     | 49/100 [44:43<45:40, 53.73s/it]Time:  4.80405234079808
 50%|█████     | 50/100 [45:37<44:49, 53.78s/it]Time:  4.8009739480912685
 51%|█████     | 51/100 [46:30<43:50, 53.69s/it]Time:  4.830773489549756
 52%|█████▏    | 52/100 [47:24<42:58, 53.71s/it]Time:  4.809955234639347
 53%|█████▎    | 53/100 [48:17<42:04, 53.71s/it]Time:  4.870437025092542
 54%|█████▍    | 54/100 [49:11<41:09, 53.70s/it]Time:  4.901955179870129
 55%|█████▌    | 55/100 [50:05<40:23, 53.85s/it]Time:  4.876602944917977
 56%|█████▌    | 56/100 [50:59<39:26, 53.80s/it]Time:  4.894388019107282
 57%|█████▋    | 57/100 [51:53<38:33, 53.80s/it]Time:  4.851865134201944
 58%|█████▊    | 58/100 [52:46<37:37, 53.74s/it]Time:  4.874857421964407
 59%|█████▉    | 59/100 [53:40<36:43, 53.75s/it]Time:  4.8584223333746195
 60%|██████    | 60/100 [54:34<35:51, 53.78s/it]Time:  4.864161101169884
 61%|██████    | 61/100 [55:28<34:59, 53.83s/it]Time:  4.7803484024479985
 62%|██████▏   | 62/100 [56:22<34:03, 53.78s/it]Time:  4.832530510611832
 63%|██████▎   | 63/100 [57:15<33:09, 53.77s/it]Time:  4.812031989917159
 64%|██████▍   | 64/100 [58:09<32:10, 53.64s/it]Time:  4.856187168508768
 65%|██████▌   | 65/100 [59:03<31:19, 53.69s/it]Time:  4.921485277824104
 66%|██████▌   | 66/100 [59:56<30:23, 53.64s/it]Time:  4.950789101421833
 67%|██████▋   | 67/100 [1:00:50<29:30, 53.64s/it]Time:  4.9455312909558415
 68%|██████▊   | 68/100 [1:01:43<28:34, 53.56s/it]Time:  4.9268462443724275
 69%|██████▉   | 69/100 [1:02:36<27:37, 53.47s/it]Time:  4.889113321900368
 70%|███████   | 70/100 [1:03:30<26:45, 53.52s/it]Time:  4.917947122827172
 71%|███████   | 71/100 [1:04:24<25:52, 53.54s/it]Time:  4.935773801989853
 72%|███████▏  | 72/100 [1:05:17<24:59, 53.54s/it]Time:  4.866467331536114
 73%|███████▎  | 73/100 [1:06:10<24:03, 53.47s/it]Time:  4.956397719681263
 74%|███████▍  | 74/100 [1:07:04<23:09, 53.44s/it]Time:  4.9417134039103985
 75%|███████▌  | 75/100 [1:07:57<22:17, 53.50s/it]Time:  4.9001573557034135
 76%|███████▌  | 76/100 [1:08:51<21:24, 53.51s/it]Time:  4.847869758494198
 77%|███████▋  | 77/100 [1:09:44<20:30, 53.49s/it]Time:  4.88516796939075
 78%|███████▊  | 78/100 [1:10:38<19:35, 53.43s/it]Time:  4.883331025019288
 79%|███████▉  | 79/100 [1:11:31<18:42, 53.44s/it]Time:  4.9250221364200115
 80%|████████  | 80/100 [1:12:24<17:48, 53.41s/it]Time:  4.940668209455907
 81%|████████  | 81/100 [1:13:18<16:55, 53.46s/it]Time:  4.8937928806990385
 82%|████████▏ | 82/100 [1:14:12<16:02, 53.45s/it]Time:  4.948957661166787
 83%|████████▎ | 83/100 [1:15:05<15:09, 53.53s/it]Time:  4.877398148179054
 84%|████████▍ | 84/100 [1:15:59<14:16, 53.50s/it]Time:  4.936176021583378
 85%|████████▌ | 85/100 [1:16:52<13:21, 53.44s/it]Time:  4.900485027581453
 86%|████████▌ | 86/100 [1:17:46<12:29, 53.51s/it]Time:  4.960540248081088
 87%|████████▋ | 87/100 [1:18:39<11:36, 53.56s/it]Time:  4.929649847559631
 88%|████████▊ | 88/100 [1:19:33<10:42, 53.51s/it]Time:  4.90675494633615
 89%|████████▉ | 89/100 [1:20:26<09:48, 53.51s/it]Time:  4.910593482665718
 90%|█████████ | 90/100 [1:21:20<08:55, 53.59s/it]Time:  4.947592759504914
 91%|█████████ | 91/100 [1:22:14<08:02, 53.62s/it]Time:  4.937396142631769
 92%|█████████▏| 92/100 [1:23:07<07:08, 53.62s/it]Time:  4.968378581106663
 93%|█████████▎| 93/100 [1:24:00<06:12, 53.27s/it]Time:  4.899608356878161
 94%|█████████▍| 94/100 [1:24:53<05:19, 53.23s/it]Time:  4.918735749088228
 95%|█████████▌| 95/100 [1:25:46<04:26, 53.23s/it]Time:  4.905435993336141
 96%|█████████▌| 96/100 [1:26:40<03:33, 53.33s/it]Time:  4.9012414729222655
 97%|█████████▋| 97/100 [1:27:34<02:40, 53.51s/it]Time:  4.973919626325369
 98%|█████████▊| 98/100 [1:28:27<01:47, 53.55s/it]Time:  4.9221745654940605
 99%|█████████▉| 99/100 [1:29:21<00:53, 53.58s/it]Time:  2.559364872984588
100%|██████████| 100/100 [1:30:04<00:00, 50.48s/it]100%|██████████| 100/100 [1:30:04<00:00, 54.05s/it]
Post-training time: 5407.34 seconds
GPU memory allocated: 375.42 MB, peak: 999.49 MB
Train results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy   test_time
Init.      0           NaN   2.417717          11.73          50.17  126.416308
Pre-Prune  0           NaN   2.417717          11.73          50.17  126.416308
Post-Prune 0           NaN   2.302121          10.01          50.56    2.679595
Final      100    0.104237   0.641240          87.95          98.96    2.559365
Prune results:
             module   param  sparsity     size             shape     flops  score mean  score variance    score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.614583     1728     (64, 3, 3, 3)   1769472   -0.027050        1.005513   -46.742134        0.805873            0.356814   1.392548e+03      True
1    layers.0.conv    bias  1.000000       64             (64,)     65536    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
2    layers.1.conv  weight  0.631565    36864    (64, 64, 3, 3)  37748736    0.000872        0.996963    32.139240        0.796522            0.362516   2.936300e+04      True
3    layers.1.conv    bias  1.000000       64             (64,)     65536    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
4    layers.3.conv  weight  0.631375    73728   (128, 64, 3, 3)  18874368    0.000958        0.993901    70.645149        0.796130            0.360079   5.869704e+04      True
5    layers.3.conv    bias  1.000000      128            (128,)     32768    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
6    layers.4.conv  weight  0.632385   147456  (128, 128, 3, 3)  37748736    0.001246        0.994699   183.733383        0.795026            0.362635   1.172313e+05      True
7    layers.4.conv    bias  1.000000      128            (128,)     32768    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
8    layers.6.conv  weight  0.629337   294912  (256, 128, 3, 3)  18874368   -0.001563        1.001676  -460.913849        0.798645            0.363844   2.355301e+05      True
9    layers.6.conv    bias  1.000000      256            (256,)     16384    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
10   layers.7.conv  weight  0.631010   589824  (256, 256, 3, 3)  37748736    0.000378        0.999599   222.715530        0.797776            0.363152   4.705474e+05      True
11   layers.7.conv    bias  1.000000      256            (256,)     16384    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
12   layers.8.conv  weight  0.631302   589824  (256, 256, 3, 3)  37748736    0.000403        0.998166   237.463287        0.796769            0.363324   4.699538e+05      True
13   layers.8.conv    bias  1.000000      256            (256,)     16384    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
14  layers.10.conv  weight  0.631446  1179648  (512, 256, 3, 3)  18874368    0.000511        0.999746   602.936768        0.797616            0.363555   9.409056e+05      True
15  layers.10.conv    bias  1.000000      512            (512,)      8192    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
16  layers.11.conv  weight  0.630497  2359296  (512, 512, 3, 3)  37748736   -0.000190        1.001348  -448.645966        0.798652            0.363503   1.884256e+06      True
17  layers.11.conv    bias  1.000000      512            (512,)      8192    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
18  layers.12.conv  weight  0.631314  2359296  (512, 512, 3, 3)  37748736    0.000249        0.999722   588.484009        0.797592            0.363569   1.881756e+06      True
19  layers.12.conv    bias  1.000000      512            (512,)      8192    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
20  layers.14.conv  weight  0.630505  2359296  (512, 512, 3, 3)   9437184   -0.000339        0.999254  -799.927002        0.797659            0.362994   1.881914e+06      True
21  layers.14.conv    bias  1.000000      512            (512,)      2048    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
22  layers.15.conv  weight  0.631101  2359296  (512, 512, 3, 3)   9437184    0.000708        1.000298  1670.380615        0.797886            0.363677   1.882448e+06      True
23  layers.15.conv    bias  1.000000      512            (512,)      2048    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
24  layers.16.conv  weight  0.631115  2359296  (512, 512, 3, 3)   9437184    0.000980        0.999300  2311.431885        0.797918            0.362629   1.882524e+06      True
25  layers.16.conv    bias  1.000000      512            (512,)      2048    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
26              fc  weight  0.637500     5120         (10, 512)      5120    0.007839        0.966788    40.134628        0.782462            0.354602   4.006208e+03      True
27              fc    bias  1.000000       10             (10,)        10    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
Parameter Sparsity: 9289139/14719818 (0.6311)
FLOP Sparsity: 197937438/313478154 (0.6314)
Saving results.
