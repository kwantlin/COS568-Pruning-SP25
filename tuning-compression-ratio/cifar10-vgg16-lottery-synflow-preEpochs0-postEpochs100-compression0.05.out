Namespace(dataset='cifar10', model='vgg16', model_class='lottery', dense_classifier=False, pretrained=False, optimizer='adam', train_batch_size=256, test_batch_size=256, pre_epochs=0, post_epochs=100, lr=0.001, lr_drops=[], lr_drop_rate=0.1, weight_decay=0.0, pruner='synflow', compression=0.05, quantization=False, prune_epochs=1, compression_schedule='exponential', mask_scope='global', prune_dataset_ratio=10, prune_batch_size=256, prune_bias=False, prune_batchnorm=False, prune_residual=False, prune_train_mode=False, reinitialize=False, shuffle=False, invert=False, pruner_list=[], prune_epoch_list=[], compression_list=[], level_list=[], experiment='singleshot', expid='cifar10-vgg16-lottery-synflow-preEpochs0-postEpochs100-compression0.05', result_dir='Results/data', gpu=0, workers=4, no_cuda=False, seed=1, verbose=False)
Loading cifar10 dataset.
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with synflow for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:01<00:00,  1.13s/it]100%|██████████| 1/1 [00:01<00:00,  1.13s/it]
Post-Training for 100 epochs.
  0%|          | 0/100 [00:00<?, ?it/s]Time:  3.187024233979173
  1%|          | 1/100 [00:45<1:14:25, 45.10s/it]Time:  3.1282757529988885
  2%|▏         | 2/100 [01:31<1:15:02, 45.94s/it]Time:  3.3046549510909244
  3%|▎         | 3/100 [02:17<1:14:34, 46.13s/it]Time:  3.379955371026881
  4%|▍         | 4/100 [03:04<1:13:52, 46.17s/it]Time:  3.373120840988122
  5%|▌         | 5/100 [03:50<1:13:08, 46.20s/it]Time:  3.482039005961269
  6%|▌         | 6/100 [04:36<1:12:26, 46.24s/it]Time:  3.4962682179175317
  7%|▋         | 7/100 [05:22<1:11:38, 46.22s/it]Time:  3.5458763579372317
  8%|▊         | 8/100 [06:09<1:10:57, 46.27s/it]Time:  3.4487318630563095
  9%|▉         | 9/100 [06:55<1:10:15, 46.32s/it]Time:  3.457986215944402
 10%|█         | 10/100 [07:41<1:09:21, 46.23s/it]Time:  3.428263047011569
 11%|█         | 11/100 [08:27<1:08:30, 46.18s/it]Time:  4.981418646057136
 12%|█▏        | 12/100 [09:14<1:08:00, 46.37s/it]Time:  3.5804818010656163
 13%|█▎        | 13/100 [10:00<1:07:10, 46.33s/it]Time:  3.463103960035369
 14%|█▍        | 14/100 [10:47<1:06:20, 46.28s/it]Time:  3.464296543970704
 15%|█▌        | 15/100 [11:32<1:05:16, 46.08s/it]Time:  3.30401476402767
 16%|█▌        | 16/100 [12:18<1:04:15, 45.90s/it]Time:  2.9938604859635234
 17%|█▋        | 17/100 [13:02<1:02:56, 45.50s/it]Time:  3.776391168939881
 18%|█▊        | 18/100 [13:47<1:01:43, 45.16s/it]Time:  3.7251998509746045
 19%|█▉        | 19/100 [14:32<1:01:09, 45.31s/it]Time:  3.280366675928235
 20%|██        | 20/100 [15:18<1:00:34, 45.43s/it]Time:  3.0822037749458104
 21%|██        | 21/100 [16:04<59:59, 45.56s/it]  Time:  3.8084026789292693
 22%|██▏       | 22/100 [16:49<58:59, 45.38s/it]Time:  4.11608080402948
 23%|██▎       | 23/100 [17:33<57:40, 44.94s/it]Time:  4.152180662960745
 24%|██▍       | 24/100 [18:16<56:25, 44.54s/it]Time:  5.407301458995789
 25%|██▌       | 25/100 [19:01<55:45, 44.61s/it]Time:  4.063485065009445
 26%|██▌       | 26/100 [19:45<54:55, 44.53s/it]Time:  4.086263497010805
 27%|██▋       | 27/100 [20:30<54:07, 44.49s/it]Time:  4.140737280948088
 28%|██▊       | 28/100 [21:14<53:19, 44.44s/it]Time:  4.118613169994205
 29%|██▉       | 29/100 [21:58<52:25, 44.30s/it]Time:  4.107961738016456
 30%|███       | 30/100 [22:42<51:37, 44.25s/it]Time:  4.119977880967781
 31%|███       | 31/100 [23:26<50:52, 44.24s/it]Time:  4.1044217749731615
 32%|███▏      | 32/100 [24:11<50:11, 44.29s/it]Time:  4.055426832986996
 33%|███▎      | 33/100 [24:55<49:29, 44.32s/it]Time:  4.083438909030519
 34%|███▍      | 34/100 [25:40<48:51, 44.42s/it]Time:  4.110236270935275
 35%|███▌      | 35/100 [26:24<48:09, 44.46s/it]Time:  4.080043163965456
 36%|███▌      | 36/100 [27:09<47:20, 44.38s/it]Time:  4.105892157997005
 37%|███▋      | 37/100 [27:54<46:45, 44.52s/it]Time:  4.090901418006979
 38%|███▊      | 38/100 [28:38<46:05, 44.61s/it]Time:  4.097251996980049
 39%|███▉      | 39/100 [29:23<45:21, 44.62s/it]Time:  4.089305633911863
 40%|████      | 40/100 [30:08<44:37, 44.63s/it]Time:  4.118748936918564
 41%|████      | 41/100 [30:53<43:58, 44.72s/it]Time:  4.126682396046817
 42%|████▏     | 42/100 [31:39<43:40, 45.18s/it]Time:  4.075086703989655
 43%|████▎     | 43/100 [32:24<42:48, 45.06s/it]Time:  4.09783702599816
 44%|████▍     | 44/100 [33:08<41:56, 44.94s/it]Time:  4.062257259036414
 45%|████▌     | 45/100 [33:53<41:07, 44.86s/it]Time:  4.124021910945885
 46%|████▌     | 46/100 [34:38<40:18, 44.79s/it]Time:  4.125002069980837
 47%|████▋     | 47/100 [35:22<39:33, 44.78s/it]Time:  4.096099963993765
 48%|████▊     | 48/100 [36:07<38:46, 44.74s/it]Time:  3.978395988000557
 49%|████▉     | 49/100 [37:00<40:09, 47.24s/it]Time:  3.9483747219201177
 50%|█████     | 50/100 [37:45<38:43, 46.46s/it]Time:  3.9709229569416493
 51%|█████     | 51/100 [38:29<37:29, 45.92s/it]Time:  3.9882581549463794
 52%|█████▏    | 52/100 [39:14<36:25, 45.54s/it]Time:  3.9323106369702145
 53%|█████▎    | 53/100 [39:59<35:29, 45.31s/it]Time:  3.9168489640578628
 54%|█████▍    | 54/100 [40:43<34:32, 45.05s/it]Time:  3.9765549909789115
 55%|█████▌    | 55/100 [41:28<33:42, 44.95s/it]Time:  3.87728784896899
 56%|█████▌    | 56/100 [42:15<33:29, 45.68s/it]Time:  3.881391308037564
 57%|█████▋    | 57/100 [43:00<32:38, 45.54s/it]Time:  3.8992439049761742
 58%|█████▊    | 58/100 [43:46<31:50, 45.49s/it]Time:  3.9426656790310517
 59%|█████▉    | 59/100 [44:31<31:01, 45.41s/it]Time:  3.9994519650936127
 60%|██████    | 60/100 [45:16<30:11, 45.30s/it]Time:  3.867148570017889
 61%|██████    | 61/100 [46:01<29:24, 45.24s/it]Time:  3.937353778979741
 62%|██████▏   | 62/100 [46:46<28:36, 45.18s/it]Time:  3.916099683032371
 63%|██████▎   | 63/100 [47:31<27:51, 45.17s/it]Time:  3.9362200839677826
 64%|██████▍   | 64/100 [48:17<27:05, 45.17s/it]Time:  3.9218030859483406
 65%|██████▌   | 65/100 [49:02<26:22, 45.23s/it]Time:  3.852661781013012
 66%|██████▌   | 66/100 [49:47<25:38, 45.24s/it]Time:  3.9208980259718373
 67%|██████▋   | 67/100 [50:33<24:57, 45.39s/it]Time:  3.908729523071088
 68%|██████▊   | 68/100 [51:19<24:13, 45.43s/it]Time:  3.8982584349578246
 69%|██████▉   | 69/100 [52:04<23:28, 45.43s/it]Time:  3.9401440960355103
 70%|███████   | 70/100 [52:50<22:48, 45.60s/it]Time:  3.9290884570218623
 71%|███████   | 71/100 [53:35<21:59, 45.50s/it]Time:  3.827404937008396
 72%|███████▏  | 72/100 [54:20<21:10, 45.38s/it]Time:  3.9075609679566696
 73%|███████▎  | 73/100 [55:05<20:22, 45.27s/it]Time:  5.182010747026652
 74%|███████▍  | 74/100 [55:52<19:44, 45.55s/it]Time:  3.865708149038255
 75%|███████▌  | 75/100 [56:37<18:54, 45.39s/it]Time:  3.9430655350442976
 76%|███████▌  | 76/100 [57:22<18:08, 45.36s/it]Time:  3.911558051011525
 77%|███████▋  | 77/100 [58:07<17:21, 45.28s/it]Time:  3.86045284406282
 78%|███████▊  | 78/100 [58:52<16:34, 45.18s/it]Time:  3.8453606229741126
 79%|███████▉  | 79/100 [59:37<15:49, 45.21s/it]Time:  3.909460994997062
 80%|████████  | 80/100 [1:00:22<15:04, 45.24s/it]Time:  3.961936421925202
 81%|████████  | 81/100 [1:01:08<14:19, 45.22s/it]Time:  3.8659875630401075
 82%|████████▏ | 82/100 [1:01:52<13:32, 45.11s/it]Time:  3.891001539072022
 83%|████████▎ | 83/100 [1:02:37<12:43, 44.92s/it]Time:  3.9005079839844257
 84%|████████▍ | 84/100 [1:03:21<11:56, 44.79s/it]Time:  3.9556023159530014
 85%|████████▌ | 85/100 [1:04:06<11:11, 44.79s/it]Time:  3.9255407529417425
 86%|████████▌ | 86/100 [1:04:51<10:26, 44.73s/it]Time:  3.9435886909486726
 87%|████████▋ | 87/100 [1:05:35<09:41, 44.72s/it]Time:  3.8992204890819266
 88%|████████▊ | 88/100 [1:06:20<08:57, 44.77s/it]Time:  3.9499692750396207
 89%|████████▉ | 89/100 [1:07:05<08:11, 44.71s/it]Time:  3.9757545930333436
 90%|█████████ | 90/100 [1:07:49<07:26, 44.63s/it]Time:  3.9457073799567297
 91%|█████████ | 91/100 [1:08:36<06:46, 45.22s/it]Time:  3.9505708339856938
 92%|█████████▏| 92/100 [1:09:21<06:00, 45.08s/it]Time:  3.8823809350142255
 93%|█████████▎| 93/100 [1:10:06<05:15, 45.12s/it]Time:  3.8583338001044467
 94%|█████████▍| 94/100 [1:10:51<04:31, 45.21s/it]Time:  3.855396818020381
 95%|█████████▌| 95/100 [1:11:37<03:46, 45.21s/it]Time:  3.9902619359781966
 96%|█████████▌| 96/100 [1:12:21<03:00, 45.00s/it]Time:  3.7384833220858127
 97%|█████████▋| 97/100 [1:13:17<02:24, 48.30s/it]Time:  3.9439292270690203
 98%|█████████▊| 98/100 [1:14:02<01:34, 47.15s/it]Time:  3.2100048819556832
 99%|█████████▉| 99/100 [1:14:44<00:45, 45.65s/it]Time:  1.6225971039384604
100%|██████████| 100/100 [1:15:09<00:00, 39.61s/it]100%|██████████| 100/100 [1:15:09<00:00, 45.10s/it]
Post-training time: 4512.99 seconds
GPU memory allocated: 375.68 MB, peak: 999.49 MB
/n/fs/klips/anaconda3/envs/cos568/lib/python3.10/site-packages/numpy/_core/_methods.py:197: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/n/fs/klips/anaconda3/envs/cos568/lib/python3.10/site-packages/numpy/_core/_methods.py:208: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
Train results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy  test_time
Init.      0           NaN   2.417717          11.73          50.17   5.341486
Pre-Prune  0           NaN   2.417717          11.73          50.17   5.341486
Post-Prune 0           NaN   2.414332          11.66          50.38   3.250641
Final      100    0.084772   0.585733          88.58          99.09   1.622597
Prune results:
             module   param  sparsity     size             shape     flops    score mean  score variance     score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  1.000000     1728     (64, 3, 3, 3)   1769472  1.704298e+19             inf  2.945027e+22    1.704298e+19                 inf   2.945027e+22      True
1    layers.0.conv    bias  1.000000       64             (64,)     65536  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
2    layers.1.conv  weight  0.998264    36864    (64, 64, 3, 3)  37748736  7.988898e+17             inf  2.945027e+22    7.988898e+17                 inf   2.945027e+22      True
3    layers.1.conv    bias  1.000000       64             (64,)     65536  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
4    layers.3.conv  weight  0.996460    73728   (128, 64, 3, 3)  18874368  3.994448e+17             inf  2.945027e+22    3.994448e+17                 inf   2.945027e+22      True
5    layers.3.conv    bias  1.000000      128            (128,)     32768  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
6    layers.4.conv  weight  0.993198   147456  (128, 128, 3, 3)  37748736  1.997224e+17             inf  2.945027e+22    1.997224e+17                 inf   2.945027e+22      True
7    layers.4.conv    bias  1.000000      128            (128,)     32768  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
8    layers.6.conv  weight  0.986152   294912  (256, 128, 3, 3)  18874368  9.986122e+16             inf  2.945027e+22    9.986122e+16                 inf   2.945027e+22      True
9    layers.6.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
10   layers.7.conv  weight  0.972612   589824  (256, 256, 3, 3)  37748736  4.993060e+16             inf  2.945027e+22    4.993060e+16                 inf   2.945027e+22      True
11   layers.7.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
12   layers.8.conv  weight  0.972772   589824  (256, 256, 3, 3)  37748736  4.993061e+16             inf  2.945027e+22    4.993061e+16                 inf   2.945027e+22      True
13   layers.8.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
14  layers.10.conv  weight  0.944177  1179648  (512, 256, 3, 3)  18874368  2.496530e+16             inf  2.945027e+22    2.496530e+16                 inf   2.945027e+22      True
15  layers.10.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
16  layers.11.conv  weight  0.888635  2359296  (512, 512, 3, 3)  37748736  1.248265e+16    9.402646e+31  2.945026e+22    1.248265e+16        9.402646e+31   2.945026e+22      True
17  layers.11.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
18  layers.12.conv  weight  0.888531  2359296  (512, 512, 3, 3)  37748736  1.248265e+16    9.415211e+31  2.945027e+22    1.248265e+16        9.415211e+31   2.945027e+22      True
19  layers.12.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
20  layers.14.conv  weight  0.865655  2359296  (512, 512, 3, 3)   9437184  1.248265e+16             inf  2.945026e+22    1.248265e+16                 inf   2.945026e+22      True
21  layers.14.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
22  layers.15.conv  weight  0.866078  2359296  (512, 512, 3, 3)   9437184  1.248265e+16             inf  2.945026e+22    1.248265e+16                 inf   2.945026e+22      True
23  layers.15.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
24  layers.16.conv  weight  0.856663  2359296  (512, 512, 3, 3)   9437184  1.248265e+16             inf  2.945028e+22    1.248265e+16                 inf   2.945028e+22      True
25  layers.16.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
26              fc  weight  0.999609     5120         (10, 512)      5120  5.752007e+18             inf  2.945027e+22    5.752007e+18                 inf   2.945027e+22      True
27              fc    bias  1.000000       10             (10,)        10  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
Parameter Sparsity: 13119512/14719818 (0.8913)
FLOP Sparsity: 297416253/313478154 (0.9488)
Saving results.
