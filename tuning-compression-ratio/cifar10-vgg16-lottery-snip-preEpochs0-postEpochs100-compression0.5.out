Namespace(dataset='cifar10', model='vgg16', model_class='lottery', dense_classifier=False, pretrained=False, optimizer='adam', train_batch_size=256, test_batch_size=256, pre_epochs=0, post_epochs=100, lr=0.001, lr_drops=[], lr_drop_rate=0.1, weight_decay=0.0, pruner='snip', compression=0.5, quantization=False, prune_epochs=1, compression_schedule='exponential', mask_scope='global', prune_dataset_ratio=10, prune_batch_size=256, prune_bias=False, prune_batchnorm=False, prune_residual=False, prune_train_mode=False, reinitialize=False, shuffle=False, invert=False, pruner_list=[], prune_epoch_list=[], compression_list=[], level_list=[], experiment='singleshot', expid='cifar10-vgg16-lottery-snip-preEpochs0-postEpochs100-compression0.5', result_dir='Results/data', gpu=0, workers=4, no_cuda=False, seed=1, verbose=False)
Loading cifar10 dataset.
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with snip for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:25<00:00, 25.54s/it]100%|██████████| 1/1 [00:25<00:00, 25.54s/it]
Post-Training for 100 epochs.
  0%|          | 0/100 [00:00<?, ?it/s]Time:  2.0078240080038086
  1%|          | 1/100 [00:42<1:09:51, 42.34s/it]Time:  1.8998069979716092
  2%|▏         | 2/100 [01:12<57:19, 35.10s/it]  Time:  1.9614998320466839
  3%|▎         | 3/100 [01:42<53:25, 33.05s/it]Time:  2.000296247017104
  4%|▍         | 4/100 [02:18<54:44, 34.21s/it]Time:  1.8482006739941426
  5%|▌         | 5/100 [02:48<51:43, 32.67s/it]Time:  1.944774793984834
  6%|▌         | 6/100 [03:19<50:15, 32.08s/it]Time:  1.9829230689792894
  7%|▋         | 7/100 [03:50<48:53, 31.54s/it]Time:  1.7888918089447543
  8%|▊         | 8/100 [04:20<47:48, 31.18s/it]Time:  1.7941369260079227
  9%|▉         | 9/100 [04:49<46:12, 30.46s/it]Time:  2.04507911298424
 10%|█         | 10/100 [05:17<44:21, 29.57s/it]Time:  2.2303267220268026
 11%|█         | 11/100 [05:46<43:37, 29.41s/it]Time:  2.2514662360190414
 12%|█▏        | 12/100 [06:14<42:38, 29.07s/it]Time:  2.006078676029574
 13%|█▎        | 13/100 [06:43<42:01, 28.98s/it]Time:  1.957033712998964
 14%|█▍        | 14/100 [07:10<40:53, 28.53s/it]Time:  1.931641353003215
 15%|█▌        | 15/100 [07:38<40:03, 28.27s/it]Time:  1.8570036640157923
 16%|█▌        | 16/100 [08:06<39:31, 28.23s/it]Time:  2.0089178269845434
 17%|█▋        | 17/100 [08:35<39:11, 28.33s/it]Time:  1.9093504729680717
 18%|█▊        | 18/100 [09:03<38:40, 28.30s/it]Time:  1.8316973380278796
 19%|█▉        | 19/100 [09:32<38:34, 28.57s/it]Time:  1.840015163004864
 20%|██        | 20/100 [10:00<37:39, 28.24s/it]Time:  1.8040232950006612
 21%|██        | 21/100 [10:27<36:46, 27.94s/it]Time:  2.0108697970281355
 22%|██▏       | 22/100 [10:54<36:13, 27.86s/it]Time:  1.966481045994442
 23%|██▎       | 23/100 [11:22<35:47, 27.89s/it]Time:  1.9866855510044843
 24%|██▍       | 24/100 [11:51<35:25, 27.97s/it]Time:  1.9581655639922246
 25%|██▌       | 25/100 [12:21<35:46, 28.62s/it]Time:  1.8486539339646697
 26%|██▌       | 26/100 [12:54<37:09, 30.13s/it]Time:  1.3696318449801765
 27%|██▋       | 27/100 [13:23<36:02, 29.62s/it]Time:  2.2254016980296
 28%|██▊       | 28/100 [13:53<35:51, 29.88s/it]Time:  2.2454476910061203
 29%|██▉       | 29/100 [14:23<35:10, 29.73s/it]Time:  2.228306986973621
 30%|███       | 30/100 [14:53<34:54, 29.92s/it]Time:  1.9603613439830951
 31%|███       | 31/100 [15:22<34:03, 29.62s/it]Time:  1.9040056839585304
 32%|███▏      | 32/100 [15:52<33:53, 29.90s/it]Time:  1.9481293529970571
 33%|███▎      | 33/100 [16:20<32:41, 29.28s/it]Time:  1.7755273560178466
 34%|███▍      | 34/100 [16:48<31:38, 28.77s/it]Time:  1.8033472480019554
 35%|███▌      | 35/100 [17:15<30:47, 28.42s/it]Time:  1.7079015219933353
 36%|███▌      | 36/100 [17:43<30:03, 28.18s/it]Time:  1.69149691198254
 37%|███▋      | 37/100 [18:11<29:35, 28.18s/it]Time:  1.4083044749568217
 38%|███▊      | 38/100 [18:38<28:46, 27.85s/it]Time:  2.255269071960356
 39%|███▉      | 39/100 [19:06<28:11, 27.73s/it]Time:  2.2532792999991216
 40%|████      | 40/100 [19:33<27:24, 27.41s/it]Time:  2.254696296004113
 41%|████      | 41/100 [20:01<27:07, 27.59s/it]Time:  2.2087156450143084
 42%|████▏     | 42/100 [20:29<26:49, 27.75s/it]Time:  1.5492949340259656
 43%|████▎     | 43/100 [21:03<28:17, 29.78s/it]Time:  2.1899355949717574
 44%|████▍     | 44/100 [21:33<27:46, 29.76s/it]Time:  2.1928904639789835
 45%|████▌     | 45/100 [22:04<27:42, 30.22s/it]Time:  2.213797330041416
 46%|████▌     | 46/100 [22:36<27:35, 30.66s/it]Time:  1.9684814710053615
 47%|████▋     | 47/100 [23:07<27:11, 30.79s/it]Time:  2.023611844982952
 48%|████▊     | 48/100 [23:41<27:27, 31.68s/it]Time:  1.959622413967736
 49%|████▉     | 49/100 [24:11<26:28, 31.14s/it]Time:  2.0265499870292842
 50%|█████     | 50/100 [24:43<26:23, 31.67s/it]Time:  2.202563661034219
 51%|█████     | 51/100 [25:13<25:15, 30.93s/it]Time:  1.9798924319911748
 52%|█████▏    | 52/100 [25:45<25:11, 31.49s/it]Time:  1.7906219399883412
 53%|█████▎    | 53/100 [26:18<24:48, 31.67s/it]Time:  1.4643341980408877
 54%|█████▍    | 54/100 [26:44<23:07, 30.16s/it]Time:  2.035207007022109
 55%|█████▌    | 55/100 [27:16<23:03, 30.73s/it]Time:  2.004230018996168
 56%|█████▌    | 56/100 [27:48<22:45, 31.02s/it]Time:  1.8803507609991357
 57%|█████▋    | 57/100 [28:20<22:27, 31.35s/it]Time:  1.9536178770358674
 58%|█████▊    | 58/100 [28:48<21:07, 30.18s/it]Time:  1.8898401320329867
 59%|█████▉    | 59/100 [29:16<20:16, 29.67s/it]Time:  1.8184425350045785
 60%|██████    | 60/100 [29:48<20:15, 30.38s/it]Time:  1.6440081560285762
 61%|██████    | 61/100 [30:19<19:46, 30.43s/it]Time:  1.8173120919964276
 62%|██████▏   | 62/100 [30:47<18:56, 29.92s/it]Time:  1.5698631799896248
 63%|██████▎   | 63/100 [31:17<18:18, 29.70s/it]Time:  1.826758130977396
 64%|██████▍   | 64/100 [31:46<17:46, 29.64s/it]Time:  1.7598843689775094
 65%|██████▌   | 65/100 [32:15<17:08, 29.37s/it]Time:  1.8255688249482773
 66%|██████▌   | 66/100 [32:45<16:48, 29.67s/it]Time:  1.4420993360108696
 67%|██████▋   | 67/100 [33:18<16:51, 30.66s/it]Time:  1.0417138460325077
 68%|██████▊   | 68/100 [33:48<16:15, 30.49s/it]Time:  1.630248808010947
 69%|██████▉   | 69/100 [34:19<15:49, 30.64s/it]Time:  1.9106381930178031
 70%|███████   | 70/100 [34:47<14:51, 29.72s/it]Time:  1.8347602499998175
 71%|███████   | 71/100 [35:16<14:15, 29.49s/it]Time:  1.8142203870229423
 72%|███████▏  | 72/100 [35:45<13:46, 29.53s/it]Time:  1.4344291309826076
 73%|███████▎  | 73/100 [36:13<12:58, 28.83s/it]Time:  1.7745642210356891
 74%|███████▍  | 74/100 [36:41<12:27, 28.74s/it]Time:  2.065577869012486
 75%|███████▌  | 75/100 [37:09<11:53, 28.54s/it]Time:  2.2011611529742368
 76%|███████▌  | 76/100 [37:39<11:31, 28.82s/it]Time:  2.175602108996827
 77%|███████▋  | 77/100 [38:06<10:54, 28.45s/it]Time:  2.078152048983611
 78%|███████▊  | 78/100 [38:33<10:17, 28.09s/it]Time:  1.9924466410302557
 79%|███████▉  | 79/100 [39:00<09:41, 27.68s/it]Time:  1.9408309180289507
 80%|████████  | 80/100 [39:27<09:11, 27.57s/it]Time:  1.9212519599823281
 81%|████████  | 81/100 [39:55<08:40, 27.41s/it]Time:  2.1906907010124996
 82%|████████▏ | 82/100 [40:22<08:15, 27.56s/it]Time:  2.0754813990206458
 83%|████████▎ | 83/100 [40:50<07:50, 27.70s/it]Time:  1.8004365820088424
 84%|████████▍ | 84/100 [41:18<07:22, 27.68s/it]Time:  1.7454040219890885
 85%|████████▌ | 85/100 [41:47<07:01, 28.10s/it]Time:  1.9116595219820738
 86%|████████▌ | 86/100 [42:16<06:37, 28.39s/it]Time:  2.0028848109650426
 87%|████████▋ | 87/100 [42:44<06:05, 28.14s/it]Time:  1.812683399009984
 88%|████████▊ | 88/100 [43:14<05:44, 28.73s/it]Time:  2.0284532750374638
 89%|████████▉ | 89/100 [43:41<05:11, 28.36s/it]Time:  1.892570245952811
 90%|█████████ | 90/100 [44:09<04:41, 28.16s/it]Time:  1.7357351240352727
 91%|█████████ | 91/100 [44:37<04:12, 28.10s/it]Time:  1.7741716409800574
 92%|█████████▏| 92/100 [45:04<03:42, 27.87s/it]Time:  1.784340901998803
 93%|█████████▎| 93/100 [45:32<03:13, 27.67s/it]Time:  1.893915017019026
 94%|█████████▍| 94/100 [46:03<02:53, 28.91s/it]Time:  1.5339336350443773
 95%|█████████▌| 95/100 [46:31<02:22, 28.59s/it]Time:  1.9295019169803709
 96%|█████████▌| 96/100 [47:03<01:57, 29.42s/it]Time:  1.883675643010065
 97%|█████████▋| 97/100 [47:33<01:29, 29.71s/it]Time:  1.6399945300072432
 98%|█████████▊| 98/100 [48:05<01:01, 30.52s/it]Time:  1.2959778109798208
 99%|█████████▉| 99/100 [48:24<00:26, 26.91s/it]Time:  0.7639957120409235
100%|██████████| 100/100 [48:34<00:00, 21.74s/it]100%|██████████| 100/100 [48:34<00:00, 29.14s/it]
Post-training time: 2917.15 seconds
GPU memory allocated: 435.85 MB, peak: 1060.06 MB
Train results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy   test_time
Init.      0           NaN   2.417717          11.73          50.17  127.927985
Pre-Prune  0           NaN   2.417717          11.73          50.17  127.927985
Post-Prune 0           NaN   2.445838          10.00          50.89    1.348074
Final      100    0.084338   0.536644          88.36          98.95    0.763996
Prune results:
             module   param  sparsity     size             shape     flops    score mean  score variance  score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.964120     1728     (64, 3, 3, 3)   1769472  2.224966e-06    6.371801e-12   0.003845    2.224966e-06        6.371801e-12       0.003845      True
1    layers.0.conv    bias  1.000000       64             (64,)     65536  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
2    layers.1.conv  weight  0.792915    36864    (64, 64, 3, 3)  37748736  3.802294e-07    3.769638e-13   0.014017    3.802294e-07        3.769638e-13       0.014017      True
3    layers.1.conv    bias  1.000000       64             (64,)     65536  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
4    layers.3.conv  weight  0.687378    73728   (128, 64, 3, 3)  18874368  3.019000e-07    2.926917e-13   0.022258    3.019000e-07        2.926917e-13       0.022258      True
5    layers.3.conv    bias  1.000000      128            (128,)     32768  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
6    layers.4.conv  weight  0.550808   147456  (128, 128, 3, 3)  37748736  1.766793e-07    1.293368e-13   0.026052    1.766793e-07        1.293368e-13       0.026052      True
7    layers.4.conv    bias  1.000000      128            (128,)     32768  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
8    layers.6.conv  weight  0.488990   294912  (256, 128, 3, 3)  18874368  1.371369e-07    8.621578e-14   0.040443    1.371369e-07        8.621578e-14       0.040443      True
9    layers.6.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
10   layers.7.conv  weight  0.395096   589824  (256, 256, 3, 3)  37748736  9.237761e-08    4.426949e-14   0.054487    9.237761e-08        4.426949e-14       0.054487      True
11   layers.7.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
12   layers.8.conv  weight  0.429765   589824  (256, 256, 3, 3)  37748736  9.919984e-08    4.516633e-14   0.058510    9.919984e-08        4.516633e-14       0.058510      True
13   layers.8.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
14  layers.10.conv  weight  0.410134  1179648  (512, 256, 3, 3)  18874368  8.549011e-08    3.122918e-14   0.100848    8.549011e-08        3.122918e-14       0.100848      True
15  layers.10.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
16  layers.11.conv  weight  0.304324  2359296  (512, 512, 3, 3)  37748736  5.522888e-08    1.596758e-14   0.130301    5.522888e-08        1.596758e-14       0.130301      True
17  layers.11.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
18  layers.12.conv  weight  0.317955  2359296  (512, 512, 3, 3)  37748736  5.628561e-08    1.532595e-14   0.132794    5.628561e-08        1.532595e-14       0.132794      True
19  layers.12.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
20  layers.14.conv  weight  0.318180  2359296  (512, 512, 3, 3)   9437184  6.324614e-08    2.105084e-14   0.149216    6.324614e-08        2.105084e-14       0.149216      True
21  layers.14.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
22  layers.15.conv  weight  0.242648  2359296  (512, 512, 3, 3)   9437184  5.294718e-08    2.291931e-14   0.124918    5.294718e-08        2.291931e-14       0.124918      True
23  layers.15.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
24  layers.16.conv  weight  0.245948  2359296  (512, 512, 3, 3)   9437184  5.487940e-08    2.587618e-14   0.129477    5.487940e-08        2.587618e-14       0.129477      True
25  layers.16.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
26              fc  weight  0.892969     5120         (10, 512)      5120  2.506261e-06    1.778373e-11   0.012832    2.506261e-06        1.778373e-11       0.012832      True
27              fc    bias  1.000000       10             (10,)        10  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
Parameter Sparsity: 4657711/14719818 (0.3164)
FLOP Sparsity: 144896542/313478154 (0.4622)
Saving results.
