Namespace(dataset='cifar10', model='vgg16', model_class='lottery', dense_classifier=False, pretrained=False, optimizer='adam', train_batch_size=256, test_batch_size=256, pre_epochs=0, post_epochs=100, lr=0.001, lr_drops=[], lr_drop_rate=0.1, weight_decay=0.0, pruner='snip', compression=0.2, quantization=False, prune_epochs=1, compression_schedule='exponential', mask_scope='global', prune_dataset_ratio=10, prune_batch_size=256, prune_bias=False, prune_batchnorm=False, prune_residual=False, prune_train_mode=False, reinitialize=False, shuffle=False, invert=False, pruner_list=[], prune_epoch_list=[], compression_list=[], level_list=[], experiment='singleshot', expid='cifar10-vgg16-lottery-snip-preEpochs0-postEpochs100-compression0.2', result_dir='Results/data', gpu=0, workers=4, no_cuda=False, seed=1, verbose=False)
Loading cifar10 dataset.
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with snip for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:25<00:00, 25.54s/it]100%|██████████| 1/1 [00:25<00:00, 25.54s/it]
Post-Training for 100 epochs.
  0%|          | 0/100 [00:00<?, ?it/s]Time:  2.0078986920416355
  1%|          | 1/100 [00:44<1:13:44, 44.69s/it]Time:  1.8230627129669301
  2%|▏         | 2/100 [01:14<58:54, 36.07s/it]  Time:  1.8948895600042306
  3%|▎         | 3/100 [01:45<54:14, 33.55s/it]Time:  1.968196565983817
  4%|▍         | 4/100 [02:21<55:16, 34.55s/it]Time:  1.9599297050153837
  5%|▌         | 5/100 [02:51<52:10, 32.96s/it]Time:  1.9023988849949092
  6%|▌         | 6/100 [03:22<50:27, 32.20s/it]Time:  2.036130554974079
  7%|▋         | 7/100 [03:52<49:06, 31.68s/it]Time:  1.872283794975374
  8%|▊         | 8/100 [04:23<47:51, 31.22s/it]Time:  1.9534703430254012
  9%|▉         | 9/100 [04:52<46:43, 30.81s/it]Time:  1.5722096699755639
 10%|█         | 10/100 [05:21<45:09, 30.10s/it]Time:  1.0852548370021395
 11%|█         | 11/100 [05:50<44:14, 29.83s/it]Time:  0.8491366449743509
 12%|█▏        | 12/100 [06:17<42:20, 28.87s/it]Time:  1.7666103960364126
 13%|█▎        | 13/100 [06:45<41:44, 28.78s/it]Time:  1.7425737990415655
 14%|█▍        | 14/100 [07:13<40:41, 28.39s/it]Time:  1.789172122022137
 15%|█▌        | 15/100 [07:41<39:53, 28.16s/it]Time:  1.9243100190069526
 16%|█▌        | 16/100 [08:08<39:18, 28.07s/it]Time:  2.050833464018069
 17%|█▋        | 17/100 [08:37<39:05, 28.25s/it]Time:  1.92929242097307
 18%|█▊        | 18/100 [09:05<38:33, 28.21s/it]Time:  1.9456916220369749
 19%|█▉        | 19/100 [09:35<38:32, 28.55s/it]Time:  1.8158426550216973
 20%|██        | 20/100 [10:02<37:40, 28.26s/it]Time:  1.901739807042759
 21%|██        | 21/100 [10:30<36:56, 28.05s/it]Time:  1.7972032710094936
 22%|██▏       | 22/100 [10:57<36:15, 27.89s/it]Time:  1.888824535999447
 23%|██▎       | 23/100 [11:25<35:45, 27.87s/it]Time:  1.967460755025968
 24%|██▍       | 24/100 [11:53<35:22, 27.92s/it]Time:  1.8529126279754564
 25%|██▌       | 25/100 [12:23<35:40, 28.54s/it]Time:  1.7961552320048213
 26%|██▌       | 26/100 [12:57<37:05, 30.08s/it]Time:  1.3998102429904975
 27%|██▋       | 27/100 [13:26<36:07, 29.69s/it]Time:  2.0686634989688173
 28%|██▊       | 28/100 [13:56<35:51, 29.88s/it]Time:  2.07723558798898
 29%|██▉       | 29/100 [14:25<35:08, 29.69s/it]Time:  2.139585689001251
 30%|███       | 30/100 [14:55<34:49, 29.85s/it]Time:  1.960014701995533
 31%|███       | 31/100 [15:24<34:04, 29.63s/it]Time:  1.851124114007689
 32%|███▏      | 32/100 [15:55<33:50, 29.86s/it]Time:  1.8285526849795133
 33%|███▎      | 33/100 [16:22<32:30, 29.11s/it]Time:  1.9794025459559634
 34%|███▍      | 34/100 [16:50<31:26, 28.58s/it]Time:  1.8431681850343011
 35%|███▌      | 35/100 [17:17<30:40, 28.32s/it]Time:  1.9022713450249285
 36%|███▌      | 36/100 [17:45<29:57, 28.09s/it]Time:  2.161639947036747
 37%|███▋      | 37/100 [18:12<29:11, 27.81s/it]Time:  2.4346718770102598
 38%|███▊      | 38/100 [18:39<28:22, 27.46s/it]Time:  2.430516002001241
 39%|███▉      | 39/100 [19:04<27:13, 26.79s/it]Time:  2.4690781790413894
 40%|████      | 40/100 [19:29<26:22, 26.38s/it]Time:  2.4285472839837894
 41%|████      | 41/100 [19:56<25:57, 26.39s/it]Time:  2.4595276349573396
 42%|████▏     | 42/100 [20:21<25:04, 25.95s/it]Time:  2.4291328160325065
 43%|████▎     | 43/100 [20:52<26:13, 27.60s/it]Time:  0.8270531800226308
 44%|████▍     | 44/100 [21:10<23:03, 24.71s/it]Time:  0.7506133639835753
 45%|████▌     | 45/100 [21:38<23:31, 25.66s/it]Time:  0.8987669390044175
 46%|████▌     | 46/100 [22:08<24:23, 27.10s/it]Time:  0.8856908790185116
 47%|████▋     | 47/100 [22:38<24:42, 27.98s/it]Time:  1.947819254011847
 48%|████▊     | 48/100 [23:10<25:11, 29.07s/it]Time:  1.7146287629730068
 49%|████▉     | 49/100 [23:43<25:45, 30.31s/it]Time:  1.9728506690007634
 50%|█████     | 50/100 [24:13<25:07, 30.15s/it]Time:  2.013190764992032
 51%|█████     | 51/100 [24:46<25:18, 30.98s/it]Time:  2.193331387999933
 52%|█████▏    | 52/100 [25:15<24:16, 30.35s/it]Time:  2.226145750959404
 53%|█████▎    | 53/100 [25:48<24:25, 31.18s/it]Time:  1.8913825550116599
 54%|█████▍    | 54/100 [26:20<24:01, 31.35s/it]Time:  1.708823441003915
 55%|█████▌    | 55/100 [26:46<22:30, 30.00s/it]Time:  2.045840483973734
 56%|█████▌    | 56/100 [27:19<22:29, 30.66s/it]Time:  1.998066769039724
 57%|█████▋    | 57/100 [27:50<22:11, 30.97s/it]Time:  1.8669426929554902
 58%|█████▊    | 58/100 [28:22<21:55, 31.31s/it]Time:  1.9916553429793566
 59%|█████▉    | 59/100 [28:50<20:31, 30.04s/it]Time:  1.8090340370545164
 60%|██████    | 60/100 [29:18<19:43, 29.59s/it]Time:  1.8795194329577498
 61%|██████    | 61/100 [29:50<19:41, 30.30s/it]Time:  2.003648083016742
 62%|██████▏   | 62/100 [30:21<19:15, 30.40s/it]Time:  1.8869986819918267
 63%|██████▎   | 63/100 [30:49<18:21, 29.77s/it]Time:  2.0617965279961936
 64%|██████▍   | 64/100 [31:19<17:50, 29.74s/it]Time:  1.9466692210407928
 65%|██████▌   | 65/100 [31:48<17:15, 29.60s/it]Time:  1.9603386289672926
 66%|██████▌   | 66/100 [32:17<16:40, 29.42s/it]Time:  1.941778167965822
 67%|██████▋   | 67/100 [32:47<16:14, 29.54s/it]Time:  2.1253646350232884
 68%|██████▊   | 68/100 [33:19<16:09, 30.29s/it]Time:  2.3333343589911237
 69%|██████▉   | 69/100 [33:50<15:48, 30.61s/it]Time:  2.048566846002359
 70%|███████   | 70/100 [34:21<15:24, 30.83s/it]Time:  1.979768919001799
 71%|███████   | 71/100 [34:49<14:26, 29.87s/it]Time:  1.7876568969804794
 72%|███████▏  | 72/100 [35:18<13:48, 29.61s/it]Time:  1.7937417869688943
 73%|███████▎  | 73/100 [35:48<13:19, 29.61s/it]Time:  1.3151196599937975
 74%|███████▍  | 74/100 [36:15<12:31, 28.89s/it]Time:  1.707866484997794
 75%|███████▌  | 75/100 [36:43<11:55, 28.61s/it]Time:  2.216575716040097
 76%|███████▌  | 76/100 [37:11<11:25, 28.54s/it]Time:  2.1950532459886745
 77%|███████▋  | 77/100 [37:41<11:01, 28.77s/it]Time:  2.2168555239913985
 78%|███████▊  | 78/100 [38:08<10:24, 28.37s/it]Time:  2.205619206011761
 79%|███████▉  | 79/100 [38:35<09:46, 27.94s/it]Time:  2.2885956009849906
 80%|████████  | 80/100 [39:02<09:11, 27.56s/it]Time:  2.2656698360224254
 81%|████████  | 81/100 [39:29<08:45, 27.65s/it]Time:  2.204884161998052
 82%|████████▏ | 82/100 [39:57<08:15, 27.51s/it]Time:  2.178536549967248
 83%|████████▎ | 83/100 [40:25<07:49, 27.64s/it]Time:  2.08164193702396
 84%|████████▍ | 84/100 [40:53<07:23, 27.73s/it]Time:  2.1404991659801453
 85%|████████▌ | 85/100 [41:20<06:54, 27.66s/it]Time:  2.1249288450344466
 86%|████████▌ | 86/100 [41:49<06:33, 28.14s/it]Time:  2.01637500297511
 87%|████████▋ | 87/100 [42:19<06:10, 28.49s/it]Time:  1.9926264869864099
 88%|████████▊ | 88/100 [42:46<05:38, 28.25s/it]Time:  1.9641379219829105
 89%|████████▉ | 89/100 [43:16<05:16, 28.77s/it]Time:  1.8943221059744246
 90%|█████████ | 90/100 [43:43<04:42, 28.28s/it]Time:  1.9411883699940518
 91%|█████████ | 91/100 [44:11<04:12, 28.03s/it]Time:  1.982792569033336
 92%|█████████▏| 92/100 [44:39<03:44, 28.02s/it]Time:  1.9440046260133386
 93%|█████████▎| 93/100 [45:06<03:14, 27.71s/it]Time:  2.032807518960908
 94%|█████████▍| 94/100 [45:34<02:46, 27.69s/it]Time:  1.9022954539977945
 95%|█████████▌| 95/100 [46:05<02:24, 28.90s/it]Time:  2.006745591002982
 96%|█████████▌| 96/100 [46:33<01:54, 28.68s/it]Time:  1.869297594006639
 97%|█████████▋| 97/100 [47:05<01:28, 29.50s/it]Time:  1.8754324999754317
 98%|█████████▊| 98/100 [47:35<00:59, 29.70s/it]Time:  2.0398115429561585
 99%|█████████▉| 99/100 [48:08<00:30, 30.60s/it]Time:  1.2568488919641823
100%|██████████| 100/100 [48:26<00:00, 26.96s/it]100%|██████████| 100/100 [48:26<00:00, 29.07s/it]
Post-training time: 2907.36 seconds
GPU memory allocated: 435.85 MB, peak: 1060.06 MB
Train results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy   test_time
Init.      0           NaN   2.417717          11.73          50.17  127.929903
Pre-Prune  0           NaN   2.417717          11.73          50.17  127.929903
Post-Prune 0           NaN   2.424375           9.28          49.80    0.700901
Final      100    0.090646   0.598155          87.83          99.03    1.256849
Prune results:
             module   param  sparsity     size             shape     flops    score mean  score variance  score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.993634     1728     (64, 3, 3, 3)   1769472  2.224966e-06    6.371802e-12   0.003845    2.224966e-06        6.371802e-12       0.003845      True
1    layers.0.conv    bias  1.000000       64             (64,)     65536  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
2    layers.1.conv  weight  0.968370    36864    (64, 64, 3, 3)  37748736  3.802294e-07    3.769638e-13   0.014017    3.802294e-07        3.769638e-13       0.014017      True
3    layers.1.conv    bias  1.000000       64             (64,)     65536  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
4    layers.3.conv  weight  0.929715    73728   (128, 64, 3, 3)  18874368  3.019000e-07    2.926917e-13   0.022258    3.019000e-07        2.926917e-13       0.022258      True
5    layers.3.conv    bias  1.000000      128            (128,)     32768  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
6    layers.4.conv  weight  0.869127   147456  (128, 128, 3, 3)  37748736  1.766793e-07    1.293368e-13   0.026052    1.766793e-07        1.293368e-13       0.026052      True
7    layers.4.conv    bias  1.000000      128            (128,)     32768  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
8    layers.6.conv  weight  0.837650   294912  (256, 128, 3, 3)  18874368  1.371369e-07    8.621578e-14   0.040443    1.371369e-07        8.621578e-14       0.040443      True
9    layers.6.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
10   layers.7.conv  weight  0.768828   589824  (256, 256, 3, 3)  37748736  9.237761e-08    4.426949e-14   0.054487    9.237761e-08        4.426949e-14       0.054487      True
11   layers.7.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
12   layers.8.conv  weight  0.791177   589824  (256, 256, 3, 3)  37748736  9.919984e-08    4.516633e-14   0.058510    9.919984e-08        4.516633e-14       0.058510      True
13   layers.8.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
14  layers.10.conv  weight  0.775078  1179648  (512, 256, 3, 3)  18874368  8.549011e-08    3.122918e-14   0.100848    8.549011e-08        3.122918e-14       0.100848      True
15  layers.10.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
16  layers.11.conv  weight  0.676945  2359296  (512, 512, 3, 3)  37748736  5.522888e-08    1.596758e-14   0.130301    5.522888e-08        1.596758e-14       0.130301      True
17  layers.11.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
18  layers.12.conv  weight  0.693201  2359296  (512, 512, 3, 3)  37748736  5.628561e-08    1.532595e-14   0.132794    5.628561e-08        1.532595e-14       0.132794      True
19  layers.12.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
20  layers.14.conv  weight  0.619005  2359296  (512, 512, 3, 3)   9437184  6.324614e-08    2.105084e-14   0.149216    6.324614e-08        2.105084e-14       0.149216      True
21  layers.14.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
22  layers.15.conv  weight  0.483118  2359296  (512, 512, 3, 3)   9437184  5.294718e-08    2.291931e-14   0.124918    5.294718e-08        2.291931e-14       0.124918      True
23  layers.15.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
24  layers.16.conv  weight  0.479594  2359296  (512, 512, 3, 3)   9437184  5.487940e-08    2.587618e-14   0.129477    5.487940e-08        2.587618e-14       0.129477      True
25  layers.16.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
26              fc  weight  0.974023     5120         (10, 512)      5120  2.506261e-06    1.778373e-11   0.012832    2.506261e-06        1.778373e-11       0.012832      True
27              fc    bias  1.000000       10             (10,)        10  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
Parameter Sparsity: 9289139/14719818 (0.6311)
FLOP Sparsity: 244926306/313478154 (0.7813)
Saving results.
