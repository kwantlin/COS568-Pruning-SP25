Namespace(dataset='cifar10', model='vgg16', model_class='lottery', dense_classifier=False, pretrained=False, optimizer='adam', train_batch_size=256, test_batch_size=256, pre_epochs=0, post_epochs=100, lr=0.001, lr_drops=[], lr_drop_rate=0.1, weight_decay=0.0, pruner='rand', compression=0.5, quantization=False, prune_epochs=1, compression_schedule='exponential', mask_scope='global', prune_dataset_ratio=10, prune_batch_size=256, prune_bias=False, prune_batchnorm=False, prune_residual=False, prune_train_mode=False, reinitialize=False, shuffle=False, invert=False, pruner_list=[], prune_epoch_list=[], compression_list=[], level_list=[], experiment='singleshot', expid='cifar10-vgg16-lottery-rand-preEpochs0-postEpochs100-compression0.5', result_dir='Results/data', gpu=0, workers=4, no_cuda=False, seed=1, verbose=False)
Loading cifar10 dataset.
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with rand for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:09<00:00,  9.62s/it]100%|██████████| 1/1 [00:09<00:00,  9.63s/it]
Post-Training for 100 epochs.
  0%|          | 0/100 [00:00<?, ?it/s]Time:  3.899525249376893
  1%|          | 1/100 [01:12<1:59:20, 72.33s/it]Time:  3.9104661075398326
  2%|▏         | 2/100 [02:07<1:41:35, 62.20s/it]Time:  3.369228125549853
  3%|▎         | 3/100 [03:02<1:35:17, 58.94s/it]Time:  3.689974196255207
  4%|▍         | 4/100 [03:58<1:32:11, 57.62s/it]Time:  3.317557484842837
  5%|▌         | 5/100 [04:52<1:29:27, 56.50s/it]Time:  3.905969883315265
  6%|▌         | 6/100 [05:47<1:27:46, 56.02s/it]Time:  4.661503481678665
  7%|▋         | 7/100 [06:43<1:26:32, 55.83s/it]Time:  3.758837128058076
  8%|▊         | 8/100 [07:37<1:24:55, 55.39s/it]Time:  4.355917566455901
  9%|▉         | 9/100 [08:32<1:23:41, 55.18s/it]Time:  4.367821770720184
 10%|█         | 10/100 [09:27<1:22:43, 55.15s/it]Time:  4.606829224154353
 11%|█         | 11/100 [10:22<1:21:44, 55.11s/it]Time:  4.388104123994708
 12%|█▏        | 12/100 [11:17<1:20:39, 55.00s/it]Time:  4.358686379157007
 13%|█▎        | 13/100 [12:11<1:19:33, 54.87s/it]Time:  4.287709238938987
 14%|█▍        | 14/100 [13:10<1:20:13, 55.97s/it]Time:  4.494573058560491
 15%|█▌        | 15/100 [14:05<1:18:52, 55.68s/it]Time:  4.509309401735663
 16%|█▌        | 16/100 [14:59<1:17:31, 55.38s/it]Time:  4.437223078683019
 17%|█▋        | 17/100 [15:54<1:16:15, 55.13s/it]Time:  4.4509271774441
 18%|█▊        | 18/100 [16:49<1:15:11, 55.01s/it]Time:  4.489645164459944
 19%|█▉        | 19/100 [17:43<1:13:57, 54.78s/it]Time:  3.974389818497002
 20%|██        | 20/100 [18:38<1:13:20, 55.01s/it]Time:  4.153498310595751
 21%|██        | 21/100 [19:32<1:11:51, 54.58s/it]Time:  4.5281527722254395
 22%|██▏       | 22/100 [20:26<1:10:46, 54.45s/it]Time:  4.341765476390719
 23%|██▎       | 23/100 [21:20<1:09:41, 54.30s/it]Time:  3.820754347369075
 24%|██▍       | 24/100 [22:16<1:09:25, 54.81s/it]Time:  4.96264885365963
 25%|██▌       | 25/100 [23:10<1:08:16, 54.61s/it]Time:  3.443562409840524
 26%|██▌       | 26/100 [24:07<1:08:03, 55.18s/it]Time:  3.7314118249341846
 27%|██▋       | 27/100 [25:02<1:07:05, 55.15s/it]Time:  4.055845611728728
 28%|██▊       | 28/100 [25:58<1:06:22, 55.32s/it]Time:  3.737071927636862
 29%|██▉       | 29/100 [26:54<1:05:57, 55.75s/it]Time:  4.72719600610435
 30%|███       | 30/100 [27:47<1:04:00, 54.87s/it]Time:  4.743394883349538
 31%|███       | 31/100 [28:41<1:02:44, 54.56s/it]Time:  4.87272110953927
 32%|███▏      | 32/100 [29:35<1:01:46, 54.51s/it]Time:  4.79249963350594
 33%|███▎      | 33/100 [30:29<1:00:41, 54.35s/it]Time:  5.1717679761350155
 34%|███▍      | 34/100 [31:24<59:48, 54.38s/it]  Time:  4.9382844641804695
 35%|███▌      | 35/100 [32:18<58:49, 54.29s/it]Time:  4.421870772726834
 36%|███▌      | 36/100 [33:12<57:45, 54.15s/it]Time:  4.553638255223632
 37%|███▋      | 37/100 [34:06<56:51, 54.16s/it]Time:  4.585255267098546
 38%|███▊      | 38/100 [35:00<56:01, 54.23s/it]Time:  4.662293606437743
 39%|███▉      | 39/100 [35:55<55:13, 54.32s/it]Time:  4.954798634164035
 40%|████      | 40/100 [36:49<54:18, 54.30s/it]Time:  4.8975562416017056
 41%|████      | 41/100 [37:44<53:26, 54.35s/it]Time:  4.752309833653271
 42%|████▏     | 42/100 [38:37<52:24, 54.21s/it]Time:  4.819391271099448
 43%|████▎     | 43/100 [39:31<51:27, 54.16s/it]Time:  4.7955944472923875
 44%|████▍     | 44/100 [40:26<50:34, 54.19s/it]Time:  4.729456172324717
 45%|████▌     | 45/100 [41:20<49:38, 54.15s/it]Time:  4.719182706438005
 46%|████▌     | 46/100 [42:14<48:41, 54.10s/it]Time:  4.747133142314851
 47%|████▋     | 47/100 [43:08<47:44, 54.04s/it]Time:  4.766204514540732
 48%|████▊     | 48/100 [44:02<46:50, 54.05s/it]Time:  4.77805036213249
 49%|████▉     | 49/100 [44:56<45:53, 53.99s/it]Time:  4.86717270873487
 50%|█████     | 50/100 [45:50<45:01, 54.02s/it]Time:  4.8447599690407515
 51%|█████     | 51/100 [46:44<44:09, 54.07s/it]Time:  4.813052169978619
 52%|█████▏    | 52/100 [47:38<43:13, 54.04s/it]Time:  4.850405057892203
 53%|█████▎    | 53/100 [48:32<42:23, 54.11s/it]Time:  4.824477561749518
 54%|█████▍    | 54/100 [49:26<41:31, 54.16s/it]Time:  4.752569630742073
 55%|█████▌    | 55/100 [50:21<40:36, 54.14s/it]Time:  4.755459958687425
 56%|█████▌    | 56/100 [51:15<39:46, 54.24s/it]Time:  4.717260244302452
 57%|█████▋    | 57/100 [52:09<38:52, 54.24s/it]Time:  4.754592298530042
 58%|█████▊    | 58/100 [53:03<37:54, 54.15s/it]Time:  4.738000921905041
 59%|█████▉    | 59/100 [53:57<36:57, 54.10s/it]Time:  4.782986649312079
 60%|██████    | 60/100 [54:51<36:04, 54.11s/it]Time:  4.756219081580639
 61%|██████    | 61/100 [55:45<35:09, 54.10s/it]Time:  4.800123957917094
 62%|██████▏   | 62/100 [56:39<34:15, 54.10s/it]Time:  4.751136774197221
 63%|██████▎   | 63/100 [57:34<33:24, 54.17s/it]Time:  4.822351871989667
 64%|██████▍   | 64/100 [58:28<32:26, 54.06s/it]Time:  4.808539904654026
 65%|██████▌   | 65/100 [59:22<31:33, 54.10s/it]Time:  4.749381492845714
 66%|██████▌   | 66/100 [1:00:16<30:38, 54.09s/it]Time:  4.699829067103565
 67%|██████▋   | 67/100 [1:01:10<29:46, 54.13s/it]Time:  4.753224895335734
 68%|██████▊   | 68/100 [1:02:04<28:51, 54.11s/it]Time:  4.736525481566787
 69%|██████▉   | 69/100 [1:02:59<28:01, 54.23s/it]Time:  4.665119543671608
 70%|███████   | 70/100 [1:03:53<27:05, 54.18s/it]Time:  4.719065765850246
 71%|███████   | 71/100 [1:04:47<26:11, 54.20s/it]Time:  4.6762364730238914
 72%|███████▏  | 72/100 [1:05:41<25:17, 54.18s/it]Time:  4.692916157655418
 73%|███████▎  | 73/100 [1:06:35<24:22, 54.16s/it]Time:  4.697553395293653
 74%|███████▍  | 74/100 [1:07:29<23:27, 54.13s/it]Time:  4.721015647053719
 75%|███████▌  | 75/100 [1:08:23<22:34, 54.16s/it]Time:  4.690493521280587
 76%|███████▌  | 76/100 [1:09:18<21:40, 54.18s/it]Time:  4.647131602279842
 77%|███████▋  | 77/100 [1:10:12<20:45, 54.14s/it]Time:  4.78232846967876
 78%|███████▊  | 78/100 [1:11:06<19:52, 54.22s/it]Time:  4.711148548871279
 79%|███████▉  | 79/100 [1:12:01<19:00, 54.32s/it]Time:  4.715877033770084
 80%|████████  | 80/100 [1:12:55<18:05, 54.30s/it]Time:  4.676779026165605
 81%|████████  | 81/100 [1:13:49<17:11, 54.27s/it]Time:  4.68019191082567
 82%|████████▏ | 82/100 [1:14:44<16:18, 54.35s/it]Time:  4.6733614364638925
 83%|████████▎ | 83/100 [1:15:38<15:22, 54.27s/it]Time:  4.693571302108467
 84%|████████▍ | 84/100 [1:16:32<14:26, 54.16s/it]Time:  4.675886012613773
 85%|████████▌ | 85/100 [1:17:26<13:32, 54.18s/it]Time:  4.677875816822052
 86%|████████▌ | 86/100 [1:18:20<12:37, 54.11s/it]Time:  4.719302695244551
 87%|████████▋ | 87/100 [1:19:14<11:43, 54.14s/it]Time:  4.7715968722477555
 88%|████████▊ | 88/100 [1:20:08<10:49, 54.15s/it]Time:  4.7823466109111905
 89%|████████▉ | 89/100 [1:21:02<09:55, 54.15s/it]Time:  4.766962798312306
 90%|█████████ | 90/100 [1:21:57<09:01, 54.19s/it]Time:  4.714525202289224
 91%|█████████ | 91/100 [1:22:51<08:07, 54.20s/it]Time:  4.702417473308742
 92%|█████████▏| 92/100 [1:23:48<07:20, 55.01s/it]Time:  4.444277998991311
 93%|█████████▎| 93/100 [1:24:43<06:24, 54.93s/it]Time:  4.439265132881701
 94%|█████████▍| 94/100 [1:25:37<05:29, 54.90s/it]Time:  4.425328958779573
 95%|█████████▌| 95/100 [1:26:32<04:34, 54.87s/it]Time:  4.29504532366991
 96%|█████████▌| 96/100 [1:27:26<03:38, 54.66s/it]Time:  4.2832139721140265
 97%|█████████▋| 97/100 [1:28:21<02:43, 54.55s/it]Time:  4.287476171739399
 98%|█████████▊| 98/100 [1:29:15<01:48, 54.46s/it]Time:  3.127555060200393
 99%|█████████▉| 99/100 [1:30:01<00:52, 52.09s/it]Time:  0.7353273909538984
100%|██████████| 100/100 [1:30:14<00:00, 40.25s/it]100%|██████████| 100/100 [1:30:14<00:00, 54.15s/it]
Post-training time: 5420.50 seconds
GPU memory allocated: 375.42 MB, peak: 999.49 MB
Train results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy   test_time
Init.      0           NaN   2.417717          11.73          50.17  126.413039
Pre-Prune  0           NaN   2.417717          11.73          50.17  126.413039
Post-Prune 0           NaN   2.302584          10.00          50.56    2.686851
Final      100    0.069791   0.703253          87.35          99.13    0.735327
Prune results:
             module   param  sparsity     size             shape     flops  score mean  score variance    score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.305556     1728     (64, 3, 3, 3)   1769472   -0.027050        1.005513   -46.742134        0.805873            0.356814   1.392548e+03      True
1    layers.0.conv    bias  1.000000       64             (64,)     65536    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
2    layers.1.conv  weight  0.314236    36864    (64, 64, 3, 3)  37748736    0.000872        0.996963    32.139240        0.796522            0.362516   2.936300e+04      True
3    layers.1.conv    bias  1.000000       64             (64,)     65536    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
4    layers.3.conv  weight  0.316447    73728   (128, 64, 3, 3)  18874368    0.000958        0.993901    70.645149        0.796130            0.360079   5.869704e+04      True
5    layers.3.conv    bias  1.000000      128            (128,)     32768    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
6    layers.4.conv  weight  0.316352   147456  (128, 128, 3, 3)  37748736    0.001246        0.994699   183.733383        0.795026            0.362635   1.172313e+05      True
7    layers.4.conv    bias  1.000000      128            (128,)     32768    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
8    layers.6.conv  weight  0.316254   294912  (256, 128, 3, 3)  18874368   -0.001563        1.001676  -460.913849        0.798645            0.363844   2.355301e+05      True
9    layers.6.conv    bias  1.000000      256            (256,)     16384    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
10   layers.7.conv  weight  0.316233   589824  (256, 256, 3, 3)  37748736    0.000378        0.999599   222.715530        0.797776            0.363152   4.705474e+05      True
11   layers.7.conv    bias  1.000000      256            (256,)     16384    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
12   layers.8.conv  weight  0.315952   589824  (256, 256, 3, 3)  37748736    0.000403        0.998166   237.463287        0.796769            0.363324   4.699538e+05      True
13   layers.8.conv    bias  1.000000      256            (256,)     16384    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
14  layers.10.conv  weight  0.316293  1179648  (512, 256, 3, 3)  18874368    0.000511        0.999746   602.936768        0.797616            0.363555   9.409056e+05      True
15  layers.10.conv    bias  1.000000      512            (512,)      8192    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
16  layers.11.conv  weight  0.316343  2359296  (512, 512, 3, 3)  37748736   -0.000190        1.001348  -448.645966        0.798652            0.363503   1.884256e+06      True
17  layers.11.conv    bias  1.000000      512            (512,)      8192    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
18  layers.12.conv  weight  0.315997  2359296  (512, 512, 3, 3)  37748736    0.000249        0.999722   588.484009        0.797592            0.363569   1.881756e+06      True
19  layers.12.conv    bias  1.000000      512            (512,)      8192    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
20  layers.14.conv  weight  0.316019  2359296  (512, 512, 3, 3)   9437184   -0.000339        0.999254  -799.927002        0.797659            0.362994   1.881914e+06      True
21  layers.14.conv    bias  1.000000      512            (512,)      2048    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
22  layers.15.conv  weight  0.316295  2359296  (512, 512, 3, 3)   9437184    0.000708        1.000298  1670.380615        0.797886            0.363677   1.882448e+06      True
23  layers.15.conv    bias  1.000000      512            (512,)      2048    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
24  layers.16.conv  weight  0.316552  2359296  (512, 512, 3, 3)   9437184    0.000980        0.999300  2311.431885        0.797918            0.362629   1.882524e+06      True
25  layers.16.conv    bias  1.000000      512            (512,)      2048    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
26              fc  weight  0.311133     5120         (10, 512)      5120    0.007839        0.966788    40.134628        0.782462            0.354602   4.006208e+03      True
27              fc    bias  1.000000       10             (10,)        10    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
Parameter Sparsity: 4657711/14719818 (0.3164)
FLOP Sparsity: 99223159/313478154 (0.3165)
Saving results.
