Namespace(dataset='cifar10', model='vgg16', model_class='lottery', dense_classifier=False, pretrained=False, optimizer='adam', train_batch_size=256, test_batch_size=256, pre_epochs=0, post_epochs=100, lr=0.001, lr_drops=[], lr_drop_rate=0.1, weight_decay=0.0, pruner='rand', compression=2.0, quantization=True, prune_epochs=1, compression_schedule='exponential', mask_scope='global', prune_dataset_ratio=10, prune_batch_size=256, prune_bias=False, prune_batchnorm=False, prune_residual=False, prune_train_mode=False, reinitialize=False, shuffle=False, invert=False, pruner_list=[], prune_epoch_list=[], compression_list=[], level_list=[], experiment='singleshot', expid='QUANTIZED-cifar10-vgg16-lottery-rand-preEpochs0-postEpochs100-compression2', result_dir='Results/data', gpu=0, workers=4, no_cuda=False, seed=1, verbose=False)
Loading cifar10 dataset.
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with rand for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.74s/it]100%|██████████| 1/1 [00:02<00:00,  2.74s/it]
Post-Training for 100 epochs.
  0%|          | 0/100 [00:00<?, ?it/s]Time:  3.838413462974131
  1%|          | 1/100 [01:01<1:41:54, 61.77s/it]Time:  3.676608309033327
  2%|▏         | 2/100 [01:36<1:14:39, 45.71s/it]Time:  3.622421805979684
  3%|▎         | 3/100 [02:10<1:05:19, 40.41s/it]Time:  3.2403312399983406
  4%|▍         | 4/100 [02:43<59:51, 37.41s/it]  Time:  4.209113886929117
  5%|▌         | 5/100 [03:15<56:13, 35.51s/it]Time:  4.4848973110783845
  6%|▌         | 6/100 [03:46<53:18, 34.03s/it]Time:  4.4992639580741525
  7%|▋         | 7/100 [04:16<50:53, 32.83s/it]Time:  4.664847604930401
  8%|▊         | 8/100 [04:47<49:17, 32.15s/it]Time:  4.57348130189348
  9%|▉         | 9/100 [05:20<49:00, 32.31s/it]Time:  4.629362368956208
 10%|█         | 10/100 [05:51<48:04, 32.05s/it]Time:  4.366250980994664
 11%|█         | 11/100 [06:22<47:02, 31.72s/it]Time:  4.68434611405246
 12%|█▏        | 12/100 [06:54<46:37, 31.79s/it]Time:  4.75145674101077
 13%|█▎        | 13/100 [07:26<46:09, 31.84s/it]Time:  4.391442519961856
 14%|█▍        | 14/100 [07:58<45:43, 31.91s/it]Time:  4.282818021019921
 15%|█▌        | 15/100 [08:30<45:12, 31.91s/it]Time:  3.980440824991092
 16%|█▌        | 16/100 [09:05<45:50, 32.75s/it]Time:  4.573555562994443
 17%|█▋        | 17/100 [09:37<45:06, 32.60s/it]Time:  4.297905322047882
 18%|█▊        | 18/100 [10:09<44:18, 32.42s/it]Time:  4.283618237008341
 19%|█▉        | 19/100 [10:41<43:37, 32.32s/it]Time:  4.274115297012031
 20%|██        | 20/100 [11:13<43:01, 32.27s/it]Time:  4.229637343087234
 21%|██        | 21/100 [11:45<42:23, 32.19s/it]Time:  4.207892590085976
 22%|██▏       | 22/100 [12:18<41:53, 32.23s/it]Time:  4.29565690504387
 23%|██▎       | 23/100 [12:50<41:26, 32.30s/it]Time:  4.283729596063495
 24%|██▍       | 24/100 [13:23<41:13, 32.54s/it]Time:  4.360357954050414
 25%|██▌       | 25/100 [13:58<41:39, 33.33s/it]Time:  4.357502242084593
 26%|██▌       | 26/100 [14:31<40:45, 33.05s/it]Time:  4.337412324966863
 27%|██▋       | 27/100 [15:03<40:06, 32.96s/it]Time:  3.8636082620359957
 28%|██▊       | 28/100 [15:35<39:00, 32.51s/it]Time:  3.8845065829809755
 29%|██▉       | 29/100 [16:07<38:18, 32.38s/it]Time:  3.5092108689714223
 30%|███       | 30/100 [16:40<38:02, 32.61s/it]Time:  3.760116360965185
 31%|███       | 31/100 [17:14<37:53, 32.95s/it]Time:  3.961249421001412
 32%|███▏      | 32/100 [17:49<37:57, 33.49s/it]Time:  4.2261290539754555
 33%|███▎      | 33/100 [18:23<37:40, 33.74s/it]Time:  4.031822892953642
 34%|███▍      | 34/100 [19:01<38:28, 34.98s/it]Time:  4.131111026974395
 35%|███▌      | 35/100 [19:33<36:55, 34.08s/it]Time:  3.956401292933151
 36%|███▌      | 36/100 [20:10<37:22, 35.03s/it]Time:  4.15791306796018
 37%|███▋      | 37/100 [20:44<36:21, 34.62s/it]Time:  3.9559415070107207
 38%|███▊      | 38/100 [21:17<35:23, 34.25s/it]Time:  3.972590720979497
 39%|███▉      | 39/100 [21:50<34:20, 33.78s/it]Time:  3.679002242977731
 40%|████      | 40/100 [22:22<33:24, 33.41s/it]Time:  4.054391467943788
 41%|████      | 41/100 [22:54<32:26, 32.99s/it]Time:  4.08290392998606
 42%|████▏     | 42/100 [23:26<31:39, 32.76s/it]Time:  4.418743117013946
 43%|████▎     | 43/100 [23:58<30:42, 32.32s/it]Time:  4.36972578195855
 44%|████▍     | 44/100 [24:29<29:52, 32.00s/it]Time:  4.356324948021211
 45%|████▌     | 45/100 [25:01<29:24, 32.08s/it]Time:  4.364411292015575
 46%|████▌     | 46/100 [25:35<29:17, 32.55s/it]Time:  4.320086705032736
 47%|████▋     | 47/100 [26:07<28:36, 32.39s/it]Time:  4.325631308951415
 48%|████▊     | 48/100 [26:40<28:11, 32.53s/it]Time:  4.273467696970329
 49%|████▉     | 49/100 [27:12<27:27, 32.30s/it]Time:  4.342402291949838
 50%|█████     | 50/100 [27:40<25:56, 31.14s/it]Time:  4.345439616008662
 51%|█████     | 51/100 [28:13<25:46, 31.57s/it]Time:  4.347336333943531
 52%|█████▏    | 52/100 [28:45<25:23, 31.73s/it]Time:  4.358821268077008
 53%|█████▎    | 53/100 [29:17<25:00, 31.92s/it]Time:  4.390014678006992
 54%|█████▍    | 54/100 [29:49<24:30, 31.96s/it]Time:  4.355573824956082
 55%|█████▌    | 55/100 [30:21<23:56, 31.92s/it]Time:  4.38003201095853
 56%|█████▌    | 56/100 [30:53<23:23, 31.91s/it]Time:  4.384974781889468
 57%|█████▋    | 57/100 [31:24<22:45, 31.75s/it]Time:  4.378421535948291
 58%|█████▊    | 58/100 [31:57<22:23, 31.98s/it]Time:  4.211954909958877
 59%|█████▉    | 59/100 [32:30<22:02, 32.26s/it]Time:  4.206630885018967
 60%|██████    | 60/100 [33:02<21:35, 32.40s/it]Time:  3.919249804923311
 61%|██████    | 61/100 [33:35<21:05, 32.45s/it]Time:  3.51724460197147
 62%|██████▏   | 62/100 [34:07<20:30, 32.38s/it]Time:  3.3350708530051634
 63%|██████▎   | 63/100 [34:39<19:51, 32.22s/it]Time:  3.5737361310748383
 64%|██████▍   | 64/100 [35:12<19:24, 32.35s/it]Time:  4.08563257008791
 65%|██████▌   | 65/100 [35:45<19:01, 32.62s/it]Time:  4.072664964012802
 66%|██████▌   | 66/100 [36:19<18:42, 33.02s/it]Time:  4.058154007070698
 67%|██████▋   | 67/100 [36:53<18:21, 33.39s/it]Time:  3.8513719600159675
 68%|██████▊   | 68/100 [37:26<17:43, 33.25s/it]Time:  3.9199818240012974
 69%|██████▉   | 69/100 [37:59<17:09, 33.21s/it]Time:  3.741968978079967
 70%|███████   | 70/100 [38:31<16:28, 32.94s/it]Time:  3.875361085985787
 71%|███████   | 71/100 [39:04<15:49, 32.75s/it]Time:  4.000484839081764
 72%|███████▏  | 72/100 [39:37<15:23, 33.00s/it]Time:  3.9519345510052517
 73%|███████▎  | 73/100 [40:10<14:50, 32.99s/it]Time:  3.825572115019895
 74%|███████▍  | 74/100 [40:42<14:09, 32.67s/it]Time:  4.287417137995362
 75%|███████▌  | 75/100 [41:13<13:19, 32.00s/it]Time:  4.316486603929661
 76%|███████▌  | 76/100 [41:44<12:46, 31.92s/it]Time:  4.355479169054888
 77%|███████▋  | 77/100 [42:16<12:14, 31.93s/it]Time:  4.328521059011109
 78%|███████▊  | 78/100 [42:49<11:45, 32.09s/it]Time:  4.351264244993217
 79%|███████▉  | 79/100 [43:21<11:14, 32.10s/it]Time:  4.35872597200796
 80%|████████  | 80/100 [43:53<10:44, 32.23s/it]Time:  4.3311620720196515
 81%|████████  | 81/100 [44:27<10:18, 32.55s/it]Time:  4.3485234919935465
 82%|████████▏ | 82/100 [44:59<09:42, 32.34s/it]Time:  4.400228755082935
 83%|████████▎ | 83/100 [45:28<08:52, 31.30s/it]Time:  4.3732029049424455
 84%|████████▍ | 84/100 [45:58<08:17, 31.12s/it]Time:  4.393937099957839
 85%|████████▌ | 85/100 [46:29<07:45, 31.06s/it]Time:  4.355673409998417
 86%|████████▌ | 86/100 [47:04<07:29, 32.09s/it]Time:  1.3688066540053114
 87%|████████▋ | 87/100 [47:29<06:29, 30.00s/it]Time:  3.1073707150062546
 88%|████████▊ | 88/100 [48:03<06:14, 31.21s/it]Time:  3.5960042470833287
 89%|████████▉ | 89/100 [48:38<05:55, 32.32s/it]Time:  3.833587698987685
 90%|█████████ | 90/100 [49:13<05:31, 33.14s/it]Time:  3.627058251062408
 91%|█████████ | 91/100 [49:47<05:00, 33.43s/it]Time:  3.423659613938071
 92%|█████████▏| 92/100 [50:21<04:28, 33.58s/it]Time:  3.530306651024148
 93%|█████████▎| 93/100 [50:56<03:57, 33.95s/it]Time:  3.53585458395537
 94%|█████████▍| 94/100 [51:31<03:26, 34.37s/it]Time:  3.961142368032597
 95%|█████████▌| 95/100 [52:06<02:52, 34.51s/it]Time:  3.9416034809546545
 96%|█████████▌| 96/100 [52:40<02:17, 34.38s/it]Time:  3.7900737749878317
 97%|█████████▋| 97/100 [53:14<01:42, 34.25s/it]Time:  3.7465000440133736
 98%|█████████▊| 98/100 [53:49<01:09, 34.58s/it]Time:  4.080173136899248
 99%|█████████▉| 99/100 [54:25<00:34, 34.99s/it]Time:  4.311062430031598
100%|██████████| 100/100 [54:58<00:00, 34.47s/it]100%|██████████| 100/100 [54:58<00:00, 32.99s/it]
Post-training time: 3302.67 seconds
GPU memory allocated: 376.33 MB, peak: 876.67 MB
Train results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy  test_time
Init.      0           NaN   2.417717          11.73          50.17  45.441681
Pre-Prune  0           NaN   2.417717          11.73          50.17  45.441681
Post-Prune 0           NaN   2.302585          10.45          49.06   3.808610
Final      100    2.302695   2.302588          10.00          50.00   4.311062
Prune results:
             module   param  sparsity     size             shape     flops  score mean  score variance    score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.011574     1728     (64, 3, 3, 3)   1769472   -0.027050        1.005513   -46.742134        0.805873            0.356814   1.392548e+03      True
1    layers.0.conv    bias  1.000000       64             (64,)     65536    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
2    layers.1.conv  weight  0.009494    36864    (64, 64, 3, 3)  37748736    0.000872        0.996963    32.139240        0.796522            0.362516   2.936300e+04      True
3    layers.1.conv    bias  1.000000       64             (64,)     65536    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
4    layers.3.conv  weight  0.010335    73728   (128, 64, 3, 3)  18874368    0.000958        0.993901    70.645149        0.796130            0.360079   5.869704e+04      True
5    layers.3.conv    bias  1.000000      128            (128,)     32768    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
6    layers.4.conv  weight  0.009881   147456  (128, 128, 3, 3)  37748736    0.001246        0.994699   183.733383        0.795026            0.362635   1.172313e+05      True
7    layers.4.conv    bias  1.000000      128            (128,)     32768    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
8    layers.6.conv  weight  0.010067   294912  (256, 128, 3, 3)  18874368   -0.001563        1.001676  -460.913849        0.798645            0.363844   2.355301e+05      True
9    layers.6.conv    bias  1.000000      256            (256,)     16384    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
10   layers.7.conv  weight  0.009972   589824  (256, 256, 3, 3)  37748736    0.000378        0.999599   222.715530        0.797776            0.363152   4.705474e+05      True
11   layers.7.conv    bias  1.000000      256            (256,)     16384    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
12   layers.8.conv  weight  0.010044   589824  (256, 256, 3, 3)  37748736    0.000403        0.998166   237.463287        0.796769            0.363324   4.699538e+05      True
13   layers.8.conv    bias  1.000000      256            (256,)     16384    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
14  layers.10.conv  weight  0.009916  1179648  (512, 256, 3, 3)  18874368    0.000511        0.999746   602.936768        0.797616            0.363555   9.409056e+05      True
15  layers.10.conv    bias  1.000000      512            (512,)      8192    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
16  layers.11.conv  weight  0.009981  2359296  (512, 512, 3, 3)  37748736   -0.000190        1.001348  -448.645966        0.798652            0.363503   1.884256e+06      True
17  layers.11.conv    bias  1.000000      512            (512,)      8192    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
18  layers.12.conv  weight  0.010007  2359296  (512, 512, 3, 3)  37748736    0.000249        0.999722   588.484009        0.797592            0.363569   1.881756e+06      True
19  layers.12.conv    bias  1.000000      512            (512,)      8192    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
20  layers.14.conv  weight  0.009933  2359296  (512, 512, 3, 3)   9437184   -0.000339        0.999254  -799.927002        0.797659            0.362994   1.881914e+06      True
21  layers.14.conv    bias  1.000000      512            (512,)      2048    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
22  layers.15.conv  weight  0.010127  2359296  (512, 512, 3, 3)   9437184    0.000708        1.000298  1670.380615        0.797886            0.363677   1.882448e+06      True
23  layers.15.conv    bias  1.000000      512            (512,)      2048    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
24  layers.16.conv  weight  0.009989  2359296  (512, 512, 3, 3)   9437184    0.000980        0.999300  2311.431885        0.797918            0.362629   1.882524e+06      True
25  layers.16.conv    bias  1.000000      512            (512,)      2048    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
26              fc  weight  0.008008     5120         (10, 512)      5120    0.007839        0.966788    40.134628        0.782462            0.354602   4.006208e+03      True
27              fc    bias  1.000000       10             (10,)        10    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
Parameter Sparsity: 151390/14719818 (0.0103)
FLOP Sparsity: 3394350/313478154 (0.0108)
Saving results.
