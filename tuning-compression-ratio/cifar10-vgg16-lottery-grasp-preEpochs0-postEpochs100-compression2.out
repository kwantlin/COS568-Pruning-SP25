Namespace(dataset='cifar10', model='vgg16', model_class='lottery', dense_classifier=False, pretrained=False, optimizer='adam', train_batch_size=256, test_batch_size=256, pre_epochs=0, post_epochs=100, lr=0.001, lr_drops=[], lr_drop_rate=0.1, weight_decay=0.0, pruner='grasp', compression=2.0, quantization=False, prune_epochs=1, compression_schedule='exponential', mask_scope='global', prune_dataset_ratio=10, prune_batch_size=256, prune_bias=False, prune_batchnorm=False, prune_residual=False, prune_train_mode=False, reinitialize=False, shuffle=False, invert=False, pruner_list=[], prune_epoch_list=[], compression_list=[], level_list=[], experiment='singleshot', expid='cifar10-vgg16-lottery-grasp-preEpochs0-postEpochs100-compression2', result_dir='Results/data', gpu=0, workers=4, no_cuda=False, seed=1, verbose=False)
Loading cifar10 dataset.
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with grasp for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:35<00:00, 35.65s/it]100%|██████████| 1/1 [00:35<00:00, 35.65s/it]
Post-Training for 100 epochs.
  0%|          | 0/100 [00:00<?, ?it/s]Time:  3.81522907782346
  1%|          | 1/100 [00:55<1:30:53, 55.08s/it]Time:  3.874369617551565
  2%|▏         | 2/100 [01:50<1:30:34, 55.46s/it]Time:  4.04455204680562
  3%|▎         | 3/100 [02:46<1:29:51, 55.58s/it]Time:  4.147244704887271
  4%|▍         | 4/100 [03:41<1:28:24, 55.25s/it]Time:  4.439673561602831
  5%|▌         | 5/100 [04:35<1:26:55, 54.90s/it]Time:  4.708592999726534
  6%|▌         | 6/100 [05:29<1:25:44, 54.73s/it]Time:  4.69416693598032
  7%|▋         | 7/100 [06:24<1:24:37, 54.59s/it]Time:  4.798800909891725
  8%|▊         | 8/100 [07:18<1:23:33, 54.50s/it]Time:  4.880530619062483
  9%|▉         | 9/100 [08:12<1:22:14, 54.23s/it]Time:  4.902822948992252
 10%|█         | 10/100 [09:06<1:21:30, 54.34s/it]Time:  4.890421339310706
 11%|█         | 11/100 [10:00<1:20:28, 54.25s/it]Time:  4.967205697670579
 12%|█▏        | 12/100 [10:54<1:19:07, 53.95s/it]Time:  4.980967138893902
 13%|█▎        | 13/100 [11:47<1:18:01, 53.81s/it]Time:  4.9475153079256415
 14%|█▍        | 14/100 [12:46<1:19:11, 55.25s/it]Time:  4.92212472204119
 15%|█▌        | 15/100 [13:40<1:17:44, 54.87s/it]Time:  4.98859632294625
 16%|█▌        | 16/100 [14:33<1:16:16, 54.48s/it]Time:  5.025268712081015
 17%|█▋        | 17/100 [15:27<1:14:55, 54.16s/it]Time:  4.97793840803206
 18%|█▊        | 18/100 [16:20<1:13:44, 53.96s/it]Time:  4.963578332215548
 19%|█▉        | 19/100 [17:14<1:12:45, 53.89s/it]Time:  4.933999019674957
 20%|██        | 20/100 [18:06<1:11:13, 53.42s/it]Time:  4.987656244076788
 21%|██        | 21/100 [18:58<1:09:46, 52.99s/it]Time:  4.974420611746609
 22%|██▏       | 22/100 [19:51<1:08:54, 53.01s/it]Time:  4.909790040925145
 23%|██▎       | 23/100 [20:44<1:07:57, 52.95s/it]Time:  4.9464306784793735
 24%|██▍       | 24/100 [21:37<1:07:12, 53.06s/it]Time:  4.891964182257652
 25%|██▌       | 25/100 [22:30<1:06:09, 52.93s/it]Time:  4.11721661593765
 26%|██▌       | 26/100 [23:24<1:05:33, 53.16s/it]Time:  4.539056558161974
 27%|██▋       | 27/100 [24:16<1:04:27, 52.98s/it]Time:  4.754308422096074
 28%|██▊       | 28/100 [25:12<1:04:38, 53.87s/it]Time:  4.7747096829116344
 29%|██▉       | 29/100 [26:05<1:03:20, 53.53s/it]Time:  4.777472994290292
 30%|███       | 30/100 [26:57<1:01:58, 53.12s/it]Time:  4.845831589773297
 31%|███       | 31/100 [27:51<1:01:13, 53.23s/it]Time:  4.883848247118294
 32%|███▏      | 32/100 [28:45<1:00:39, 53.52s/it]Time:  4.925842524506152
 33%|███▎      | 33/100 [29:38<59:43, 53.48s/it]  Time:  4.952957743778825
 34%|███▍      | 34/100 [30:32<58:49, 53.47s/it]Time:  4.915124889463186
 35%|███▌      | 35/100 [31:25<57:59, 53.53s/it]Time:  5.025529952719808
 36%|███▌      | 36/100 [32:19<57:15, 53.67s/it]Time:  5.023792691528797
 37%|███▋      | 37/100 [33:14<56:34, 53.89s/it]Time:  4.49919078592211
 38%|███▊      | 38/100 [34:07<55:37, 53.84s/it]Time:  4.415237779729068
 39%|███▉      | 39/100 [35:01<54:46, 53.87s/it]Time:  4.529240264557302
 40%|████      | 40/100 [35:56<53:57, 53.96s/it]Time:  4.454413955099881
 41%|████      | 41/100 [36:50<53:08, 54.05s/it]Time:  4.431230062618852
 42%|████▏     | 42/100 [37:44<52:16, 54.08s/it]Time:  4.615119685418904
 43%|████▎     | 43/100 [38:39<51:33, 54.27s/it]Time:  4.523147428408265
 44%|████▍     | 44/100 [39:33<50:32, 54.15s/it]Time:  4.465228107757866
 45%|████▌     | 45/100 [40:27<49:36, 54.12s/it]Time:  4.663720679469407
 46%|████▌     | 46/100 [41:21<48:49, 54.25s/it]Time:  4.723782273940742
 47%|████▋     | 47/100 [42:15<47:56, 54.27s/it]Time:  4.643825899809599
 48%|████▊     | 48/100 [43:10<47:08, 54.39s/it]Time:  4.7804042575880885
 49%|████▉     | 49/100 [44:04<46:09, 54.30s/it]Time:  4.809962971135974
 50%|█████     | 50/100 [44:59<45:16, 54.33s/it]Time:  4.815233769826591
 51%|█████     | 51/100 [45:53<44:16, 54.21s/it]Time:  4.817405235022306
 52%|█████▏    | 52/100 [46:47<43:19, 54.15s/it]Time:  4.9515395406633615
 53%|█████▎    | 53/100 [47:40<42:21, 54.07s/it]Time:  4.940489985048771
 54%|█████▍    | 54/100 [48:35<41:29, 54.12s/it]Time:  5.171991398558021
 55%|█████▌    | 55/100 [49:30<40:45, 54.34s/it]Time:  5.135376467369497
 56%|█████▌    | 56/100 [50:24<39:54, 54.42s/it]Time:  4.80868064891547
 57%|█████▋    | 57/100 [51:19<39:01, 54.45s/it]Time:  4.582389736548066
 58%|█████▊    | 58/100 [52:13<38:02, 54.35s/it]Time:  4.412920619361103
 59%|█████▉    | 59/100 [53:07<37:05, 54.28s/it]Time:  4.37722359970212
 60%|██████    | 60/100 [54:01<36:08, 54.20s/it]Time:  4.4060042863711715
 61%|██████    | 61/100 [54:55<35:15, 54.24s/it]Time:  4.476434936746955
 62%|██████▏   | 62/100 [55:50<34:22, 54.29s/it]Time:  4.5390803990885615
 63%|██████▎   | 63/100 [56:44<33:30, 54.33s/it]Time:  4.488362370990217
 64%|██████▍   | 64/100 [57:38<32:32, 54.23s/it]Time:  4.559602327644825
 65%|██████▌   | 65/100 [58:32<31:37, 54.21s/it]Time:  4.691772462800145
 66%|██████▌   | 66/100 [59:27<30:44, 54.26s/it]Time:  4.728159996680915
 67%|██████▋   | 67/100 [1:00:21<29:47, 54.16s/it]Time:  4.633236667141318
 68%|██████▊   | 68/100 [1:01:15<28:51, 54.12s/it]Time:  4.689271967858076
 69%|██████▉   | 69/100 [1:02:08<27:53, 53.99s/it]Time:  4.640805626288056
 70%|███████   | 70/100 [1:03:02<26:59, 53.99s/it]Time:  4.728279712609947
 71%|███████   | 71/100 [1:03:57<26:09, 54.11s/it]Time:  4.756565256044269
 72%|███████▏  | 72/100 [1:04:51<25:19, 54.26s/it]Time:  4.780798370949924
 73%|███████▎  | 73/100 [1:05:46<24:27, 54.34s/it]Time:  4.706420561298728
 74%|███████▍  | 74/100 [1:06:40<23:32, 54.33s/it]Time:  4.794459459371865
 75%|███████▌  | 75/100 [1:07:34<22:35, 54.21s/it]Time:  4.75535190384835
 76%|███████▌  | 76/100 [1:08:28<21:39, 54.15s/it]Time:  4.832961610518396
 77%|███████▋  | 77/100 [1:09:22<20:43, 54.07s/it]Time:  4.894773765467107
 78%|███████▊  | 78/100 [1:10:16<19:49, 54.05s/it]Time:  4.842392913997173
 79%|███████▉  | 79/100 [1:11:10<18:54, 54.04s/it]Time:  4.876348274759948
 80%|████████  | 80/100 [1:12:04<17:59, 53.98s/it]Time:  4.918960129842162
 81%|████████  | 81/100 [1:12:58<17:07, 54.08s/it]Time:  4.97776878811419
 82%|████████▏ | 82/100 [1:13:52<16:13, 54.06s/it]Time:  4.9479389460757375
 83%|████████▎ | 83/100 [1:14:46<15:18, 54.02s/it]Time:  4.996745862998068
 84%|████████▍ | 84/100 [1:15:40<14:24, 54.05s/it]Time:  4.900785913690925
 85%|████████▌ | 85/100 [1:16:34<13:29, 53.95s/it]Time:  4.992708253674209
 86%|████████▌ | 86/100 [1:17:28<12:36, 54.05s/it]Time:  5.001179068349302
 87%|████████▋ | 87/100 [1:18:22<11:42, 54.02s/it]Time:  4.966306461952627
 88%|████████▊ | 88/100 [1:19:16<10:47, 53.96s/it]Time:  4.8957522651180625
 89%|████████▉ | 89/100 [1:20:10<09:53, 54.00s/it]Time:  4.931609155610204
 90%|█████████ | 90/100 [1:21:04<08:59, 53.98s/it]Time:  4.981343314982951
 91%|█████████ | 91/100 [1:21:58<08:07, 54.14s/it]Time:  4.940129179507494
 92%|█████████▏| 92/100 [1:22:52<07:12, 54.12s/it]Time:  4.924070857465267
 93%|█████████▎| 93/100 [1:23:46<06:18, 54.02s/it]Time:  4.930100572295487
 94%|█████████▍| 94/100 [1:24:40<05:23, 53.96s/it]Time:  4.968548743054271
 95%|█████████▌| 95/100 [1:25:34<04:29, 53.96s/it]Time:  4.973818278871477
 96%|█████████▌| 96/100 [1:26:28<03:35, 53.98s/it]Time:  4.916616722010076
 97%|█████████▋| 97/100 [1:27:22<02:41, 53.93s/it]Time:  5.004333469085395
 98%|█████████▊| 98/100 [1:28:16<01:47, 53.86s/it]Time:  5.1648784736171365
 99%|█████████▉| 99/100 [1:29:10<00:54, 54.10s/it]Time:  5.107332441024482
100%|██████████| 100/100 [1:30:05<00:00, 54.18s/it]100%|██████████| 100/100 [1:30:05<00:00, 54.05s/it]
Post-training time: 5409.05 seconds
GPU memory allocated: 374.76 MB, peak: 999.48 MB
Train results:
                 train_loss     test_loss  top1_accuracy  top5_accuracy   test_time
Init.      0           NaN      2.417717          11.73          50.17  126.390692
Pre-Prune  0           NaN      2.417717          11.73          50.17  126.390692
Post-Prune 0           NaN  65169.561150          10.00          50.00    3.933837
Final      100    1.182215      1.139151          58.50          96.13    5.107332
Prune results:
             module   param  sparsity     size             shape     flops    score mean  score variance  score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.460069     1728     (64, 3, 3, 3)   1769472  4.450376e-05    8.434829e-08   0.076903    1.499241e-04        6.385164e-08       0.259069      True
1    layers.0.conv    bias  1.000000       64             (64,)     65536  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
2    layers.1.conv  weight  0.191026    36864    (64, 64, 3, 3)  37748736  2.075447e-06    4.122782e-09   0.076509    2.485490e-05        3.509323e-09       0.916251      True
3    layers.1.conv    bias  1.000000       64             (64,)     65536  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
4    layers.3.conv  weight  0.144653    73728   (128, 64, 3, 3)  18874368  1.036265e-06    1.686676e-09   0.076402    1.571875e-05        1.440671e-09       1.158912      True
5    layers.3.conv    bias  1.000000      128            (128,)     32768  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
6    layers.4.conv  weight  0.087145   147456  (128, 128, 3, 3)  37748736  5.169903e-07    5.230480e-10   0.076233    7.786415e-06        4.626871e-10       1.148154      True
7    layers.4.conv    bias  1.000000      128            (128,)     32768  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
8    layers.6.conv  weight  0.063032   294912  (256, 128, 3, 3)  18874368  2.581017e-07    2.053401e-10   0.076117    4.942224e-06        1.809812e-10       1.457521      True
9    layers.6.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
10   layers.7.conv  weight  0.032915   589824  (256, 256, 3, 3)  37748736  1.255479e-07    5.263317e-11   0.074051    2.530799e-06        4.624397e-11       1.492726      True
11   layers.7.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
12   layers.8.conv  weight  0.026825   589824  (256, 256, 3, 3)  37748736  1.240259e-07    3.422638e-11   0.073153    2.217017e-06        2.932658e-11       1.307650      True
13   layers.8.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
14  layers.10.conv  weight  0.013724  1179648  (512, 256, 3, 3)  18874368  6.157174e-08    1.396249e-11   0.072633    1.547427e-06        1.157176e-11       1.825420      True
15  layers.10.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
16  layers.11.conv  weight  0.003844  2359296  (512, 512, 3, 3)  37748736  2.846882e-08    4.243503e-12   0.067166    8.836323e-07        3.463505e-12       2.084750      True
17  layers.11.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
18  layers.12.conv  weight  0.002859  2359296  (512, 512, 3, 3)  37748736  2.840790e-08    3.657214e-12   0.067023    8.970007e-07        2.853411e-12       2.116290      True
19  layers.12.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
20  layers.14.conv  weight  0.002981  2359296  (512, 512, 3, 3)   9437184  2.694333e-08    3.812698e-12   0.063567    8.725643e-07        3.052056e-12       2.058637      True
21  layers.14.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
22  layers.15.conv  weight  0.003785  2359296  (512, 512, 3, 3)   9437184  2.711094e-08    4.234987e-12   0.063963    8.358489e-07        3.537079e-12       1.972015      True
23  layers.15.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
24  layers.16.conv  weight  0.005128  2359296  (512, 512, 3, 3)   9437184  2.642919e-08    5.443437e-12   0.062354    9.434239e-07        4.554087e-12       2.225816      True
25  layers.16.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
26              fc  weight  0.373633     5120         (10, 512)      5120  1.443846e-05    5.200130e-09   0.073925    3.773055e-05        3.985004e-09       0.193180      True
27              fc    bias  1.000000       10             (10,)        10  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
Parameter Sparsity: 151389/14719818 (0.0103)
FLOP Sparsity: 18392443/313478154 (0.0587)
Saving results.
