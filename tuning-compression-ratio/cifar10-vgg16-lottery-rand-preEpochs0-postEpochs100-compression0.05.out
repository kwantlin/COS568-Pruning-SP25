Namespace(dataset='cifar10', model='vgg16', model_class='lottery', dense_classifier=False, pretrained=False, optimizer='adam', train_batch_size=256, test_batch_size=256, pre_epochs=0, post_epochs=100, lr=0.001, lr_drops=[], lr_drop_rate=0.1, weight_decay=0.0, pruner='rand', compression=0.05, quantization=False, prune_epochs=1, compression_schedule='exponential', mask_scope='global', prune_dataset_ratio=10, prune_batch_size=256, prune_bias=False, prune_batchnorm=False, prune_residual=False, prune_train_mode=False, reinitialize=False, shuffle=False, invert=False, pruner_list=[], prune_epoch_list=[], compression_list=[], level_list=[], experiment='singleshot', expid='cifar10-vgg16-lottery-rand-preEpochs0-postEpochs100-compression0.05', result_dir='Results/data', gpu=0, workers=4, no_cuda=False, seed=1, verbose=False)
Loading cifar10 dataset.
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with rand for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:09<00:00,  9.62s/it]100%|██████████| 1/1 [00:09<00:00,  9.63s/it]
Post-Training for 100 epochs.
  0%|          | 0/100 [00:00<?, ?it/s]Time:  3.8606852777302265
  1%|          | 1/100 [01:12<1:59:39, 72.52s/it]Time:  3.847917566075921
  2%|▏         | 2/100 [02:07<1:41:26, 62.11s/it]Time:  3.6073298444971442
  3%|▎         | 3/100 [03:02<1:35:07, 58.84s/it]Time:  3.4161709286272526
  4%|▍         | 4/100 [03:56<1:31:22, 57.11s/it]Time:  4.279259163886309
  5%|▌         | 5/100 [04:51<1:28:52, 56.13s/it]Time:  4.345356775447726
  6%|▌         | 6/100 [05:45<1:26:44, 55.37s/it]Time:  4.455022323876619
  7%|▋         | 7/100 [06:38<1:24:58, 54.82s/it]Time:  4.567160235717893
  8%|▊         | 8/100 [07:34<1:24:25, 55.06s/it]Time:  4.406057574786246
  9%|▉         | 9/100 [08:28<1:22:52, 54.64s/it]Time:  4.562564721331
 10%|█         | 10/100 [09:21<1:21:22, 54.25s/it]Time:  4.581618728116155
 11%|█         | 11/100 [10:14<1:19:50, 53.83s/it]Time:  4.61748219653964
 12%|█▏        | 12/100 [11:07<1:18:35, 53.58s/it]Time:  4.559392275288701
 13%|█▎        | 13/100 [12:00<1:17:25, 53.40s/it]Time:  4.632905258797109
 14%|█▍        | 14/100 [12:53<1:16:23, 53.30s/it]Time:  4.6458815559744835
 15%|█▌        | 15/100 [13:42<1:13:46, 52.08s/it]Time:  4.651972474530339
 16%|█▌        | 16/100 [14:36<1:13:29, 52.49s/it]Time:  4.6771442629396915
 17%|█▋        | 17/100 [15:29<1:12:57, 52.74s/it]Time:  4.819537285715342
 18%|█▊        | 18/100 [16:23<1:12:30, 53.06s/it]Time:  4.689247228205204
 19%|█▉        | 19/100 [17:17<1:12:04, 53.39s/it]Time:  4.7052431190386415
 20%|██        | 20/100 [18:11<1:11:19, 53.49s/it]Time:  4.7558899484574795
 21%|██        | 21/100 [19:05<1:10:43, 53.72s/it]Time:  4.788735813461244
 22%|██▏       | 22/100 [19:58<1:09:36, 53.55s/it]Time:  4.77311837207526
 23%|██▎       | 23/100 [20:52<1:08:54, 53.70s/it]Time:  4.720727674663067
 24%|██▍       | 24/100 [21:46<1:08:02, 53.72s/it]Time:  4.730941001325846
 25%|██▌       | 25/100 [22:38<1:06:40, 53.34s/it]Time:  4.727947887033224
 26%|██▌       | 26/100 [23:34<1:06:38, 54.03s/it]Time:  4.532471103593707
 27%|██▋       | 27/100 [24:23<1:03:57, 52.56s/it]Time:  4.5341270146891475
 28%|██▊       | 28/100 [25:14<1:02:27, 52.04s/it]Time:  4.145750047639012
 29%|██▉       | 29/100 [26:04<1:00:48, 51.39s/it]Time:  3.680694411508739
 30%|███       | 30/100 [26:59<1:01:12, 52.47s/it]Time:  4.786170303821564
 31%|███       | 31/100 [27:53<1:00:58, 53.02s/it]Time:  4.827120932750404
 32%|███▏      | 32/100 [28:47<1:00:23, 53.29s/it]Time:  4.79642928391695
 33%|███▎      | 33/100 [29:41<59:42, 53.47s/it]  Time:  4.847526894882321
 34%|███▍      | 34/100 [30:35<59:04, 53.71s/it]Time:  4.580388432368636
 35%|███▌      | 35/100 [31:28<57:57, 53.50s/it]Time:  4.465575369074941
 36%|███▌      | 36/100 [32:22<57:06, 53.54s/it]Time:  4.515792151913047
 37%|███▋      | 37/100 [33:15<56:11, 53.51s/it]Time:  4.514763416722417
 38%|███▊      | 38/100 [34:09<55:29, 53.71s/it]Time:  4.558022134006023
 39%|███▉      | 39/100 [35:03<54:40, 53.77s/it]Time:  4.351713458076119
 40%|████      | 40/100 [35:57<53:52, 53.87s/it]Time:  4.458062442950904
 41%|████      | 41/100 [36:52<53:08, 54.04s/it]Time:  4.449835051782429
 42%|████▏     | 42/100 [37:46<52:18, 54.11s/it]Time:  4.465211653150618
 43%|████▎     | 43/100 [38:40<51:25, 54.13s/it]Time:  4.480049195699394
 44%|████▍     | 44/100 [39:34<50:29, 54.10s/it]Time:  4.425470218062401
 45%|████▌     | 45/100 [40:29<49:39, 54.17s/it]Time:  4.5219625029712915
 46%|████▌     | 46/100 [41:22<48:39, 54.07s/it]Time:  4.4672991475090384
 47%|████▋     | 47/100 [42:17<47:47, 54.10s/it]Time:  4.426080670207739
 48%|████▊     | 48/100 [43:11<46:50, 54.05s/it]Time:  4.450697457417846
 49%|████▉     | 49/100 [44:05<46:01, 54.16s/it]Time:  4.587046975269914
 50%|█████     | 50/100 [44:59<45:10, 54.22s/it]Time:  4.419451194815338
 51%|█████     | 51/100 [45:53<44:10, 54.09s/it]Time:  4.431541457772255
 52%|█████▏    | 52/100 [46:47<43:12, 54.01s/it]Time:  4.434825803153217
 53%|█████▎    | 53/100 [47:41<42:20, 54.05s/it]Time:  4.426596988923848
 54%|█████▍    | 54/100 [48:35<41:27, 54.08s/it]Time:  4.401025261729956
 55%|█████▌    | 55/100 [49:29<40:30, 54.01s/it]Time:  4.407851588912308
 56%|█████▌    | 56/100 [50:23<39:35, 53.98s/it]Time:  4.382264439016581
 57%|█████▋    | 57/100 [51:17<38:46, 54.10s/it]Time:  4.56571133993566
 58%|█████▊    | 58/100 [52:12<37:53, 54.14s/it]Time:  4.507836948148906
 59%|█████▉    | 59/100 [53:06<36:59, 54.14s/it]Time:  4.4756747437641025
 60%|██████    | 60/100 [54:00<36:06, 54.17s/it]Time:  4.47764853015542
 61%|██████    | 61/100 [54:54<35:14, 54.22s/it]Time:  4.448090069927275
 62%|██████▏   | 62/100 [55:48<34:14, 54.07s/it]Time:  4.532163324765861
 63%|██████▎   | 63/100 [56:42<33:24, 54.17s/it]Time:  4.455495563335717
 64%|██████▍   | 64/100 [57:37<32:32, 54.22s/it]Time:  4.383568738587201
 65%|██████▌   | 65/100 [58:31<31:34, 54.14s/it]Time:  4.351535622030497
 66%|██████▌   | 66/100 [59:24<30:32, 53.89s/it]Time:  4.460171244107187
 67%|██████▋   | 67/100 [1:00:18<29:37, 53.85s/it]Time:  4.44795545283705
 68%|██████▊   | 68/100 [1:01:12<28:44, 53.90s/it]Time:  4.43661804869771
 69%|██████▉   | 69/100 [1:02:06<27:57, 54.11s/it]Time:  4.420238035731018
 70%|███████   | 70/100 [1:03:00<27:01, 54.05s/it]Time:  4.508394800126553
 71%|███████   | 71/100 [1:03:55<26:09, 54.12s/it]Time:  4.469135330989957
 72%|███████▏  | 72/100 [1:04:49<25:16, 54.17s/it]Time:  4.486833825707436
 73%|███████▎  | 73/100 [1:05:43<24:23, 54.22s/it]Time:  4.547849912196398
 74%|███████▍  | 74/100 [1:06:37<23:30, 54.24s/it]Time:  4.4190255384892225
 75%|███████▌  | 75/100 [1:07:32<22:34, 54.18s/it]Time:  4.442985104396939
 76%|███████▌  | 76/100 [1:08:25<21:38, 54.12s/it]Time:  4.491255288012326
 77%|███████▋  | 77/100 [1:09:20<20:44, 54.10s/it]Time:  4.542046911083162
 78%|███████▊  | 78/100 [1:10:14<19:54, 54.28s/it]Time:  4.448999939486384
 79%|███████▉  | 79/100 [1:11:09<18:59, 54.28s/it]Time:  4.3896802412346005
 80%|████████  | 80/100 [1:12:03<18:05, 54.27s/it]Time:  4.444368950091302
 81%|████████  | 81/100 [1:12:57<17:09, 54.21s/it]Time:  4.538950562477112
 82%|████████▏ | 82/100 [1:13:51<16:14, 54.14s/it]Time:  4.44609101396054
 83%|████████▎ | 83/100 [1:14:45<15:21, 54.18s/it]Time:  4.456002116203308
 84%|████████▍ | 84/100 [1:15:40<14:28, 54.27s/it]Time:  4.479740638285875
 85%|████████▌ | 85/100 [1:16:34<13:33, 54.21s/it]Time:  4.4182548420503736
 86%|████████▌ | 86/100 [1:17:28<12:39, 54.22s/it]Time:  4.527798430994153
 87%|████████▋ | 87/100 [1:18:22<11:45, 54.30s/it]Time:  4.3778813518583775
 88%|████████▊ | 88/100 [1:19:16<10:50, 54.18s/it]Time:  4.451565113849938
 89%|████████▉ | 89/100 [1:20:11<09:56, 54.27s/it]Time:  4.576895986683667
 90%|█████████ | 90/100 [1:21:05<09:02, 54.28s/it]Time:  4.430100394412875
 91%|█████████ | 91/100 [1:21:59<08:07, 54.22s/it]Time:  4.39256211835891
 92%|█████████▏| 92/100 [1:22:53<07:13, 54.25s/it]Time:  4.202917193993926
 93%|█████████▎| 93/100 [1:23:47<06:19, 54.15s/it]Time:  4.5725039737299085
 94%|█████████▍| 94/100 [1:24:42<05:25, 54.23s/it]Time:  4.614297418855131
 95%|█████████▌| 95/100 [1:25:37<04:32, 54.45s/it]Time:  4.478302987292409
 96%|█████████▌| 96/100 [1:26:31<03:38, 54.52s/it]Time:  4.404235909692943
 97%|█████████▋| 97/100 [1:27:25<02:43, 54.36s/it]Time:  4.528274831362069
 98%|█████████▊| 98/100 [1:28:19<01:48, 54.22s/it]Time:  4.573969862423837
 99%|█████████▉| 99/100 [1:29:13<00:54, 54.17s/it]Time:  3.7022579284384847
100%|██████████| 100/100 [1:30:00<00:00, 52.06s/it]100%|██████████| 100/100 [1:30:00<00:00, 54.01s/it]
Post-training time: 5403.67 seconds
GPU memory allocated: 375.42 MB, peak: 999.49 MB
Train results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy  test_time
Init.      0           NaN   2.417717          11.73          50.17  23.930025
Pre-Prune  0           NaN   2.417717          11.73          50.17  23.930025
Post-Prune 0           NaN   2.322258          10.43          50.00   2.670453
Final      100    0.078418   0.610953          87.98          98.85   3.702258
Prune results:
             module   param  sparsity     size             shape     flops  score mean  score variance    score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.881944     1728     (64, 3, 3, 3)   1769472   -0.027050        1.005513   -46.742134        0.805873            0.356814   1.392548e+03      True
1    layers.0.conv    bias  1.000000       64             (64,)     65536    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
2    layers.1.conv  weight  0.892659    36864    (64, 64, 3, 3)  37748736    0.000872        0.996963    32.139240        0.796522            0.362516   2.936300e+04      True
3    layers.1.conv    bias  1.000000       64             (64,)     65536    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
4    layers.3.conv  weight  0.892632    73728   (128, 64, 3, 3)  18874368    0.000958        0.993901    70.645149        0.796130            0.360079   5.869704e+04      True
5    layers.3.conv    bias  1.000000      128            (128,)     32768    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
6    layers.4.conv  weight  0.891283   147456  (128, 128, 3, 3)  37748736    0.001246        0.994699   183.733383        0.795026            0.362635   1.172313e+05      True
7    layers.4.conv    bias  1.000000      128            (128,)     32768    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
8    layers.6.conv  weight  0.890771   294912  (256, 128, 3, 3)  18874368   -0.001563        1.001676  -460.913849        0.798645            0.363844   2.355301e+05      True
9    layers.6.conv    bias  1.000000      256            (256,)     16384    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
10   layers.7.conv  weight  0.891366   589824  (256, 256, 3, 3)  37748736    0.000378        0.999599   222.715530        0.797776            0.363152   4.705474e+05      True
11   layers.7.conv    bias  1.000000      256            (256,)     16384    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
12   layers.8.conv  weight  0.891573   589824  (256, 256, 3, 3)  37748736    0.000403        0.998166   237.463287        0.796769            0.363324   4.699538e+05      True
13   layers.8.conv    bias  1.000000      256            (256,)     16384    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
14  layers.10.conv  weight  0.891617  1179648  (512, 256, 3, 3)  18874368    0.000511        0.999746   602.936768        0.797616            0.363555   9.409056e+05      True
15  layers.10.conv    bias  1.000000      512            (512,)      8192    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
16  layers.11.conv  weight  0.890967  2359296  (512, 512, 3, 3)  37748736   -0.000190        1.001348  -448.645966        0.798652            0.363503   1.884256e+06      True
17  layers.11.conv    bias  1.000000      512            (512,)      8192    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
18  layers.12.conv  weight  0.891159  2359296  (512, 512, 3, 3)  37748736    0.000249        0.999722   588.484009        0.797592            0.363569   1.881756e+06      True
19  layers.12.conv    bias  1.000000      512            (512,)      8192    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
20  layers.14.conv  weight  0.891282  2359296  (512, 512, 3, 3)   9437184   -0.000339        0.999254  -799.927002        0.797659            0.362994   1.881914e+06      True
21  layers.14.conv    bias  1.000000      512            (512,)      2048    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
22  layers.15.conv  weight  0.891270  2359296  (512, 512, 3, 3)   9437184    0.000708        1.000298  1670.380615        0.797886            0.363677   1.882448e+06      True
23  layers.15.conv    bias  1.000000      512            (512,)      2048    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
24  layers.16.conv  weight  0.891274  2359296  (512, 512, 3, 3)   9437184    0.000980        0.999300  2311.431885        0.797918            0.362629   1.882524e+06      True
25  layers.16.conv    bias  1.000000      512            (512,)      2048    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
26              fc  weight  0.895508     5120         (10, 512)      5120    0.007839        0.966788    40.134628        0.782462            0.354602   4.006208e+03      True
27              fc    bias  1.000000       10             (10,)        10    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
Parameter Sparsity: 13119511/14719818 (0.8913)
FLOP Sparsity: 279482611/313478154 (0.8916)
Saving results.
