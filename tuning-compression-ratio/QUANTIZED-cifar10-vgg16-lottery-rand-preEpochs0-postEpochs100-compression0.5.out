Namespace(dataset='cifar10', model='vgg16', model_class='lottery', dense_classifier=False, pretrained=False, optimizer='adam', train_batch_size=256, test_batch_size=256, pre_epochs=0, post_epochs=100, lr=0.001, lr_drops=[], lr_drop_rate=0.1, weight_decay=0.0, pruner='rand', compression=0.5, quantization=True, prune_epochs=1, compression_schedule='exponential', mask_scope='global', prune_dataset_ratio=10, prune_batch_size=256, prune_bias=False, prune_batchnorm=False, prune_residual=False, prune_train_mode=False, reinitialize=False, shuffle=False, invert=False, pruner_list=[], prune_epoch_list=[], compression_list=[], level_list=[], experiment='singleshot', expid='QUANTIZED-cifar10-vgg16-lottery-rand-preEpochs0-postEpochs100-compression0.5', result_dir='Results/data', gpu=0, workers=4, no_cuda=False, seed=1, verbose=False)
Loading cifar10 dataset.
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with rand for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.74s/it]100%|██████████| 1/1 [00:02<00:00,  2.74s/it]
Post-Training for 100 epochs.
  0%|          | 0/100 [00:00<?, ?it/s]Time:  3.848241852945648
  1%|          | 1/100 [01:01<1:41:50, 61.72s/it]Time:  3.773309195996262
  2%|▏         | 2/100 [01:36<1:14:41, 45.73s/it]Time:  3.751108278054744
  3%|▎         | 3/100 [02:10<1:05:39, 40.61s/it]Time:  3.78459955600556
  4%|▍         | 4/100 [02:44<1:00:41, 37.94s/it]Time:  3.7067228680243716
  5%|▌         | 5/100 [03:18<57:28, 36.30s/it]  Time:  4.156298603978939
  6%|▌         | 6/100 [03:52<55:43, 35.57s/it]Time:  4.048748156055808
  7%|▋         | 7/100 [04:26<54:20, 35.06s/it]Time:  3.876063865958713
  8%|▊         | 8/100 [04:59<52:58, 34.55s/it]Time:  3.877156974049285
  9%|▉         | 9/100 [05:33<51:51, 34.19s/it]Time:  3.385246357996948
 10%|█         | 10/100 [06:06<50:55, 33.95s/it]Time:  3.9267215080326423
 11%|█         | 11/100 [06:38<49:42, 33.51s/it]Time:  3.9156560860574245
 12%|█▏        | 12/100 [07:12<49:02, 33.43s/it]Time:  4.089689158950932
 13%|█▎        | 13/100 [07:44<47:56, 33.06s/it]Time:  4.125287318020128
 14%|█▍        | 14/100 [08:16<47:01, 32.81s/it]Time:  4.191482795053162
 15%|█▌        | 15/100 [08:49<46:16, 32.67s/it]Time:  4.285705887014046
 16%|█▌        | 16/100 [09:20<45:10, 32.27s/it]Time:  4.323284488986246
 17%|█▋        | 17/100 [09:52<44:40, 32.29s/it]Time:  4.25785532000009
 18%|█▊        | 18/100 [10:24<44:02, 32.22s/it]Time:  4.286626648972742
 19%|█▉        | 19/100 [10:57<43:35, 32.29s/it]Time:  4.3403148020152
 20%|██        | 20/100 [11:29<42:58, 32.23s/it]Time:  4.306325813056901
 21%|██        | 21/100 [12:01<42:23, 32.19s/it]Time:  4.305240937042981
 22%|██▏       | 22/100 [12:33<41:43, 32.09s/it]Time:  4.292285623028874
 23%|██▎       | 23/100 [13:05<41:08, 32.06s/it]Time:  4.295517078018747
 24%|██▍       | 24/100 [13:36<40:26, 31.93s/it]Time:  4.153991888975725
 25%|██▌       | 25/100 [14:10<40:25, 32.34s/it]Time:  4.504821609007195
 26%|██▌       | 26/100 [14:43<40:04, 32.50s/it]Time:  4.528995604021475
 27%|██▋       | 27/100 [15:15<39:33, 32.51s/it]Time:  4.504256812972017
 28%|██▊       | 28/100 [15:46<38:23, 32.00s/it]Time:  4.562852045986801
 29%|██▉       | 29/100 [16:18<38:01, 32.14s/it]Time:  4.553645420935936
 30%|███       | 30/100 [16:51<37:32, 32.17s/it]Time:  4.533995567006059
 31%|███       | 31/100 [17:21<36:33, 31.79s/it]Time:  2.4980841260403395
 32%|███▏      | 32/100 [17:52<35:27, 31.29s/it]Time:  2.970120192039758
 33%|███▎      | 33/100 [18:23<35:02, 31.37s/it]Time:  4.039260723046027
 34%|███▍      | 34/100 [19:01<36:33, 33.24s/it]Time:  4.0591353280469775
 35%|███▌      | 35/100 [19:32<35:23, 32.67s/it]Time:  4.117043882026337
 36%|███▌      | 36/100 [20:10<36:31, 34.24s/it]Time:  4.073543131002225
 37%|███▋      | 37/100 [20:44<35:46, 34.07s/it]Time:  3.9314205669797957
 38%|███▊      | 38/100 [21:18<35:10, 34.03s/it]Time:  3.585919821052812
 39%|███▉      | 39/100 [21:51<34:22, 33.82s/it]Time:  3.2483454099856317
 40%|████      | 40/100 [22:24<33:41, 33.68s/it]Time:  3.2471107030287385
 41%|████      | 41/100 [22:58<33:05, 33.65s/it]Time:  2.4513611670117825
 42%|████▏     | 42/100 [23:31<32:19, 33.43s/it]Time:  4.1579723010072485
 43%|████▎     | 43/100 [24:04<31:39, 33.32s/it]Time:  4.102269986993633
 44%|████▍     | 44/100 [24:36<30:51, 33.07s/it]Time:  4.372730345930904
 45%|████▌     | 45/100 [25:07<29:44, 32.45s/it]Time:  3.8467407770222053
 46%|████▌     | 46/100 [25:39<28:56, 32.16s/it]Time:  3.8947013589786366
 47%|████▋     | 47/100 [26:10<28:12, 31.93s/it]Time:  3.427617982029915
 48%|████▊     | 48/100 [26:42<27:43, 31.98s/it]Time:  3.6188900580164045
 49%|████▉     | 49/100 [27:18<28:11, 33.17s/it]Time:  4.505964449024759
 50%|█████     | 50/100 [27:48<26:48, 32.18s/it]Time:  4.513184868032113
 51%|█████     | 51/100 [28:20<26:16, 32.18s/it]Time:  4.50974758004304
 52%|█████▏    | 52/100 [28:52<25:32, 31.93s/it]Time:  4.525774499983527
 53%|█████▎    | 53/100 [29:23<24:51, 31.74s/it]Time:  4.5453779000090435
 54%|█████▍    | 54/100 [29:55<24:21, 31.77s/it]Time:  4.509806955000386
 55%|█████▌    | 55/100 [30:27<23:50, 31.78s/it]Time:  4.535674390965141
 56%|█████▌    | 56/100 [30:59<23:26, 31.96s/it]Time:  4.519110316061415
 57%|█████▋    | 57/100 [31:31<22:53, 31.94s/it]Time:  4.540516766952351
 58%|█████▊    | 58/100 [32:03<22:25, 32.03s/it]Time:  4.601051875972189
 59%|█████▉    | 59/100 [32:36<22:01, 32.24s/it]Time:  4.5993844559416175
 60%|██████    | 60/100 [33:08<21:30, 32.25s/it]Time:  4.7138884459855035
 61%|██████    | 61/100 [33:40<20:56, 32.23s/it]Time:  4.180178623995744
 62%|██████▏   | 62/100 [34:12<20:21, 32.14s/it]Time:  4.264396269922145
 63%|██████▎   | 63/100 [34:44<19:48, 32.11s/it]Time:  4.157325883046724
 64%|██████▍   | 64/100 [35:16<19:13, 32.05s/it]Time:  3.3328395459102467
 65%|██████▌   | 65/100 [35:48<18:36, 31.91s/it]Time:  2.2503319960087538
 66%|██████▌   | 66/100 [36:20<18:08, 32.02s/it]Time:  3.5898827770724893
 67%|██████▋   | 67/100 [36:54<17:55, 32.58s/it]Time:  3.448866894003004
 68%|██████▊   | 68/100 [37:27<17:30, 32.83s/it]Time:  3.6153971520252526
 69%|██████▉   | 69/100 [38:01<17:02, 32.98s/it]Time:  4.035029590013437
 70%|███████   | 70/100 [38:34<16:33, 33.13s/it]Time:  3.729138590977527
 71%|███████   | 71/100 [39:07<15:58, 33.07s/it]Time:  2.3844941969728097
 72%|███████▏  | 72/100 [39:39<15:14, 32.66s/it]Time:  3.073286031954922
 73%|███████▎  | 73/100 [40:12<14:47, 32.89s/it]Time:  3.679580547963269
 74%|███████▍  | 74/100 [40:47<14:27, 33.35s/it]Time:  4.0676852581091225
 75%|███████▌  | 75/100 [41:19<13:49, 33.16s/it]Time:  4.0999183770036325
 76%|███████▌  | 76/100 [41:52<13:12, 33.03s/it]Time:  4.082764229970053
 77%|███████▋  | 77/100 [42:25<12:39, 33.03s/it]Time:  4.158020825008862
 78%|███████▊  | 78/100 [42:59<12:09, 33.18s/it]Time:  3.9589224400697276
 79%|███████▉  | 79/100 [43:31<11:32, 33.00s/it]Time:  3.6854653210612014
 80%|████████  | 80/100 [44:05<11:04, 33.20s/it]Time:  4.105023103998974
 81%|████████  | 81/100 [44:38<10:28, 33.09s/it]Time:  4.084375123959035
 82%|████████▏ | 82/100 [45:11<09:58, 33.26s/it]Time:  4.082147325971164
 83%|████████▎ | 83/100 [45:44<09:24, 33.19s/it]Time:  4.3417847660603
 84%|████████▍ | 84/100 [46:17<08:50, 33.14s/it]Time:  4.502741424948908
 85%|████████▌ | 85/100 [46:51<08:21, 33.41s/it]Time:  4.224325389019214
 86%|████████▌ | 86/100 [47:27<07:57, 34.11s/it]Time:  4.020577808958478
 87%|████████▋ | 87/100 [48:02<07:25, 34.28s/it]Time:  3.7071272879838943
 88%|████████▊ | 88/100 [48:38<06:56, 34.72s/it]Time:  3.849433096940629
 89%|████████▉ | 89/100 [49:13<06:23, 34.89s/it]Time:  3.7269103890284896
 90%|█████████ | 90/100 [49:47<05:47, 34.70s/it]Time:  3.347245537908748
 91%|█████████ | 91/100 [50:21<05:09, 34.36s/it]Time:  3.5515404200414196
 92%|█████████▏| 92/100 [50:56<04:36, 34.54s/it]Time:  3.494318813085556
 93%|█████████▎| 93/100 [51:31<04:03, 34.74s/it]Time:  3.9647367579163983
 94%|█████████▍| 94/100 [52:06<03:28, 34.79s/it]Time:  3.935137221007608
 95%|█████████▌| 95/100 [52:39<02:51, 34.38s/it]Time:  4.077916646958329
 96%|█████████▌| 96/100 [53:13<02:16, 34.13s/it]Time:  4.200791152077727
 97%|█████████▋| 97/100 [53:44<01:39, 33.23s/it]Time:  4.504263033973984
 98%|█████████▊| 98/100 [54:12<01:03, 31.62s/it]Time:  4.4935374890919775
 99%|█████████▉| 99/100 [54:36<00:29, 29.34s/it]Time:  1.6989310460630804
100%|██████████| 100/100 [55:02<00:00, 28.34s/it]100%|██████████| 100/100 [55:02<00:00, 33.02s/it]
Post-training time: 3306.16 seconds
GPU memory allocated: 376.33 MB, peak: 876.67 MB
Train results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy  test_time
Init.      0           NaN   2.417717          11.73          50.17  45.435499
Pre-Prune  0           NaN   2.417717          11.73          50.17  45.435499
Post-Prune 0           NaN   2.302584          10.00          50.56   3.804946
Final      100    0.070468   0.656893          87.47          99.17   1.698931
Prune results:
             module   param  sparsity     size             shape     flops  score mean  score variance    score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.305556     1728     (64, 3, 3, 3)   1769472   -0.027050        1.005513   -46.742134        0.805873            0.356814   1.392548e+03      True
1    layers.0.conv    bias  1.000000       64             (64,)     65536    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
2    layers.1.conv  weight  0.314236    36864    (64, 64, 3, 3)  37748736    0.000872        0.996963    32.139240        0.796522            0.362516   2.936300e+04      True
3    layers.1.conv    bias  1.000000       64             (64,)     65536    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
4    layers.3.conv  weight  0.316447    73728   (128, 64, 3, 3)  18874368    0.000958        0.993901    70.645149        0.796130            0.360079   5.869704e+04      True
5    layers.3.conv    bias  1.000000      128            (128,)     32768    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
6    layers.4.conv  weight  0.316352   147456  (128, 128, 3, 3)  37748736    0.001246        0.994699   183.733383        0.795026            0.362635   1.172313e+05      True
7    layers.4.conv    bias  1.000000      128            (128,)     32768    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
8    layers.6.conv  weight  0.316254   294912  (256, 128, 3, 3)  18874368   -0.001563        1.001676  -460.913849        0.798645            0.363844   2.355301e+05      True
9    layers.6.conv    bias  1.000000      256            (256,)     16384    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
10   layers.7.conv  weight  0.316233   589824  (256, 256, 3, 3)  37748736    0.000378        0.999599   222.715530        0.797776            0.363152   4.705474e+05      True
11   layers.7.conv    bias  1.000000      256            (256,)     16384    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
12   layers.8.conv  weight  0.315952   589824  (256, 256, 3, 3)  37748736    0.000403        0.998166   237.463287        0.796769            0.363324   4.699538e+05      True
13   layers.8.conv    bias  1.000000      256            (256,)     16384    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
14  layers.10.conv  weight  0.316293  1179648  (512, 256, 3, 3)  18874368    0.000511        0.999746   602.936768        0.797616            0.363555   9.409056e+05      True
15  layers.10.conv    bias  1.000000      512            (512,)      8192    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
16  layers.11.conv  weight  0.316343  2359296  (512, 512, 3, 3)  37748736   -0.000190        1.001348  -448.645966        0.798652            0.363503   1.884256e+06      True
17  layers.11.conv    bias  1.000000      512            (512,)      8192    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
18  layers.12.conv  weight  0.315997  2359296  (512, 512, 3, 3)  37748736    0.000249        0.999722   588.484009        0.797592            0.363569   1.881756e+06      True
19  layers.12.conv    bias  1.000000      512            (512,)      8192    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
20  layers.14.conv  weight  0.316019  2359296  (512, 512, 3, 3)   9437184   -0.000339        0.999254  -799.927002        0.797659            0.362994   1.881914e+06      True
21  layers.14.conv    bias  1.000000      512            (512,)      2048    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
22  layers.15.conv  weight  0.316295  2359296  (512, 512, 3, 3)   9437184    0.000708        1.000298  1670.380615        0.797886            0.363677   1.882448e+06      True
23  layers.15.conv    bias  1.000000      512            (512,)      2048    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
24  layers.16.conv  weight  0.316552  2359296  (512, 512, 3, 3)   9437184    0.000980        0.999300  2311.431885        0.797918            0.362629   1.882524e+06      True
25  layers.16.conv    bias  1.000000      512            (512,)      2048    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
26              fc  weight  0.311133     5120         (10, 512)      5120    0.007839        0.966788    40.134628        0.782462            0.354602   4.006208e+03      True
27              fc    bias  1.000000       10             (10,)        10    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
Parameter Sparsity: 4657711/14719818 (0.3164)
FLOP Sparsity: 99223159/313478154 (0.3165)
Saving results.
