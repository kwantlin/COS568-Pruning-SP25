Namespace(dataset='cifar10', model='vgg16', model_class='lottery', dense_classifier=False, pretrained=False, optimizer='adam', train_batch_size=256, test_batch_size=256, pre_epochs=0, post_epochs=100, lr=0.001, lr_drops=[], lr_drop_rate=0.1, weight_decay=0.0, pruner='grasp', compression=0.2, quantization=False, prune_epochs=1, compression_schedule='exponential', mask_scope='global', prune_dataset_ratio=10, prune_batch_size=256, prune_bias=False, prune_batchnorm=False, prune_residual=False, prune_train_mode=False, reinitialize=False, shuffle=False, invert=False, pruner_list=[], prune_epoch_list=[], compression_list=[], level_list=[], experiment='singleshot', expid='cifar10-vgg16-lottery-grasp-preEpochs0-postEpochs100-compression0.2', result_dir='Results/data', gpu=0, workers=4, no_cuda=False, seed=1, verbose=False)
Loading cifar10 dataset.
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with grasp for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:35<00:00, 35.65s/it]100%|██████████| 1/1 [00:35<00:00, 35.65s/it]
Post-Training for 100 epochs.
  0%|          | 0/100 [00:00<?, ?it/s]Time:  3.6758459443226457
  1%|          | 1/100 [00:54<1:29:54, 54.49s/it]Time:  3.8180703734979033
  2%|▏         | 2/100 [01:50<1:30:00, 55.11s/it]Time:  4.394502945244312
  3%|▎         | 3/100 [02:45<1:29:22, 55.28s/it]Time:  4.50836272444576
  4%|▍         | 4/100 [03:40<1:28:07, 55.08s/it]Time:  4.6053520096465945
  5%|▌         | 5/100 [04:34<1:26:44, 54.78s/it]Time:  4.742617855779827
  6%|▌         | 6/100 [05:30<1:26:11, 55.01s/it]Time:  4.4603232610970736
  7%|▋         | 7/100 [06:25<1:25:22, 55.08s/it]Time:  4.3120095785707235
  8%|▊         | 8/100 [07:19<1:24:16, 54.97s/it]Time:  4.452150314114988
  9%|▉         | 9/100 [08:14<1:23:14, 54.88s/it]Time:  4.297609889879823
 10%|█         | 10/100 [09:09<1:22:20, 54.89s/it]Time:  4.435205809772015
 11%|█         | 11/100 [10:04<1:21:25, 54.90s/it]Time:  5.031955977901816
 12%|█▏        | 12/100 [10:59<1:20:40, 55.01s/it]Time:  4.767611755058169
 13%|█▎        | 13/100 [11:54<1:19:42, 54.97s/it]Time:  4.1712083499878645
 14%|█▍        | 14/100 [12:48<1:18:27, 54.74s/it]Time:  4.521498721092939
 15%|█▌        | 15/100 [13:43<1:17:35, 54.77s/it]Time:  4.963193629868329
 16%|█▌        | 16/100 [14:39<1:17:03, 55.04s/it]Time:  4.760543417185545
 17%|█▋        | 17/100 [15:34<1:16:02, 54.97s/it]Time:  4.7882489543408155
 18%|█▊        | 18/100 [16:29<1:15:26, 55.21s/it]Time:  4.737483740784228
 19%|█▉        | 19/100 [17:26<1:15:16, 55.76s/it]Time:  4.6984275141730905
 20%|██        | 20/100 [18:24<1:14:55, 56.20s/it]Time:  4.623519831337035
 21%|██        | 21/100 [19:18<1:13:13, 55.61s/it]Time:  4.6961425160989165
 22%|██▏       | 22/100 [20:13<1:12:12, 55.55s/it]Time:  4.594011106528342
 23%|██▎       | 23/100 [21:08<1:11:02, 55.35s/it]Time:  4.735072762705386
 24%|██▍       | 24/100 [22:03<1:10:02, 55.29s/it]Time:  4.766605698503554
 25%|██▌       | 25/100 [22:58<1:08:51, 55.09s/it]Time:  4.930690930224955
 26%|██▌       | 26/100 [23:53<1:07:48, 54.97s/it]Time:  4.944407104514539
 27%|██▋       | 27/100 [24:49<1:07:22, 55.38s/it]Time:  4.890957950614393
 28%|██▊       | 28/100 [25:40<1:04:55, 54.10s/it]Time:  4.960373588837683
 29%|██▉       | 29/100 [26:33<1:03:32, 53.69s/it]Time:  5.007058202289045
 30%|███       | 30/100 [27:27<1:02:41, 53.74s/it]Time:  4.943551781587303
 31%|███       | 31/100 [28:20<1:01:46, 53.71s/it]Time:  4.973967132158577
 32%|███▏      | 32/100 [29:14<1:00:49, 53.67s/it]Time:  4.991344667971134
 33%|███▎      | 33/100 [30:09<1:00:20, 54.03s/it]Time:  4.981711136177182
 34%|███▍      | 34/100 [31:04<59:41, 54.26s/it]  Time:  5.004895872436464
 35%|███▌      | 35/100 [31:59<59:01, 54.49s/it]Time:  5.014451396651566
 36%|███▌      | 36/100 [32:53<58:14, 54.60s/it]Time:  4.915037648752332
 37%|███▋      | 37/100 [33:48<57:15, 54.53s/it]Time:  4.959304094314575
 38%|███▊      | 38/100 [34:43<56:29, 54.68s/it]Time:  4.954589347355068
 39%|███▉      | 39/100 [35:38<55:46, 54.86s/it]Time:  4.909002719447017
 40%|████      | 40/100 [36:33<54:59, 54.99s/it]Time:  4.942771205678582
 41%|████      | 41/100 [37:28<54:01, 54.94s/it]Time:  4.9472939213737845
 42%|████▏     | 42/100 [38:23<53:04, 54.90s/it]Time:  4.975056575611234
 43%|████▎     | 43/100 [39:18<52:10, 54.92s/it]Time:  4.980626992881298
 44%|████▍     | 44/100 [40:13<51:19, 55.00s/it]Time:  4.937084005214274
 45%|████▌     | 45/100 [41:08<50:24, 55.00s/it]Time:  4.9841561103239655
 46%|████▌     | 46/100 [42:03<49:22, 54.86s/it]Time:  4.958590170368552
 47%|████▋     | 47/100 [42:58<48:27, 54.86s/it]Time:  4.8790412452071905
 48%|████▊     | 48/100 [43:52<47:30, 54.82s/it]Time:  4.941505244001746
 49%|████▉     | 49/100 [44:47<46:39, 54.89s/it]Time:  4.877790353260934
 50%|█████     | 50/100 [45:42<45:46, 54.94s/it]Time:  4.864776705391705
 51%|█████     | 51/100 [46:38<44:55, 55.01s/it]Time:  4.799328933469951
 52%|█████▏    | 52/100 [47:33<44:03, 55.08s/it]Time:  4.789897000417113
 53%|█████▎    | 53/100 [48:28<43:10, 55.11s/it]Time:  4.786319161765277
 54%|█████▍    | 54/100 [49:23<42:14, 55.11s/it]Time:  4.789707490243018
 55%|█████▌    | 55/100 [50:19<41:29, 55.31s/it]Time:  4.689553924836218
 56%|█████▌    | 56/100 [51:14<40:32, 55.28s/it]Time:  4.495072991587222
 57%|█████▋    | 57/100 [52:09<39:35, 55.25s/it]Time:  4.505910143256187
 58%|█████▊    | 58/100 [53:04<38:35, 55.12s/it]Time:  4.483362106606364
 59%|█████▉    | 59/100 [53:59<37:39, 55.10s/it]Time:  4.492920617572963
 60%|██████    | 60/100 [54:54<36:44, 55.12s/it]Time:  4.478820607997477
 61%|██████    | 61/100 [55:50<35:55, 55.27s/it]Time:  4.493132977746427
 62%|██████▏   | 62/100 [56:45<34:58, 55.23s/it]Time:  4.874701620079577
 63%|██████▎   | 63/100 [57:41<34:07, 55.34s/it]Time:  4.728721688501537
 64%|██████▍   | 64/100 [58:36<33:11, 55.31s/it]Time:  4.9944303669035435
 65%|██████▌   | 65/100 [59:32<32:19, 55.42s/it]Time:  4.7899783458560705
 66%|██████▌   | 66/100 [1:00:27<31:21, 55.34s/it]Time:  4.738216148689389
 67%|██████▋   | 67/100 [1:01:23<30:30, 55.47s/it]Time:  4.782700412906706
 68%|██████▊   | 68/100 [1:02:18<29:35, 55.49s/it]Time:  4.649657082743943
 69%|██████▉   | 69/100 [1:03:14<28:39, 55.47s/it]Time:  4.647718141786754
 70%|███████   | 70/100 [1:04:09<27:41, 55.38s/it]Time:  4.701167114078999
 71%|███████   | 71/100 [1:05:04<26:43, 55.31s/it]Time:  4.7003419660031796
 72%|███████▏  | 72/100 [1:05:59<25:44, 55.17s/it]Time:  4.702912009321153
 73%|███████▎  | 73/100 [1:06:53<24:46, 55.04s/it]Time:  4.650355473160744
 74%|███████▍  | 74/100 [1:07:48<23:49, 55.00s/it]Time:  4.654613467864692
 75%|███████▌  | 75/100 [1:08:44<22:56, 55.06s/it]Time:  4.688685365952551
 76%|███████▌  | 76/100 [1:09:39<22:03, 55.16s/it]Time:  5.095458808355033
 77%|███████▋  | 77/100 [1:10:34<21:10, 55.24s/it]Time:  4.863626457750797
 78%|███████▊  | 78/100 [1:11:29<20:13, 55.18s/it]Time:  4.830293768085539
 79%|███████▉  | 79/100 [1:12:25<19:18, 55.17s/it]Time:  4.854281610809267
 80%|████████  | 80/100 [1:13:20<18:23, 55.16s/it]Time:  4.849124951288104
 81%|████████  | 81/100 [1:14:15<17:27, 55.13s/it]Time:  4.92144565936178
 82%|████████▏ | 82/100 [1:15:10<16:33, 55.17s/it]Time:  4.884164028801024
 83%|████████▎ | 83/100 [1:16:05<15:37, 55.12s/it]Time:  4.875437981449068
 84%|████████▍ | 84/100 [1:17:00<14:40, 55.06s/it]Time:  4.915994179435074
 85%|████████▌ | 85/100 [1:17:55<13:45, 55.01s/it]Time:  4.885434766300023
 86%|████████▌ | 86/100 [1:18:50<12:51, 55.08s/it]Time:  4.843660719692707
 87%|████████▋ | 87/100 [1:19:45<11:55, 55.06s/it]Time:  4.878384274430573
 88%|████████▊ | 88/100 [1:20:40<11:01, 55.09s/it]Time:  4.866385944187641
 89%|████████▉ | 89/100 [1:21:35<10:06, 55.15s/it]Time:  4.853642527014017
 90%|█████████ | 90/100 [1:22:31<09:11, 55.16s/it]Time:  4.857948926277459
 91%|█████████ | 91/100 [1:23:26<08:16, 55.18s/it]Time:  4.811148934997618
 92%|█████████▏| 92/100 [1:24:21<07:20, 55.10s/it]Time:  4.813270527869463
 93%|█████████▎| 93/100 [1:25:16<06:25, 55.08s/it]Time:  4.836574096232653
 94%|█████████▍| 94/100 [1:26:11<05:30, 55.14s/it]Time:  4.820925835520029
 95%|█████████▌| 95/100 [1:27:06<04:35, 55.13s/it]Time:  4.874176675453782
 96%|█████████▌| 96/100 [1:28:01<03:40, 55.05s/it]Time:  4.819522777572274
 97%|█████████▋| 97/100 [1:28:56<02:45, 55.09s/it]Time:  4.764988029375672
 98%|█████████▊| 98/100 [1:29:51<01:50, 55.03s/it]Time:  3.9536815527826548
 99%|█████████▉| 99/100 [1:30:39<00:52, 52.88s/it]Time:  1.331864731386304
100%|██████████| 100/100 [1:31:09<00:00, 46.04s/it]100%|██████████| 100/100 [1:31:09<00:00, 54.70s/it]
Post-training time: 5473.55 seconds
GPU memory allocated: 374.76 MB, peak: 999.48 MB
Train results:
                 train_loss     test_loss  top1_accuracy  top5_accuracy   test_time
Init.      0           NaN  2.417717e+00          11.73          50.17  126.391562
Pre-Prune  0           NaN  2.417717e+00          11.73          50.17  126.391562
Post-Prune 0           NaN  4.971449e+10          10.00          49.94    3.907993
Final      100    1.883179  1.866295e+00          33.76          82.04    1.331865
Prune results:
             module   param  sparsity     size             shape     flops    score mean  score variance  score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.559028     1728     (64, 3, 3, 3)   1769472  4.450377e-05    8.434832e-08   0.076903    1.499241e-04        6.385167e-08       0.259069      True
1    layers.0.conv    bias  1.000000       64             (64,)     65536  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
2    layers.1.conv  weight  0.509576    36864    (64, 64, 3, 3)  37748736  2.075448e-06    4.122782e-09   0.076509    2.485491e-05        3.509323e-09       0.916251      True
3    layers.1.conv    bias  1.000000       64             (64,)     65536  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
4    layers.3.conv  weight  0.523682    73728   (128, 64, 3, 3)  18874368  1.036265e-06    1.686676e-09   0.076402    1.571875e-05        1.440671e-09       1.158912      True
5    layers.3.conv    bias  1.000000      128            (128,)     32768  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
6    layers.4.conv  weight  0.544054   147456  (128, 128, 3, 3)  37748736  5.169903e-07    5.230483e-10   0.076233    7.786418e-06        4.626872e-10       1.148154      True
7    layers.4.conv    bias  1.000000      128            (128,)     32768  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
8    layers.6.conv  weight  0.553480   294912  (256, 128, 3, 3)  18874368  2.581017e-07    2.053402e-10   0.076117    4.942225e-06        1.809812e-10       1.457521      True
9    layers.6.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
10   layers.7.conv  weight  0.574329   589824  (256, 256, 3, 3)  37748736  1.255479e-07    5.263318e-11   0.074051    2.530799e-06        4.624398e-11       1.492726      True
11   layers.7.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
12   layers.8.conv  weight  0.562378   589824  (256, 256, 3, 3)  37748736  1.240259e-07    3.422638e-11   0.073153    2.217017e-06        2.932659e-11       1.307650      True
13   layers.8.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
14  layers.10.conv  weight  0.569565  1179648  (512, 256, 3, 3)  18874368  6.157175e-08    1.396250e-11   0.072633    1.547428e-06        1.157176e-11       1.825420      True
15  layers.10.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
16  layers.11.conv  weight  0.593061  2359296  (512, 512, 3, 3)  37748736  2.846883e-08    4.243504e-12   0.067166    8.836324e-07        3.463507e-12       2.084750      True
17  layers.11.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
18  layers.12.conv  weight  0.586743  2359296  (512, 512, 3, 3)  37748736  2.840790e-08    3.657216e-12   0.067023    8.970010e-07        2.853413e-12       2.116291      True
19  layers.12.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
20  layers.14.conv  weight  0.643379  2359296  (512, 512, 3, 3)   9437184  2.694333e-08    3.812700e-12   0.063567    8.725644e-07        3.052057e-12       2.058638      True
21  layers.14.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
22  layers.15.conv  weight  0.708177  2359296  (512, 512, 3, 3)   9437184  2.711094e-08    4.234990e-12   0.063963    8.358491e-07        3.537082e-12       1.972016      True
23  layers.15.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
24  layers.16.conv  weight  0.705980  2359296  (512, 512, 3, 3)   9437184  2.642919e-08    5.443440e-12   0.062354    9.434242e-07        4.554089e-12       2.225817      True
25  layers.16.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
26              fc  weight  0.567773     5120         (10, 512)      5120  1.443845e-05    5.200133e-09   0.073925    3.773056e-05        3.985007e-09       0.193180      True
27              fc    bias  1.000000       10             (10,)        10  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
Parameter Sparsity: 9289139/14719818 (0.6311)
FLOP Sparsity: 178985394/313478154 (0.5710)
Saving results.
