Namespace(dataset='cifar10', model='vgg16', model_class='lottery', dense_classifier=False, pretrained=False, optimizer='adam', train_batch_size=256, test_batch_size=256, pre_epochs=0, post_epochs=100, lr=0.001, lr_drops=[], lr_drop_rate=0.1, weight_decay=0.0, pruner='rand', compression=0.05, quantization=True, prune_epochs=1, compression_schedule='exponential', mask_scope='global', prune_dataset_ratio=10, prune_batch_size=256, prune_bias=False, prune_batchnorm=False, prune_residual=False, prune_train_mode=False, reinitialize=False, shuffle=False, invert=False, pruner_list=[], prune_epoch_list=[], compression_list=[], level_list=[], experiment='singleshot', expid='QUANTIZED-cifar10-vgg16-lottery-rand-preEpochs0-postEpochs100-compression0.05', result_dir='Results/data', gpu=0, workers=4, no_cuda=False, seed=1, verbose=False)
Loading cifar10 dataset.
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with rand for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.74s/it]100%|██████████| 1/1 [00:02<00:00,  2.74s/it]
Post-Training for 100 epochs.
  0%|          | 0/100 [00:00<?, ?it/s]Time:  3.845552389975637
  1%|          | 1/100 [01:01<1:42:02, 61.84s/it]Time:  3.719462762004696
  2%|▏         | 2/100 [01:36<1:14:30, 45.62s/it]Time:  3.7291474730009213
  3%|▎         | 3/100 [02:10<1:05:34, 40.56s/it]Time:  3.7726567740319297
  4%|▍         | 4/100 [02:44<1:00:55, 38.08s/it]Time:  3.866634864010848
  5%|▌         | 5/100 [03:19<58:00, 36.64s/it]  Time:  3.8715760440099984
  6%|▌         | 6/100 [03:52<55:45, 35.60s/it]Time:  3.903397473040968
  7%|▋         | 7/100 [04:26<54:29, 35.15s/it]Time:  3.8391800170065835
  8%|▊         | 8/100 [05:00<53:03, 34.61s/it]Time:  3.814209852949716
  9%|▉         | 9/100 [05:33<51:51, 34.20s/it]Time:  3.5122634570579976
 10%|█         | 10/100 [06:07<51:07, 34.08s/it]Time:  4.002082098973915
 11%|█         | 11/100 [06:40<50:02, 33.74s/it]Time:  3.958695865003392
 12%|█▏        | 12/100 [07:14<49:29, 33.75s/it]Time:  4.1551284230081365
 13%|█▎        | 13/100 [07:47<48:40, 33.57s/it]Time:  4.128199097001925
 14%|█▍        | 14/100 [08:20<47:47, 33.35s/it]Time:  4.168879410019144
 15%|█▌        | 15/100 [08:53<47:24, 33.46s/it]Time:  4.410980883054435
 16%|█▌        | 16/100 [09:26<46:20, 33.10s/it]Time:  4.40989815001376
 17%|█▋        | 17/100 [09:59<45:51, 33.15s/it]Time:  4.371438288013451
 18%|█▊        | 18/100 [10:32<45:23, 33.21s/it]Time:  4.4026630509179085
 19%|█▉        | 19/100 [11:06<44:52, 33.24s/it]Time:  4.389981614076532
 20%|██        | 20/100 [11:39<44:21, 33.27s/it]Time:  4.385361914988607
 21%|██        | 21/100 [12:12<43:51, 33.31s/it]Time:  4.331899087061174
 22%|██▏       | 22/100 [12:46<43:17, 33.30s/it]Time:  4.396980813005939
 23%|██▎       | 23/100 [13:19<42:42, 33.27s/it]Time:  4.358917444944382
 24%|██▍       | 24/100 [13:52<42:04, 33.22s/it]Time:  4.367632052977569
 25%|██▌       | 25/100 [14:23<40:51, 32.68s/it]Time:  4.3691940599819645
 26%|██▌       | 26/100 [14:56<40:12, 32.60s/it]Time:  4.280695052002557
 27%|██▋       | 27/100 [15:30<40:23, 33.20s/it]Time:  4.388477027998306
 28%|██▊       | 28/100 [16:04<39:52, 33.23s/it]Time:  4.371332778944634
 29%|██▉       | 29/100 [16:36<39:08, 33.08s/it]Time:  4.361423349007964
 30%|███       | 30/100 [17:10<38:41, 33.16s/it]Time:  4.384586737956852
 31%|███       | 31/100 [17:43<38:03, 33.09s/it]Time:  4.3943710720632225
 32%|███▏      | 32/100 [18:14<36:57, 32.61s/it]Time:  4.367497848928906
 33%|███▎      | 33/100 [18:44<35:22, 31.68s/it]Time:  4.3481249859323725
 34%|███▍      | 34/100 [19:11<33:30, 30.46s/it]Time:  4.387565104989335
 35%|███▌      | 35/100 [19:43<33:27, 30.88s/it]Time:  1.9895998319843784
 36%|███▌      | 36/100 [20:13<32:44, 30.70s/it]Time:  1.9246452719671652
 37%|███▋      | 37/100 [20:45<32:32, 30.99s/it]Time:  3.151739408960566
 38%|███▊      | 38/100 [21:19<32:54, 31.85s/it]Time:  2.8690108259906992
 39%|███▉      | 39/100 [21:52<32:42, 32.17s/it]Time:  3.4147530490299687
 40%|████      | 40/100 [22:25<32:35, 32.60s/it]Time:  3.7093752310611308
 41%|████      | 41/100 [22:59<32:22, 32.93s/it]Time:  3.67500765202567
 42%|████▏     | 42/100 [23:32<31:42, 32.81s/it]Time:  3.9297707290388644
 43%|████▎     | 43/100 [24:05<31:20, 32.99s/it]Time:  3.9547532830620185
 44%|████▍     | 44/100 [24:39<30:58, 33.19s/it]Time:  4.01426916802302
 45%|████▌     | 45/100 [25:12<30:25, 33.18s/it]Time:  4.199417480034754
 46%|████▌     | 46/100 [25:45<29:46, 33.09s/it]Time:  4.283070506993681
 47%|████▋     | 47/100 [26:18<29:22, 33.25s/it]Time:  4.181267459061928
 48%|████▊     | 48/100 [26:51<28:44, 33.17s/it]Time:  4.149895744980313
 49%|████▉     | 49/100 [27:26<28:27, 33.49s/it]Time:  4.24125640199054
 50%|█████     | 50/100 [27:59<27:52, 33.46s/it]Time:  4.243901859968901
 51%|█████     | 51/100 [28:32<27:12, 33.31s/it]Time:  4.224868896999396
 52%|█████▏    | 52/100 [29:05<26:42, 33.38s/it]Time:  4.229006770066917
 53%|█████▎    | 53/100 [29:39<26:07, 33.35s/it]Time:  4.250756068970077
 54%|█████▍    | 54/100 [30:12<25:36, 33.41s/it]Time:  4.268524554907344
 55%|█████▌    | 55/100 [30:46<25:03, 33.41s/it]Time:  4.2360104060499
 56%|█████▌    | 56/100 [31:19<24:29, 33.40s/it]Time:  4.124149244977161
 57%|█████▋    | 57/100 [31:52<23:54, 33.37s/it]Time:  4.063598388922401
 58%|█████▊    | 58/100 [32:25<23:15, 33.23s/it]Time:  4.129450395004824
 59%|█████▉    | 59/100 [32:58<22:37, 33.11s/it]Time:  4.067978931008838
 60%|██████    | 60/100 [33:31<22:02, 33.06s/it]Time:  4.160997158032842
 61%|██████    | 61/100 [34:04<21:32, 33.14s/it]Time:  4.293982804985717
 62%|██████▏   | 62/100 [34:38<21:00, 33.18s/it]Time:  4.219278228003532
 63%|██████▎   | 63/100 [35:11<20:29, 33.22s/it]Time:  4.187357601011172
 64%|██████▍   | 64/100 [35:45<20:00, 33.35s/it]Time:  4.0670529790222645
 65%|██████▌   | 65/100 [36:19<19:36, 33.62s/it]Time:  3.9760632819961756
 66%|██████▌   | 66/100 [36:53<19:11, 33.88s/it]Time:  3.6468760109273717
 67%|██████▋   | 67/100 [37:27<18:31, 33.69s/it]Time:  3.7659668979467824
 68%|██████▊   | 68/100 [38:00<17:54, 33.58s/it]Time:  3.623202577000484
 69%|██████▉   | 69/100 [38:33<17:15, 33.40s/it]Time:  3.932164903031662
 70%|███████   | 70/100 [39:06<16:35, 33.19s/it]Time:  2.9046679029706866
 71%|███████   | 71/100 [39:38<15:54, 32.93s/it]Time:  3.652070160023868
 72%|███████▏  | 72/100 [40:12<15:27, 33.14s/it]Time:  3.442579270922579
 73%|███████▎  | 73/100 [40:46<15:03, 33.45s/it]Time:  4.249946858966723
 74%|███████▍  | 74/100 [41:19<14:26, 33.32s/it]Time:  4.2074252989841625
 75%|███████▌  | 75/100 [41:51<13:45, 33.02s/it]Time:  4.2309878000523895
 76%|███████▌  | 76/100 [42:25<13:15, 33.16s/it]Time:  4.252066400018521
 77%|███████▋  | 77/100 [42:56<12:33, 32.75s/it]Time:  4.388163469964638
 78%|███████▊  | 78/100 [43:29<11:57, 32.61s/it]Time:  4.39514016197063
 79%|███████▉  | 79/100 [44:05<11:48, 33.74s/it]Time:  4.0699294549413025
 80%|████████  | 80/100 [44:38<11:08, 33.44s/it]Time:  4.002578882034868
 81%|████████  | 81/100 [45:12<10:39, 33.65s/it]Time:  3.974476750008762
 82%|████████▏ | 82/100 [45:47<10:11, 33.97s/it]Time:  3.578256975975819
 83%|████████▎ | 83/100 [46:20<09:33, 33.74s/it]Time:  3.464402013923973
 84%|████████▍ | 84/100 [46:52<08:53, 33.36s/it]Time:  3.8272347189486027
 85%|████████▌ | 85/100 [47:27<08:28, 33.87s/it]Time:  3.9864408689318225
 86%|████████▌ | 86/100 [48:02<07:57, 34.13s/it]Time:  3.7142135039903224
 87%|████████▋ | 87/100 [48:38<07:29, 34.61s/it]Time:  3.853195767966099
 88%|████████▊ | 88/100 [49:13<06:57, 34.76s/it]Time:  3.691208446980454
 89%|████████▉ | 89/100 [49:47<06:20, 34.63s/it]Time:  3.204986423952505
 90%|█████████ | 90/100 [50:21<05:42, 34.27s/it]Time:  3.535392213962041
 91%|█████████ | 91/100 [50:56<05:11, 34.58s/it]Time:  3.2260203149635345
 92%|█████████▏| 92/100 [51:31<04:37, 34.64s/it]Time:  3.835748273995705
 93%|█████████▎| 93/100 [52:06<04:03, 34.84s/it]Time:  3.85582422290463
 94%|█████████▍| 94/100 [52:40<03:27, 34.58s/it]Time:  3.787321722949855
 95%|█████████▌| 95/100 [53:14<02:51, 34.34s/it]Time:  3.801577989012003
 96%|█████████▌| 96/100 [53:49<02:18, 34.57s/it]Time:  4.014566755970009
 97%|█████████▋| 97/100 [54:27<01:47, 35.75s/it]Time:  2.86345002905
 98%|█████████▊| 98/100 [55:00<01:09, 34.73s/it]Time:  1.4563717490527779
 99%|█████████▉| 99/100 [55:18<00:29, 29.77s/it]Time:  0.7438504869351164
100%|██████████| 100/100 [55:26<00:00, 23.27s/it]100%|██████████| 100/100 [55:26<00:00, 33.27s/it]
Post-training time: 3330.35 seconds
GPU memory allocated: 376.33 MB, peak: 876.67 MB
Train results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy  test_time
Init.      0           NaN   2.417717          11.73          50.17  45.442815
Pre-Prune  0           NaN   2.417717          11.73          50.17  45.442815
Post-Prune 0           NaN   2.322258          10.43          50.00   3.800877
Final      100     0.08262   0.626844          88.31          99.11   0.743850
Prune results:
             module   param  sparsity     size             shape     flops  score mean  score variance    score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.881944     1728     (64, 3, 3, 3)   1769472   -0.027050        1.005513   -46.742134        0.805873            0.356814   1.392548e+03      True
1    layers.0.conv    bias  1.000000       64             (64,)     65536    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
2    layers.1.conv  weight  0.892659    36864    (64, 64, 3, 3)  37748736    0.000872        0.996963    32.139240        0.796522            0.362516   2.936300e+04      True
3    layers.1.conv    bias  1.000000       64             (64,)     65536    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
4    layers.3.conv  weight  0.892632    73728   (128, 64, 3, 3)  18874368    0.000958        0.993901    70.645149        0.796130            0.360079   5.869704e+04      True
5    layers.3.conv    bias  1.000000      128            (128,)     32768    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
6    layers.4.conv  weight  0.891283   147456  (128, 128, 3, 3)  37748736    0.001246        0.994699   183.733383        0.795026            0.362635   1.172313e+05      True
7    layers.4.conv    bias  1.000000      128            (128,)     32768    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
8    layers.6.conv  weight  0.890771   294912  (256, 128, 3, 3)  18874368   -0.001563        1.001676  -460.913849        0.798645            0.363844   2.355301e+05      True
9    layers.6.conv    bias  1.000000      256            (256,)     16384    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
10   layers.7.conv  weight  0.891366   589824  (256, 256, 3, 3)  37748736    0.000378        0.999599   222.715530        0.797776            0.363152   4.705474e+05      True
11   layers.7.conv    bias  1.000000      256            (256,)     16384    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
12   layers.8.conv  weight  0.891573   589824  (256, 256, 3, 3)  37748736    0.000403        0.998166   237.463287        0.796769            0.363324   4.699538e+05      True
13   layers.8.conv    bias  1.000000      256            (256,)     16384    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
14  layers.10.conv  weight  0.891617  1179648  (512, 256, 3, 3)  18874368    0.000511        0.999746   602.936768        0.797616            0.363555   9.409056e+05      True
15  layers.10.conv    bias  1.000000      512            (512,)      8192    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
16  layers.11.conv  weight  0.890967  2359296  (512, 512, 3, 3)  37748736   -0.000190        1.001348  -448.645966        0.798652            0.363503   1.884256e+06      True
17  layers.11.conv    bias  1.000000      512            (512,)      8192    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
18  layers.12.conv  weight  0.891159  2359296  (512, 512, 3, 3)  37748736    0.000249        0.999722   588.484009        0.797592            0.363569   1.881756e+06      True
19  layers.12.conv    bias  1.000000      512            (512,)      8192    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
20  layers.14.conv  weight  0.891282  2359296  (512, 512, 3, 3)   9437184   -0.000339        0.999254  -799.927002        0.797659            0.362994   1.881914e+06      True
21  layers.14.conv    bias  1.000000      512            (512,)      2048    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
22  layers.15.conv  weight  0.891270  2359296  (512, 512, 3, 3)   9437184    0.000708        1.000298  1670.380615        0.797886            0.363677   1.882448e+06      True
23  layers.15.conv    bias  1.000000      512            (512,)      2048    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
24  layers.16.conv  weight  0.891274  2359296  (512, 512, 3, 3)   9437184    0.000980        0.999300  2311.431885        0.797918            0.362629   1.882524e+06      True
25  layers.16.conv    bias  1.000000      512            (512,)      2048    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
26              fc  weight  0.895508     5120         (10, 512)      5120    0.007839        0.966788    40.134628        0.782462            0.354602   4.006208e+03      True
27              fc    bias  1.000000       10             (10,)        10    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
Parameter Sparsity: 13119511/14719818 (0.8913)
FLOP Sparsity: 279482611/313478154 (0.8916)
Saving results.
