Namespace(dataset='cifar10', model='vgg16', model_class='lottery', dense_classifier=False, pretrained=False, optimizer='adam', train_batch_size=256, test_batch_size=256, pre_epochs=0, post_epochs=100, lr=0.001, lr_drops=[], lr_drop_rate=0.1, weight_decay=0.0, pruner='snip', compression=2.0, quantization=False, prune_epochs=1, compression_schedule='exponential', mask_scope='global', prune_dataset_ratio=10, prune_batch_size=256, prune_bias=False, prune_batchnorm=False, prune_residual=False, prune_train_mode=False, reinitialize=False, shuffle=False, invert=False, pruner_list=[], prune_epoch_list=[], compression_list=[], level_list=[], experiment='singleshot', expid='cifar10-vgg16-lottery-snip-preEpochs0-postEpochs100-compression2', result_dir='Results/data', gpu=0, workers=4, no_cuda=False, seed=1, verbose=False)
Loading cifar10 dataset.
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with snip for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:25<00:00, 25.54s/it]100%|██████████| 1/1 [00:25<00:00, 25.54s/it]
Post-Training for 100 epochs.
  0%|          | 0/100 [00:00<?, ?it/s]Time:  2.0089911709656008
  1%|          | 1/100 [00:42<1:10:08, 42.51s/it]Time:  1.8656761620077305
  2%|▏         | 2/100 [01:12<57:26, 35.16s/it]  Time:  1.8717035859590396
  3%|▎         | 3/100 [01:43<53:47, 33.27s/it]Time:  1.5918001729878597
  4%|▍         | 4/100 [02:18<54:36, 34.13s/it]Time:  1.9585684899939224
  5%|▌         | 5/100 [02:48<51:38, 32.62s/it]Time:  1.9824379720375873
  6%|▌         | 6/100 [03:19<50:11, 32.04s/it]Time:  2.0098523939959705
  7%|▋         | 7/100 [03:50<48:56, 31.57s/it]Time:  1.8718297540326603
  8%|▊         | 8/100 [04:20<47:44, 31.14s/it]Time:  1.9876169440103695
  9%|▉         | 9/100 [04:49<46:12, 30.47s/it]Time:  2.042964000022039
 10%|█         | 10/100 [05:17<44:23, 29.59s/it]Time:  2.035089873999823
 11%|█         | 11/100 [05:46<43:39, 29.43s/it]Time:  2.0099510030122474
 12%|█▏        | 12/100 [06:14<42:35, 29.04s/it]Time:  2.0147588729741983
 13%|█▎        | 13/100 [06:43<41:55, 28.92s/it]Time:  2.010358804021962
 14%|█▍        | 14/100 [07:10<40:46, 28.44s/it]Time:  1.978759488032665
 15%|█▌        | 15/100 [07:38<40:06, 28.32s/it]Time:  1.9634397680056281
 16%|█▌        | 16/100 [08:06<39:33, 28.26s/it]Time:  2.010870061989408
 17%|█▋        | 17/100 [08:35<39:11, 28.33s/it]Time:  1.9944296239991672
 18%|█▊        | 18/100 [09:03<38:43, 28.33s/it]Time:  1.9413490780279972
 19%|█▉        | 19/100 [09:32<38:38, 28.62s/it]Time:  1.8336512640235014
 20%|██        | 20/100 [10:00<37:43, 28.29s/it]Time:  1.8339906170149334
 21%|██        | 21/100 [10:27<36:51, 27.99s/it]Time:  1.81346835300792
 22%|██▏       | 22/100 [10:55<36:11, 27.85s/it]Time:  1.9398346889647655
 23%|██▎       | 23/100 [11:22<35:44, 27.86s/it]Time:  1.983944514009636
 24%|██▍       | 24/100 [11:50<35:19, 27.89s/it]Time:  1.9473446490010247
 25%|██▌       | 25/100 [12:21<35:40, 28.54s/it]Time:  1.8173723240033723
 26%|██▌       | 26/100 [12:52<36:22, 29.49s/it]Time:  2.3887722159852274
 27%|██▋       | 27/100 [13:19<34:57, 28.73s/it]Time:  2.4123810330056585
 28%|██▊       | 28/100 [13:43<32:48, 27.34s/it]Time:  2.4324073240277357
 29%|██▉       | 29/100 [13:58<27:53, 23.57s/it]Time:  0.8371038020122796
 30%|███       | 30/100 [14:25<28:32, 24.47s/it]Time:  0.9507325680460781
 31%|███       | 31/100 [14:53<29:37, 25.77s/it]Time:  1.7509047350031324
 32%|███▏      | 32/100 [15:22<30:12, 26.66s/it]Time:  1.8503594030044042
 33%|███▎      | 33/100 [15:52<31:00, 27.77s/it]Time:  1.9645337399560958
 34%|███▍      | 34/100 [16:20<30:33, 27.79s/it]Time:  1.771046195004601
 35%|███▌      | 35/100 [16:48<30:01, 27.72s/it]Time:  1.7943829820142128
 36%|███▌      | 36/100 [17:16<29:32, 27.69s/it]Time:  1.7592520479811355
 37%|███▋      | 37/100 [17:43<29:06, 27.72s/it]Time:  1.5136098089860752
 38%|███▊      | 38/100 [18:12<28:52, 27.94s/it]Time:  1.8468560089822859
 39%|███▉      | 39/100 [18:39<28:15, 27.80s/it]Time:  1.9194064440089278
 40%|████      | 40/100 [19:06<27:36, 27.61s/it]Time:  2.0701675979653373
 41%|████      | 41/100 [19:33<26:55, 27.39s/it]Time:  2.0892043840140104
 42%|████▏     | 42/100 [20:01<26:30, 27.42s/it]Time:  2.1621269379975274
 43%|████▎     | 43/100 [20:29<26:12, 27.59s/it]Time:  1.7264042409951799
 44%|████▍     | 44/100 [21:03<27:41, 29.66s/it]Time:  2.1286236890009604
 45%|████▌     | 45/100 [21:33<27:13, 29.71s/it]Time:  2.070789677032735
 46%|████▌     | 46/100 [22:04<27:09, 30.17s/it]Time:  1.9930452039698139
 47%|████▋     | 47/100 [22:36<27:00, 30.58s/it]Time:  1.968355146003887
 48%|████▊     | 48/100 [23:07<26:38, 30.73s/it]Time:  2.0157153999898583
 49%|████▉     | 49/100 [23:41<26:53, 31.64s/it]Time:  1.9623161539784633
 50%|█████     | 50/100 [24:11<25:55, 31.12s/it]Time:  2.017838641011622
 51%|█████     | 51/100 [24:45<26:18, 32.22s/it]Time:  1.0238013449707069
 52%|█████▏    | 52/100 [25:16<25:22, 31.72s/it]Time:  0.7020629979670048
 53%|█████▎    | 53/100 [25:46<24:21, 31.11s/it]Time:  1.9207994940225035
 54%|█████▍    | 54/100 [26:17<23:52, 31.13s/it]Time:  2.1365118460380472
 55%|█████▌    | 55/100 [26:44<22:29, 29.98s/it]Time:  1.968423662998248
 56%|█████▌    | 56/100 [27:16<22:28, 30.64s/it]Time:  2.02065112098353
 57%|█████▋    | 57/100 [27:48<22:10, 30.93s/it]Time:  1.7312012960319407
 58%|█████▊    | 58/100 [28:20<21:55, 31.33s/it]Time:  1.9968509480240755
 59%|█████▉    | 59/100 [28:48<20:35, 30.15s/it]Time:  1.8715379679924808
 60%|██████    | 60/100 [29:16<19:45, 29.65s/it]Time:  1.769658174016513
 61%|██████    | 61/100 [29:48<19:44, 30.36s/it]Time:  1.6508170089800842
 62%|██████▏   | 62/100 [30:19<19:14, 30.39s/it]Time:  1.7926184530369937
 63%|██████▎   | 63/100 [30:47<18:25, 29.88s/it]Time:  1.5664731219876558
 64%|██████▍   | 64/100 [31:16<17:48, 29.68s/it]Time:  1.8523360710241832
 65%|██████▌   | 65/100 [31:46<17:14, 29.56s/it]Time:  1.9052255969727412
 66%|██████▌   | 66/100 [32:14<16:37, 29.33s/it]Time:  1.9561117900302634
 67%|██████▋   | 67/100 [32:45<16:17, 29.61s/it]Time:  1.6959151499904692
 68%|██████▊   | 68/100 [33:17<16:14, 30.47s/it]Time:  1.3721634519752115
 69%|██████▉   | 69/100 [33:48<15:47, 30.58s/it]Time:  1.7001280050026253
 70%|███████   | 70/100 [34:19<15:21, 30.72s/it]Time:  1.876488083973527
 71%|███████   | 71/100 [34:47<14:22, 29.74s/it]Time:  1.8667203589575365
 72%|███████▏  | 72/100 [35:16<13:46, 29.51s/it]Time:  1.8284379449905828
 73%|███████▎  | 73/100 [35:44<13:10, 29.28s/it]Time:  2.417625277012121
 74%|███████▍  | 74/100 [36:11<12:23, 28.59s/it]Time:  2.425584254960995
 75%|███████▌  | 75/100 [36:38<11:40, 28.03s/it]Time:  2.422924556012731
 76%|███████▌  | 76/100 [37:02<10:42, 26.78s/it]Time:  2.4191287949797697
 77%|███████▋  | 77/100 [37:26<09:57, 25.98s/it]Time:  2.399257938028313
 78%|███████▊  | 78/100 [37:48<09:07, 24.89s/it]Time:  2.4038876609993167
 79%|███████▉  | 79/100 [38:11<08:28, 24.20s/it]Time:  2.2680164049961604
 80%|████████  | 80/100 [38:38<08:18, 24.93s/it]Time:  2.2422911419998854
 81%|████████  | 81/100 [39:04<08:02, 25.38s/it]Time:  1.1197958340053447
 82%|████████▏ | 82/100 [39:29<07:34, 25.27s/it]Time:  1.7075627250014804
 83%|████████▎ | 83/100 [39:57<07:22, 26.01s/it]Time:  1.0708165119867772
 84%|████████▍ | 84/100 [40:24<07:00, 26.28s/it]Time:  1.346537057950627
 85%|████████▌ | 85/100 [40:51<06:41, 26.76s/it]Time:  1.3042082379688509
 86%|████████▌ | 86/100 [41:19<06:17, 26.94s/it]Time:  1.2956770629971288
 87%|████████▋ | 87/100 [41:47<05:56, 27.44s/it]Time:  1.6963099319837056
 88%|████████▊ | 88/100 [42:17<05:35, 27.96s/it]Time:  1.8003525389940478
 89%|████████▉ | 89/100 [42:44<05:05, 27.77s/it]Time:  1.9985696240328252
 90%|█████████ | 90/100 [43:14<04:44, 28.47s/it]Time:  1.9716072030132636
 91%|█████████ | 91/100 [43:41<04:13, 28.16s/it]Time:  1.8860625449451618
 92%|█████████▏| 92/100 [44:09<03:43, 27.98s/it]Time:  1.7708431040518917
 93%|█████████▎| 93/100 [44:37<03:15, 27.97s/it]Time:  1.755348535021767
 94%|█████████▍| 94/100 [45:04<02:46, 27.69s/it]Time:  1.84032991604181
 95%|█████████▌| 95/100 [45:32<02:18, 27.64s/it]Time:  1.8572403610451147
 96%|█████████▌| 96/100 [46:03<01:55, 28.91s/it]Time:  1.5008467889856547
 97%|█████████▋| 97/100 [46:31<01:25, 28.59s/it]Time:  1.875790930003859
 98%|█████████▊| 98/100 [47:03<00:58, 29.39s/it]Time:  1.915197356021963
 99%|█████████▉| 99/100 [47:33<00:29, 29.71s/it]Time:  1.6367051470442675
100%|██████████| 100/100 [48:05<00:00, 30.52s/it]100%|██████████| 100/100 [48:05<00:00, 28.86s/it]
Post-training time: 2888.98 seconds
GPU memory allocated: 435.85 MB, peak: 1060.06 MB
Train results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy   test_time
Init.      0           NaN   2.417717          11.73          50.17  127.927749
Pre-Prune  0           NaN   2.417717          11.73          50.17  127.927749
Post-Prune 0           NaN   2.302605          10.00          50.00    1.345787
Final      100    0.445236   0.538281          81.27          99.16    1.636705
Prune results:
             module   param  sparsity     size             shape     flops    score mean  score variance  score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.629630     1728     (64, 3, 3, 3)   1769472  2.224966e-06    6.371801e-12   0.003845    2.224966e-06        6.371801e-12       0.003845      True
1    layers.0.conv    bias  1.000000       64             (64,)     65536  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
2    layers.1.conv  weight  0.132243    36864    (64, 64, 3, 3)  37748736  3.802294e-07    3.769638e-13   0.014017    3.802294e-07        3.769638e-13       0.014017      True
3    layers.1.conv    bias  1.000000       64             (64,)     65536  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
4    layers.3.conv  weight  0.103339    73728   (128, 64, 3, 3)  18874368  3.019000e-07    2.926917e-13   0.022258    3.019000e-07        2.926917e-13       0.022258      True
5    layers.3.conv    bias  1.000000      128            (128,)     32768  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
6    layers.4.conv  weight  0.048699   147456  (128, 128, 3, 3)  37748736  1.766793e-07    1.293368e-13   0.026052    1.766793e-07        1.293368e-13       0.026052      True
7    layers.4.conv    bias  1.000000      128            (128,)     32768  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
8    layers.6.conv  weight  0.033139   294912  (256, 128, 3, 3)  18874368  1.371369e-07    8.621578e-14   0.040443    1.371369e-07        8.621578e-14       0.040443      True
9    layers.6.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
10   layers.7.conv  weight  0.017139   589824  (256, 256, 3, 3)  37748736  9.237761e-08    4.426949e-14   0.054487    9.237761e-08        4.426949e-14       0.054487      True
11   layers.7.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
12   layers.8.conv  weight  0.017151   589824  (256, 256, 3, 3)  37748736  9.919984e-08    4.516633e-14   0.058510    9.919984e-08        4.516633e-14       0.058510      True
13   layers.8.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
14  layers.10.conv  weight  0.011635  1179648  (512, 256, 3, 3)  18874368  8.549011e-08    3.122918e-14   0.100848    8.549011e-08        3.122918e-14       0.100848      True
15  layers.10.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
16  layers.11.conv  weight  0.005010  2359296  (512, 512, 3, 3)  37748736  5.522888e-08    1.596758e-14   0.130301    5.522888e-08        1.596758e-14       0.130301      True
17  layers.11.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
18  layers.12.conv  weight  0.004598  2359296  (512, 512, 3, 3)  37748736  5.628561e-08    1.532595e-14   0.132794    5.628561e-08        1.532595e-14       0.132794      True
19  layers.12.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
20  layers.14.conv  weight  0.007277  2359296  (512, 512, 3, 3)   9437184  6.324614e-08    2.105084e-14   0.149216    6.324614e-08        2.105084e-14       0.149216      True
21  layers.14.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
22  layers.15.conv  weight  0.008122  2359296  (512, 512, 3, 3)   9437184  5.294718e-08    2.291931e-14   0.124918    5.294718e-08        2.291931e-14       0.124918      True
23  layers.15.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
24  layers.16.conv  weight  0.008925  2359296  (512, 512, 3, 3)   9437184  5.487940e-08    2.587618e-14   0.129477    5.487940e-08        2.587618e-14       0.129477      True
25  layers.16.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
26              fc  weight  0.510742     5120         (10, 512)      5120  2.506261e-06    1.778373e-11   0.012832    2.506261e-06        1.778373e-11       0.012832      True
27              fc    bias  1.000000       10             (10,)        10  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
Parameter Sparsity: 151389/14719818 (0.0103)
FLOP Sparsity: 12905701/313478154 (0.0412)
Saving results.
