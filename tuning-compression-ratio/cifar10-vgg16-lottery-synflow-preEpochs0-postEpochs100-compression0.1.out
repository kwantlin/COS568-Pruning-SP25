Namespace(dataset='cifar10', model='vgg16', model_class='lottery', dense_classifier=False, pretrained=False, optimizer='adam', train_batch_size=256, test_batch_size=256, pre_epochs=0, post_epochs=100, lr=0.001, lr_drops=[], lr_drop_rate=0.1, weight_decay=0.0, pruner='synflow', compression=0.1, quantization=False, prune_epochs=1, compression_schedule='exponential', mask_scope='global', prune_dataset_ratio=10, prune_batch_size=256, prune_bias=False, prune_batchnorm=False, prune_residual=False, prune_train_mode=False, reinitialize=False, shuffle=False, invert=False, pruner_list=[], prune_epoch_list=[], compression_list=[], level_list=[], experiment='singleshot', expid='cifar10-vgg16-lottery-synflow-preEpochs0-postEpochs100-compression0.1', result_dir='Results/data', gpu=0, workers=4, no_cuda=False, seed=1, verbose=False)
Loading cifar10 dataset.
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with synflow for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:01<00:00,  1.45s/it]100%|██████████| 1/1 [00:01<00:00,  1.45s/it]
Post-Training for 100 epochs.
  0%|          | 0/100 [00:00<?, ?it/s]Time:  3.141051357029937
  1%|          | 1/100 [00:45<1:14:38, 45.23s/it]Time:  2.8755931890336797
  2%|▏         | 2/100 [01:31<1:14:35, 45.67s/it]Time:  3.477395804016851
  3%|▎         | 3/100 [02:17<1:14:27, 46.06s/it]Time:  3.4788812689948827
  4%|▍         | 4/100 [03:03<1:13:48, 46.13s/it]Time:  3.5146771600702778
  5%|▌         | 5/100 [03:50<1:13:15, 46.26s/it]Time:  3.513379825046286
  6%|▌         | 6/100 [04:36<1:12:31, 46.29s/it]Time:  3.5664264399092644
  7%|▋         | 7/100 [05:22<1:11:41, 46.25s/it]Time:  3.5701367689762264
  8%|▊         | 8/100 [06:09<1:11:02, 46.34s/it]Time:  3.5517427440499887
  9%|▉         | 9/100 [06:55<1:10:17, 46.34s/it]Time:  3.574884798959829
 10%|█         | 10/100 [07:42<1:09:32, 46.36s/it]Time:  3.4309397210599855
 11%|█         | 11/100 [08:28<1:08:32, 46.21s/it]Time:  4.990074135013856
 12%|█▏        | 12/100 [09:14<1:08:02, 46.39s/it]Time:  3.546737155993469
 13%|█▎        | 13/100 [10:01<1:07:08, 46.30s/it]Time:  3.396157769020647
 14%|█▍        | 14/100 [10:46<1:06:11, 46.18s/it]Time:  3.4941019330872223
 15%|█▌        | 15/100 [11:32<1:05:01, 45.90s/it]Time:  3.6856405360158533
 16%|█▌        | 16/100 [12:17<1:04:03, 45.76s/it]Time:  3.91599824198056
 17%|█▋        | 17/100 [13:02<1:02:46, 45.38s/it]Time:  3.8682544470066205
 18%|█▊        | 18/100 [13:46<1:01:46, 45.21s/it]Time:  3.7463106750510633
 19%|█▉        | 19/100 [14:32<1:01:12, 45.34s/it]Time:  3.8022870179265738
 20%|██        | 20/100 [15:17<1:00:19, 45.24s/it]Time:  3.9386654059635475
 21%|██        | 21/100 [16:01<59:11, 44.96s/it]  Time:  4.09481824899558
 22%|██▏       | 22/100 [16:43<57:12, 44.01s/it]Time:  4.137180046061985
 23%|██▎       | 23/100 [17:26<56:01, 43.65s/it]Time:  4.099426463013515
 24%|██▍       | 24/100 [18:08<54:51, 43.31s/it]Time:  4.13815863605123
 25%|██▌       | 25/100 [18:52<54:14, 43.39s/it]Time:  4.144992896006443
 26%|██▌       | 26/100 [19:36<53:47, 43.61s/it]Time:  4.107101430068724
 27%|██▋       | 27/100 [20:20<53:08, 43.68s/it]Time:  4.138385321013629
 28%|██▊       | 28/100 [21:04<52:36, 43.83s/it]Time:  4.133736431016587
 29%|██▉       | 29/100 [21:48<51:51, 43.82s/it]Time:  4.046262154006399
 30%|███       | 30/100 [22:32<51:14, 43.92s/it]Time:  4.133898252970539
 31%|███       | 31/100 [23:16<50:36, 44.01s/it]Time:  4.111478523002006
 32%|███▏      | 32/100 [24:01<49:58, 44.09s/it]Time:  4.104855648940429
 33%|███▎      | 33/100 [24:45<49:10, 44.04s/it]Time:  4.119811715907417
 34%|███▍      | 34/100 [25:29<48:34, 44.16s/it]Time:  4.108097217977047
 35%|███▌      | 35/100 [26:13<47:55, 44.23s/it]Time:  4.138515054946765
 36%|███▌      | 36/100 [26:58<47:10, 44.23s/it]Time:  4.118705448927358
 37%|███▋      | 37/100 [27:42<46:32, 44.32s/it]Time:  4.152564110932872
 38%|███▊      | 38/100 [28:27<45:48, 44.33s/it]Time:  4.096885408973321
 39%|███▉      | 39/100 [29:11<45:04, 44.34s/it]Time:  4.133431943017058
 40%|████      | 40/100 [29:55<44:24, 44.40s/it]Time:  4.083923961035907
 41%|████      | 41/100 [30:40<43:42, 44.45s/it]Time:  4.130347461090423
 42%|████▏     | 42/100 [31:26<43:20, 44.83s/it]Time:  4.152478162897751
 43%|████▎     | 43/100 [32:10<42:30, 44.74s/it]Time:  4.100198545958847
 44%|████▍     | 44/100 [32:55<41:38, 44.62s/it]Time:  4.1355962010566145
 45%|████▌     | 45/100 [33:39<40:50, 44.55s/it]Time:  4.137148633948527
 46%|████▌     | 46/100 [34:24<40:07, 44.59s/it]Time:  4.248216403997503
 47%|████▋     | 47/100 [35:08<39:25, 44.63s/it]Time:  4.267800734960474
 48%|████▊     | 48/100 [35:53<38:45, 44.72s/it]Time:  4.156136628007516
 49%|████▉     | 49/100 [36:36<37:28, 44.09s/it]Time:  3.7622813190100715
 50%|█████     | 50/100 [37:21<36:51, 44.23s/it]Time:  3.840136480052024
 51%|█████     | 51/100 [38:05<36:11, 44.32s/it]Time:  3.8682637530146167
 52%|█████▏    | 52/100 [38:50<35:31, 44.40s/it]Time:  3.791364853968844
 53%|█████▎    | 53/100 [39:34<34:49, 44.47s/it]Time:  3.9086827860446647
 54%|█████▍    | 54/100 [40:19<34:06, 44.48s/it]Time:  3.9339798210421577
 55%|█████▌    | 55/100 [41:04<33:30, 44.68s/it]Time:  3.9410881840158254
 56%|█████▌    | 56/100 [41:49<32:49, 44.77s/it]Time:  3.927511784946546
 57%|█████▋    | 57/100 [42:37<32:51, 45.85s/it]Time:  3.9188493660185486
 58%|█████▊    | 58/100 [43:22<31:48, 45.43s/it]Time:  3.996356953983195
 59%|█████▉    | 59/100 [44:06<30:48, 45.08s/it]Time:  4.043818074977025
 60%|██████    | 60/100 [44:50<29:51, 44.79s/it]Time:  4.047629742068239
 61%|██████    | 61/100 [45:35<29:02, 44.69s/it]Time:  4.096578098018654
 62%|██████▏   | 62/100 [46:19<28:13, 44.55s/it]Time:  4.136599681922235
 63%|██████▎   | 63/100 [47:03<27:28, 44.54s/it]Time:  4.12327768211253
 64%|██████▍   | 64/100 [47:48<26:41, 44.47s/it]Time:  4.126138270017691
 65%|██████▌   | 65/100 [48:32<25:55, 44.45s/it]Time:  4.183697977103293
 66%|██████▌   | 66/100 [49:16<25:09, 44.39s/it]Time:  4.11606535105966
 67%|██████▋   | 67/100 [50:00<24:19, 44.23s/it]Time:  4.130637160036713
 68%|██████▊   | 68/100 [50:44<23:33, 44.18s/it]Time:  4.107428446994163
 69%|██████▉   | 69/100 [51:28<22:45, 44.05s/it]Time:  3.7595489230006933
 70%|███████   | 70/100 [52:12<21:58, 43.96s/it]Time:  3.756253318977542
 71%|███████   | 71/100 [52:55<21:12, 43.88s/it]Time:  3.9566203909926116
 72%|███████▏  | 72/100 [53:40<20:38, 44.23s/it]Time:  3.9937759980093688
 73%|███████▎  | 73/100 [54:26<20:03, 44.58s/it]Time:  3.935367379919626
 74%|███████▍  | 74/100 [55:11<19:23, 44.75s/it]Time:  3.951317416038364
 75%|███████▌  | 75/100 [55:56<18:43, 44.95s/it]Time:  3.96263324492611
 76%|███████▌  | 76/100 [56:41<17:59, 45.00s/it]Time:  3.959975318983197
 77%|███████▋  | 77/100 [57:27<17:17, 45.12s/it]Time:  3.988178506027907
 78%|███████▊  | 78/100 [58:12<16:32, 45.09s/it]Time:  3.991895610000938
 79%|███████▉  | 79/100 [58:57<15:47, 45.14s/it]Time:  3.9387109900126234
 80%|████████  | 80/100 [59:42<15:04, 45.20s/it]Time:  4.004932070965879
 81%|████████  | 81/100 [1:00:28<14:19, 45.22s/it]Time:  3.9589706249535084
 82%|████████▏ | 82/100 [1:01:13<13:32, 45.12s/it]Time:  3.9731629790039733
 83%|████████▎ | 83/100 [1:01:58<12:46, 45.06s/it]Time:  3.9032182439696044
 84%|████████▍ | 84/100 [1:02:43<12:02, 45.17s/it]Time:  3.9213579220231622
 85%|████████▌ | 85/100 [1:03:28<11:18, 45.23s/it]Time:  3.9346140429843217
 86%|████████▌ | 86/100 [1:04:14<10:33, 45.27s/it]Time:  3.8818073489237577
 87%|████████▋ | 87/100 [1:04:59<09:48, 45.28s/it]Time:  3.9227044419385493
 88%|████████▊ | 88/100 [1:05:44<09:02, 45.20s/it]Time:  3.920185449067503
 89%|████████▉ | 89/100 [1:06:29<08:16, 45.10s/it]Time:  3.9325275720329955
 90%|█████████ | 90/100 [1:07:14<07:31, 45.20s/it]Time:  3.9088531509041786
 91%|█████████ | 91/100 [1:08:00<06:48, 45.37s/it]Time:  3.933054608060047
 92%|█████████▏| 92/100 [1:08:45<06:02, 45.30s/it]Time:  3.9523624930297956
 93%|█████████▎| 93/100 [1:09:30<05:16, 45.16s/it]Time:  3.952815141994506
 94%|█████████▍| 94/100 [1:10:15<04:30, 45.12s/it]Time:  4.003938911948353
 95%|█████████▌| 95/100 [1:11:00<03:44, 44.98s/it]Time:  3.952352690976113
 96%|█████████▌| 96/100 [1:11:44<02:59, 44.86s/it]Time:  2.2582242030184716
 97%|█████████▋| 97/100 [1:12:26<02:12, 44.04s/it]Time:  3.9863465649541467
 98%|█████████▊| 98/100 [1:13:05<01:24, 42.44s/it]Time:  3.9723634970141575
 99%|█████████▉| 99/100 [1:13:49<00:42, 42.99s/it]Time:  4.061044852016494
100%|██████████| 100/100 [1:14:33<00:00, 43.28s/it]100%|██████████| 100/100 [1:14:33<00:00, 44.74s/it]
Post-training time: 4477.15 seconds
GPU memory allocated: 375.68 MB, peak: 999.49 MB
/n/fs/klips/anaconda3/envs/cos568/lib/python3.10/site-packages/numpy/_core/_methods.py:197: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/n/fs/klips/anaconda3/envs/cos568/lib/python3.10/site-packages/numpy/_core/_methods.py:208: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
Train results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy  test_time
Init.      0           NaN   2.417717          11.73          50.17   5.058594
Pre-Prune  0           NaN   2.417717          11.73          50.17   5.058594
Post-Prune 0           NaN   2.403514          11.46          49.92   3.246876
Final      100    0.072549   0.618341          88.40          99.08   4.061045
Prune results:
             module   param  sparsity     size             shape     flops    score mean  score variance     score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.999421     1728     (64, 3, 3, 3)   1769472  1.704298e+19             inf  2.945027e+22    1.704298e+19                 inf   2.945027e+22      True
1    layers.0.conv    bias  1.000000       64             (64,)     65536  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
2    layers.1.conv  weight  0.996582    36864    (64, 64, 3, 3)  37748736  7.988898e+17             inf  2.945027e+22    7.988898e+17                 inf   2.945027e+22      True
3    layers.1.conv    bias  1.000000       64             (64,)     65536  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
4    layers.3.conv  weight  0.993096    73728   (128, 64, 3, 3)  18874368  3.994448e+17             inf  2.945027e+22    3.994448e+17                 inf   2.945027e+22      True
5    layers.3.conv    bias  1.000000      128            (128,)     32768  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
6    layers.4.conv  weight  0.986925   147456  (128, 128, 3, 3)  37748736  1.997224e+17             inf  2.945027e+22    1.997224e+17                 inf   2.945027e+22      True
7    layers.4.conv    bias  1.000000      128            (128,)     32768  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
8    layers.6.conv  weight  0.974199   294912  (256, 128, 3, 3)  18874368  9.986122e+16             inf  2.945027e+22    9.986122e+16                 inf   2.945027e+22      True
9    layers.6.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
10   layers.7.conv  weight  0.947367   589824  (256, 256, 3, 3)  37748736  4.993060e+16             inf  2.945027e+22    4.993060e+16                 inf   2.945027e+22      True
11   layers.7.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
12   layers.8.conv  weight  0.947562   589824  (256, 256, 3, 3)  37748736  4.993061e+16             inf  2.945027e+22    4.993061e+16                 inf   2.945027e+22      True
13   layers.8.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
14  layers.10.conv  weight  0.892713  1179648  (512, 256, 3, 3)  18874368  2.496530e+16             inf  2.945027e+22    2.496530e+16                 inf   2.945027e+22      True
15  layers.10.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
16  layers.11.conv  weight  0.788353  2359296  (512, 512, 3, 3)  37748736  1.248265e+16    9.402646e+31  2.945026e+22    1.248265e+16        9.402646e+31   2.945026e+22      True
17  layers.11.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
18  layers.12.conv  weight  0.787956  2359296  (512, 512, 3, 3)  37748736  1.248265e+16    9.415211e+31  2.945027e+22    1.248265e+16        9.415211e+31   2.945027e+22      True
19  layers.12.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
20  layers.14.conv  weight  0.746524  2359296  (512, 512, 3, 3)   9437184  1.248265e+16             inf  2.945026e+22    1.248265e+16                 inf   2.945026e+22      True
21  layers.14.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
22  layers.15.conv  weight  0.746590  2359296  (512, 512, 3, 3)   9437184  1.248265e+16             inf  2.945026e+22    1.248265e+16                 inf   2.945026e+22      True
23  layers.15.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
24  layers.16.conv  weight  0.731970  2359296  (512, 512, 3, 3)   9437184  1.248265e+16             inf  2.945028e+22    1.248265e+16                 inf   2.945028e+22      True
25  layers.16.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
26              fc  weight  0.999609     5120         (10, 512)      5120  5.752007e+18             inf  2.945027e+22    5.752007e+18                 inf   2.945027e+22      True
27              fc    bias  1.000000       10             (10,)        10  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
Parameter Sparsity: 11693237/14719818 (0.7944)
FLOP Sparsity: 282939167/313478154 (0.9026)
Saving results.
