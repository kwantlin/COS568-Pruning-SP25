Namespace(dataset='cifar10', model='vgg16', model_class='lottery', dense_classifier=False, pretrained=False, optimizer='adam', train_batch_size=256, test_batch_size=256, pre_epochs=0, post_epochs=100, lr=0.001, lr_drops=[], lr_drop_rate=0.1, weight_decay=0.0, pruner='grasp', compression=0.1, quantization=False, prune_epochs=1, compression_schedule='exponential', mask_scope='global', prune_dataset_ratio=10, prune_batch_size=256, prune_bias=False, prune_batchnorm=False, prune_residual=False, prune_train_mode=False, reinitialize=False, shuffle=False, invert=False, pruner_list=[], prune_epoch_list=[], compression_list=[], level_list=[], experiment='singleshot', expid='cifar10-vgg16-lottery-grasp-preEpochs0-postEpochs100-compression0.1', result_dir='Results/data', gpu=0, workers=4, no_cuda=False, seed=1, verbose=False)
Loading cifar10 dataset.
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with grasp for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:35<00:00, 35.65s/it]100%|██████████| 1/1 [00:35<00:00, 35.65s/it]
Post-Training for 100 epochs.
  0%|          | 0/100 [00:00<?, ?it/s]Time:  3.833495542407036
  1%|          | 1/100 [00:54<1:30:02, 54.57s/it]Time:  3.8241566317155957
  2%|▏         | 2/100 [01:50<1:30:51, 55.63s/it]Time:  4.013851425610483
  3%|▎         | 3/100 [02:46<1:30:12, 55.80s/it]Time:  3.9537546690553427
  4%|▍         | 4/100 [03:42<1:29:19, 55.83s/it]Time:  3.847006774507463
  5%|▌         | 5/100 [04:38<1:28:02, 55.61s/it]Time:  3.4240090753883123
  6%|▌         | 6/100 [05:32<1:26:33, 55.25s/it]Time:  3.8095113541930914
  7%|▋         | 7/100 [06:27<1:25:25, 55.11s/it]Time:  3.9067944176495075
  8%|▊         | 8/100 [07:22<1:24:22, 55.03s/it]Time:  4.2352321688085794
  9%|▉         | 9/100 [08:17<1:23:23, 54.98s/it]Time:  4.24561910610646
 10%|█         | 10/100 [09:12<1:22:31, 55.01s/it]Time:  4.379437698982656
 11%|█         | 11/100 [10:07<1:21:49, 55.16s/it]Time:  4.237137775868177
 12%|█▏        | 12/100 [11:02<1:20:52, 55.14s/it]Time:  4.386855397373438
 13%|█▎        | 13/100 [11:57<1:19:50, 55.07s/it]Time:  4.298235723748803
 14%|█▍        | 14/100 [12:51<1:18:27, 54.74s/it]Time:  4.341698931530118
 15%|█▌        | 15/100 [13:46<1:17:44, 54.88s/it]Time:  4.2335287583991885
 16%|█▌        | 16/100 [14:41<1:16:44, 54.81s/it]Time:  4.249222320504487
 17%|█▋        | 17/100 [15:36<1:15:57, 54.91s/it]Time:  4.153021831065416
 18%|█▊        | 18/100 [16:31<1:15:07, 54.97s/it]Time:  4.238679325208068
 19%|█▉        | 19/100 [17:28<1:15:04, 55.61s/it]Time:  3.9847107427194715
 20%|██        | 20/100 [18:25<1:14:37, 55.97s/it]Time:  3.966391927562654
 21%|██        | 21/100 [19:20<1:13:14, 55.63s/it]Time:  3.9858426190912724
 22%|██▏       | 22/100 [20:15<1:11:57, 55.35s/it]Time:  4.217419818975031
 23%|██▎       | 23/100 [21:10<1:11:08, 55.43s/it]Time:  4.126681148074567
 24%|██▍       | 24/100 [22:06<1:10:14, 55.46s/it]Time:  4.190017944201827
 25%|██▌       | 25/100 [23:02<1:09:35, 55.67s/it]Time:  3.863296864554286
 26%|██▌       | 26/100 [23:58<1:08:46, 55.76s/it]Time:  3.6015153201296926
 27%|██▋       | 27/100 [24:53<1:07:35, 55.56s/it]Time:  4.083821860142052
 28%|██▊       | 28/100 [25:48<1:06:25, 55.35s/it]Time:  4.674807486124337
 29%|██▉       | 29/100 [26:42<1:04:56, 54.88s/it]Time:  4.577789646573365
 30%|███       | 30/100 [27:37<1:04:19, 55.13s/it]Time:  4.593233127146959
 31%|███       | 31/100 [28:33<1:03:32, 55.25s/it]Time:  4.5453639682382345
 32%|███▏      | 32/100 [29:28<1:02:30, 55.16s/it]Time:  4.435537289828062
 33%|███▎      | 33/100 [30:23<1:01:37, 55.18s/it]Time:  4.461653869599104
 34%|███▍      | 34/100 [31:18<1:00:42, 55.19s/it]Time:  4.392945217899978
 35%|███▌      | 35/100 [32:13<59:37, 55.04s/it]  Time:  4.541660316288471
 36%|███▌      | 36/100 [33:09<58:50, 55.16s/it]Time:  4.443182720802724
 37%|███▋      | 37/100 [34:03<57:47, 55.04s/it]Time:  4.5719873663038015
 38%|███▊      | 38/100 [34:58<56:45, 54.93s/it]Time:  4.536652477458119
 39%|███▉      | 39/100 [35:52<55:41, 54.78s/it]Time:  4.544138609431684
 40%|████      | 40/100 [36:47<54:39, 54.66s/it]Time:  4.637318681925535
 41%|████      | 41/100 [37:41<53:42, 54.62s/it]Time:  4.616726099513471
 42%|████▏     | 42/100 [38:36<52:43, 54.55s/it]Time:  4.602326217107475
 43%|████▎     | 43/100 [39:30<51:54, 54.63s/it]Time:  4.645636513829231
 44%|████▍     | 44/100 [40:25<51:00, 54.66s/it]Time:  4.888754677027464
 45%|████▌     | 45/100 [41:20<50:12, 54.77s/it]Time:  4.697293539531529
 46%|████▌     | 46/100 [42:16<49:27, 54.96s/it]Time:  4.710329046472907
 47%|████▋     | 47/100 [43:11<48:36, 55.02s/it]Time:  4.5979808904230595
 48%|████▊     | 48/100 [44:05<47:35, 54.92s/it]Time:  4.446068321354687
 49%|████▉     | 49/100 [45:00<46:33, 54.77s/it]Time:  4.659379651769996
 50%|█████     | 50/100 [45:55<45:41, 54.83s/it]Time:  4.688769654370844
 51%|█████     | 51/100 [46:50<44:45, 54.81s/it]Time:  4.665416033938527
 52%|█████▏    | 52/100 [47:44<43:47, 54.73s/it]Time:  4.736028970219195
 53%|█████▎    | 53/100 [48:39<42:50, 54.70s/it]Time:  4.691877937875688
 54%|█████▍    | 54/100 [49:33<41:51, 54.60s/it]Time:  4.682860185392201
 55%|█████▌    | 55/100 [50:27<40:51, 54.47s/it]Time:  4.71002849843353
 56%|█████▌    | 56/100 [51:22<39:58, 54.51s/it]Time:  4.750692987814546
 57%|█████▋    | 57/100 [52:16<39:03, 54.49s/it]Time:  4.7176943980157375
 58%|█████▊    | 58/100 [53:11<38:08, 54.49s/it]Time:  4.747284376993775
 59%|█████▉    | 59/100 [54:05<37:12, 54.44s/it]Time:  4.95947736594826
 60%|██████    | 60/100 [55:00<36:20, 54.51s/it]Time:  4.953244279138744
 61%|██████    | 61/100 [55:55<35:28, 54.58s/it]Time:  4.90311804972589
 62%|██████▏   | 62/100 [56:50<34:38, 54.70s/it]Time:  4.7215225994586945
 63%|██████▎   | 63/100 [57:44<33:40, 54.61s/it]Time:  4.759826106019318
 64%|██████▍   | 64/100 [58:38<32:44, 54.56s/it]Time:  4.6350666880607605
 65%|██████▌   | 65/100 [59:33<31:46, 54.47s/it]Time:  4.684674136340618
 66%|██████▌   | 66/100 [1:00:27<30:52, 54.50s/it]Time:  4.771977084688842
 67%|██████▋   | 67/100 [1:01:22<30:05, 54.71s/it]Time:  4.7589456690475345
 68%|██████▊   | 68/100 [1:02:17<29:11, 54.75s/it]Time:  4.823511145077646
 69%|██████▉   | 69/100 [1:03:12<28:16, 54.73s/it]Time:  4.896717399358749
 70%|███████   | 70/100 [1:04:06<27:19, 54.65s/it]Time:  4.937749542295933
 71%|███████   | 71/100 [1:05:01<26:23, 54.61s/it]Time:  4.901948688551784
 72%|███████▏  | 72/100 [1:05:55<25:25, 54.49s/it]Time:  4.9023374347016215
 73%|███████▎  | 73/100 [1:06:50<24:33, 54.57s/it]Time:  4.9091015020385385
 74%|███████▍  | 74/100 [1:07:45<23:41, 54.68s/it]Time:  4.927602790296078
 75%|███████▌  | 75/100 [1:08:40<22:48, 54.76s/it]Time:  4.9411007249727845
 76%|███████▌  | 76/100 [1:09:35<21:53, 54.74s/it]Time:  4.985651814378798
 77%|███████▋  | 77/100 [1:10:29<20:58, 54.71s/it]Time:  4.9674172922968864
 78%|███████▊  | 78/100 [1:11:23<20:00, 54.57s/it]Time:  4.9660987462848425
 79%|███████▉  | 79/100 [1:12:18<19:04, 54.49s/it]Time:  4.95652130804956
 80%|████████  | 80/100 [1:13:12<18:08, 54.45s/it]Time:  4.946327309124172
 81%|████████  | 81/100 [1:14:06<17:12, 54.33s/it]Time:  4.957160739228129
 82%|████████▏ | 82/100 [1:15:01<16:18, 54.35s/it]Time:  4.951470824889839
 83%|████████▎ | 83/100 [1:15:55<15:25, 54.42s/it]Time:  4.992330107837915
 84%|████████▍ | 84/100 [1:16:50<14:31, 54.50s/it]Time:  5.016552716493607
 85%|████████▌ | 85/100 [1:17:44<13:37, 54.51s/it]Time:  4.9847092889249325
 86%|████████▌ | 86/100 [1:18:39<12:43, 54.52s/it]Time:  4.977832439355552
 87%|████████▋ | 87/100 [1:19:33<11:48, 54.52s/it]Time:  4.939246225170791
 88%|████████▊ | 88/100 [1:20:28<10:54, 54.54s/it]Time:  4.944261722266674
 89%|████████▉ | 89/100 [1:21:22<09:59, 54.52s/it]Time:  4.993972980417311
 90%|█████████ | 90/100 [1:22:17<09:05, 54.50s/it]Time:  4.981568262912333
 91%|█████████ | 91/100 [1:23:11<08:10, 54.48s/it]Time:  4.94443634711206
 92%|█████████▏| 92/100 [1:24:06<07:16, 54.57s/it]Time:  4.997504999861121
 93%|█████████▎| 93/100 [1:25:01<06:22, 54.62s/it]Time:  4.96089337579906
 94%|█████████▍| 94/100 [1:25:55<05:27, 54.57s/it]Time:  4.978158609941602
 95%|█████████▌| 95/100 [1:26:50<04:33, 54.62s/it]Time:  4.981954012066126
 96%|█████████▌| 96/100 [1:27:44<03:38, 54.54s/it]Time:  4.949135817587376
 97%|█████████▋| 97/100 [1:28:39<02:43, 54.52s/it]Time:  4.990512695163488
 98%|█████████▊| 98/100 [1:29:33<01:48, 54.44s/it]Time:  4.093649035319686
 99%|█████████▉| 99/100 [1:30:24<00:53, 53.24s/it]Time:  2.5058129923418164
100%|██████████| 100/100 [1:31:03<00:00, 49.15s/it]100%|██████████| 100/100 [1:31:03<00:00, 54.64s/it]
Post-training time: 5467.59 seconds
GPU memory allocated: 374.76 MB, peak: 999.48 MB
Train results:
                 train_loss     test_loss  top1_accuracy  top5_accuracy   test_time
Init.      0           NaN  2.417717e+00          11.73          50.17  126.392691
Pre-Prune  0           NaN  2.417717e+00          11.73          50.17  126.392691
Post-Prune 0           NaN  3.394486e+10          10.00          49.35    3.955347
Final      100    1.346565  1.427990e+00          49.98          92.13    2.505813
Prune results:
             module   param  sparsity     size             shape     flops    score mean  score variance  score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.565394     1728     (64, 3, 3, 3)   1769472  4.450377e-05    8.434831e-08   0.076903    1.499241e-04        6.385167e-08       0.259069      True
1    layers.0.conv    bias  1.000000       64             (64,)     65536  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
2    layers.1.conv  weight  0.567003    36864    (64, 64, 3, 3)  37748736  2.075447e-06    4.122781e-09   0.076509    2.485491e-05        3.509322e-09       0.916251      True
3    layers.1.conv    bias  1.000000       64             (64,)     65536  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
4    layers.3.conv  weight  0.624647    73728   (128, 64, 3, 3)  18874368  1.036266e-06    1.686676e-09   0.076402    1.571875e-05        1.440671e-09       1.158912      True
5    layers.3.conv    bias  1.000000      128            (128,)     32768  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
6    layers.4.conv  weight  0.693943   147456  (128, 128, 3, 3)  37748736  5.169903e-07    5.230480e-10   0.076233    7.786416e-06        4.626871e-10       1.148154      True
7    layers.4.conv    bias  1.000000      128            (128,)     32768  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
8    layers.6.conv  weight  0.721890   294912  (256, 128, 3, 3)  18874368  2.581017e-07    2.053401e-10   0.076117    4.942225e-06        1.809811e-10       1.457521      True
9    layers.6.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
10   layers.7.conv  weight  0.764638   589824  (256, 256, 3, 3)  37748736  1.255479e-07    5.263316e-11   0.074051    2.530798e-06        4.624397e-11       1.492725      True
11   layers.7.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
12   layers.8.conv  weight  0.745217   589824  (256, 256, 3, 3)  37748736  1.240259e-07    3.422637e-11   0.073153    2.217017e-06        2.932659e-11       1.307650      True
13   layers.8.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
14  layers.10.conv  weight  0.757642  1179648  (512, 256, 3, 3)  18874368  6.157173e-08    1.396249e-11   0.072633    1.547427e-06        1.157176e-11       1.825420      True
15  layers.10.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
16  layers.11.conv  weight  0.803447  2359296  (512, 512, 3, 3)  37748736  2.846881e-08    4.243502e-12   0.067166    8.836321e-07        3.463504e-12       2.084750      True
17  layers.11.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
18  layers.12.conv  weight  0.785954  2359296  (512, 512, 3, 3)  37748736  2.840791e-08    3.657214e-12   0.067023    8.970007e-07        2.853411e-12       2.116290      True
19  layers.12.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
20  layers.14.conv  weight  0.799756  2359296  (512, 512, 3, 3)   9437184  2.694332e-08    3.812698e-12   0.063567    8.725642e-07        3.052054e-12       2.058637      True
21  layers.14.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
22  layers.15.conv  weight  0.827039  2359296  (512, 512, 3, 3)   9437184  2.711093e-08    4.234986e-12   0.063963    8.358488e-07        3.537078e-12       1.972015      True
23  layers.15.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
24  layers.16.conv  weight  0.818301  2359296  (512, 512, 3, 3)   9437184  2.642921e-08    5.443436e-12   0.062354    9.434238e-07        4.554086e-12       2.225816      True
25  layers.16.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
26              fc  weight  0.582031     5120         (10, 512)      5120  1.443846e-05    5.200129e-09   0.073925    3.773056e-05        3.985004e-09       0.193180      True
27              fc    bias  1.000000       10             (10,)        10  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
Parameter Sparsity: 11693237/14719818 (0.7944)
FLOP Sparsity: 228661888/313478154 (0.7294)
Saving results.
