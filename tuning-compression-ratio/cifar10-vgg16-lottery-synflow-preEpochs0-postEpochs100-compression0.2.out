Namespace(dataset='cifar10', model='vgg16', model_class='lottery', dense_classifier=False, pretrained=False, optimizer='adam', train_batch_size=256, test_batch_size=256, pre_epochs=0, post_epochs=100, lr=0.001, lr_drops=[], lr_drop_rate=0.1, weight_decay=0.0, pruner='synflow', compression=0.2, quantization=False, prune_epochs=1, compression_schedule='exponential', mask_scope='global', prune_dataset_ratio=10, prune_batch_size=256, prune_bias=False, prune_batchnorm=False, prune_residual=False, prune_train_mode=False, reinitialize=False, shuffle=False, invert=False, pruner_list=[], prune_epoch_list=[], compression_list=[], level_list=[], experiment='singleshot', expid='cifar10-vgg16-lottery-synflow-preEpochs0-postEpochs100-compression0.2', result_dir='Results/data', gpu=0, workers=4, no_cuda=False, seed=1, verbose=False)
Loading cifar10 dataset.
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with synflow for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:01<00:00,  1.11s/it]100%|██████████| 1/1 [00:01<00:00,  1.11s/it]
Post-Training for 100 epochs.
  0%|          | 0/100 [00:00<?, ?it/s]Time:  3.051738686976023
  1%|          | 1/100 [00:44<1:13:32, 44.57s/it]Time:  3.024409355944954
  2%|▏         | 2/100 [01:30<1:14:26, 45.58s/it]Time:  3.5839552630204707
  3%|▎         | 3/100 [02:17<1:14:20, 45.99s/it]Time:  3.614182438934222
  4%|▍         | 4/100 [03:03<1:13:37, 46.01s/it]Time:  3.5559595429804176
  5%|▌         | 5/100 [03:50<1:13:16, 46.28s/it]Time:  3.5227945370133966
  6%|▌         | 6/100 [04:36<1:12:29, 46.27s/it]Time:  3.452236869954504
  7%|▋         | 7/100 [05:22<1:11:46, 46.31s/it]Time:  3.5824815910309553
  8%|▊         | 8/100 [06:09<1:11:05, 46.37s/it]Time:  3.48496956191957
  9%|▉         | 9/100 [06:55<1:10:19, 46.36s/it]Time:  3.498977262992412
 10%|█         | 10/100 [07:41<1:09:32, 46.36s/it]Time:  3.5058541930047795
 11%|█         | 11/100 [08:28<1:08:47, 46.37s/it]Time:  4.4192724080057815
 12%|█▏        | 12/100 [09:15<1:08:31, 46.72s/it]Time:  2.824339853017591
 13%|█▎        | 13/100 [10:01<1:07:12, 46.35s/it]Time:  2.758710786001757
 14%|█▍        | 14/100 [10:48<1:06:37, 46.48s/it]Time:  2.666766903945245
 15%|█▌        | 15/100 [11:33<1:05:33, 46.27s/it]Time:  2.305088907945901
 16%|█▌        | 16/100 [12:19<1:04:34, 46.13s/it]Time:  2.8485350409755483
 17%|█▋        | 17/100 [13:05<1:03:39, 46.02s/it]Time:  3.5215896860463545
 18%|█▊        | 18/100 [13:50<1:02:33, 45.78s/it]Time:  2.956878117052838
 19%|█▉        | 19/100 [14:35<1:01:27, 45.53s/it]Time:  2.835301680956036
 20%|██        | 20/100 [15:20<1:00:27, 45.35s/it]Time:  2.8423408619128168
 21%|██        | 21/100 [16:05<59:36, 45.28s/it]  Time:  3.1667752319481224
 22%|██▏       | 22/100 [16:51<59:07, 45.48s/it]Time:  3.765083012985997
 23%|██▎       | 23/100 [17:37<58:30, 45.59s/it]Time:  3.716292836004868
 24%|██▍       | 24/100 [18:23<57:52, 45.70s/it]Time:  3.533065077965148
 25%|██▌       | 25/100 [19:09<57:12, 45.77s/it]Time:  3.6787245370214805
 26%|██▌       | 26/100 [19:54<56:21, 45.69s/it]Time:  3.693188344943337
 27%|██▋       | 27/100 [20:40<55:27, 45.58s/it]Time:  4.019042502972297
 28%|██▊       | 28/100 [21:26<54:45, 45.63s/it]Time:  3.942139865946956
 29%|██▉       | 29/100 [22:11<54:01, 45.66s/it]Time:  3.981678480980918
 30%|███       | 30/100 [22:57<53:10, 45.58s/it]Time:  3.9064760979963467
 31%|███       | 31/100 [23:42<52:20, 45.52s/it]Time:  3.8682818029774353
 32%|███▏      | 32/100 [24:27<51:28, 45.42s/it]Time:  3.8351631179684773
 33%|███▎      | 33/100 [25:13<50:42, 45.40s/it]Time:  3.8477677869377658
 34%|███▍      | 34/100 [25:58<49:49, 45.30s/it]Time:  3.913099510013126
 35%|███▌      | 35/100 [26:43<49:04, 45.30s/it]Time:  3.696583008975722
 36%|███▌      | 36/100 [27:28<48:18, 45.29s/it]Time:  3.757145034032874
 37%|███▋      | 37/100 [28:13<47:28, 45.22s/it]Time:  3.7406798269366845
 38%|███▊      | 38/100 [28:58<46:37, 45.12s/it]Time:  3.8535077999113128
 39%|███▉      | 39/100 [29:44<45:57, 45.21s/it]Time:  3.8814561190083623
 40%|████      | 40/100 [30:29<45:11, 45.19s/it]Time:  3.862973073963076
 41%|████      | 41/100 [31:15<44:45, 45.51s/it]Time:  3.9864176609553397
 42%|████▏     | 42/100 [32:03<44:35, 46.14s/it]Time:  4.173436484998092
 43%|████▎     | 43/100 [32:48<43:39, 45.95s/it]Time:  4.321735166013241
 44%|████▍     | 44/100 [33:34<42:45, 45.81s/it]Time:  4.178238751948811
 45%|████▌     | 45/100 [34:19<41:49, 45.63s/it]Time:  4.179956862935796
 46%|████▌     | 46/100 [35:04<40:54, 45.45s/it]Time:  4.174320055986755
 47%|████▋     | 47/100 [35:49<40:03, 45.35s/it]Time:  4.135430482914671
 48%|████▊     | 48/100 [36:32<38:45, 44.71s/it]Time:  4.115470589953475
 49%|████▉     | 49/100 [37:17<38:10, 44.91s/it]Time:  3.9905634389724582
 50%|█████     | 50/100 [38:03<37:31, 45.03s/it]Time:  3.9980616710381582
 51%|█████     | 51/100 [38:48<36:48, 45.06s/it]Time:  3.948007896076888
 52%|█████▏    | 52/100 [39:33<36:06, 45.13s/it]Time:  3.933925620978698
 53%|█████▎    | 53/100 [40:19<35:25, 45.22s/it]Time:  3.925743979983963
 54%|█████▍    | 54/100 [41:04<34:44, 45.31s/it]Time:  3.9035348569741473
 55%|█████▌    | 55/100 [41:50<34:03, 45.42s/it]Time:  3.8312232170719653
 56%|█████▌    | 56/100 [42:38<33:55, 46.27s/it]Time:  3.7547085139667615
 57%|█████▋    | 57/100 [43:23<32:51, 45.84s/it]Time:  3.866871112026274
 58%|█████▊    | 58/100 [44:08<31:57, 45.65s/it]Time:  3.9205971799092367
 59%|█████▉    | 59/100 [44:53<31:04, 45.47s/it]Time:  3.82848167209886
 60%|██████    | 60/100 [45:38<30:09, 45.24s/it]Time:  4.197835419909097
 61%|██████    | 61/100 [46:23<29:22, 45.20s/it]Time:  4.209207896958105
 62%|██████▏   | 62/100 [47:08<28:35, 45.13s/it]Time:  4.151190256001428
 63%|██████▎   | 63/100 [47:53<27:45, 45.01s/it]Time:  4.197023239918053
 64%|██████▍   | 64/100 [48:37<26:55, 44.87s/it]Time:  4.125189992017113
 65%|██████▌   | 65/100 [49:22<26:10, 44.88s/it]Time:  4.189608479035087
 66%|██████▌   | 66/100 [50:07<25:29, 44.98s/it]Time:  4.145633708918467
 67%|██████▋   | 67/100 [50:52<24:39, 44.83s/it]Time:  4.190886395983398
 68%|██████▊   | 68/100 [51:36<23:52, 44.77s/it]Time:  4.135387279908173
 69%|██████▉   | 69/100 [52:21<23:05, 44.71s/it]Time:  4.210755300009623
 70%|███████   | 70/100 [53:06<22:19, 44.66s/it]Time:  4.136175019084476
 71%|███████   | 71/100 [53:50<21:32, 44.56s/it]Time:  4.158576391986571
 72%|███████▏  | 72/100 [54:34<20:47, 44.55s/it]Time:  4.196208962006494
 73%|███████▎  | 73/100 [55:20<20:09, 44.78s/it]Time:  4.18841501895804
 74%|███████▍  | 74/100 [56:05<19:26, 44.86s/it]Time:  4.2679351849947125
 75%|███████▌  | 75/100 [56:50<18:46, 45.07s/it]Time:  4.204540665028617
 76%|███████▌  | 76/100 [57:35<18:00, 45.01s/it]Time:  4.125338732963428
 77%|███████▋  | 77/100 [58:20<17:13, 44.94s/it]Time:  4.4713421440683305
 78%|███████▊  | 78/100 [59:05<16:30, 45.01s/it]Time:  4.178369460976683
 79%|███████▉  | 79/100 [59:50<15:43, 44.95s/it]Time:  4.191422454081476
 80%|████████  | 80/100 [1:00:35<14:57, 44.89s/it]Time:  4.1385452810209244
 81%|████████  | 81/100 [1:01:19<14:11, 44.82s/it]Time:  4.155316642019898
 82%|████████▏ | 82/100 [1:02:04<13:26, 44.79s/it]Time:  4.192657514009625
 83%|████████▎ | 83/100 [1:02:49<12:43, 44.91s/it]Time:  4.204873810056597
 84%|████████▍ | 84/100 [1:03:35<12:00, 45.05s/it]Time:  4.24670093995519
 85%|████████▌ | 85/100 [1:04:20<11:16, 45.12s/it]Time:  4.096019663033076
 86%|████████▌ | 86/100 [1:05:05<10:32, 45.18s/it]Time:  4.133635627105832
 87%|████████▋ | 87/100 [1:05:51<09:47, 45.18s/it]Time:  4.164229426998645
 88%|████████▊ | 88/100 [1:06:36<09:01, 45.14s/it]Time:  4.165729066939093
 89%|████████▉ | 89/100 [1:07:21<08:17, 45.22s/it]Time:  4.128792549949139
 90%|█████████ | 90/100 [1:08:06<07:32, 45.26s/it]Time:  4.180048874113709
 91%|█████████ | 91/100 [1:08:51<06:46, 45.17s/it]Time:  4.14102769899182
 92%|█████████▏| 92/100 [1:09:36<06:00, 45.10s/it]Time:  4.182966358028352
 93%|█████████▎| 93/100 [1:10:21<05:15, 45.08s/it]Time:  4.248969592968933
 94%|█████████▍| 94/100 [1:11:06<04:29, 44.94s/it]Time:  4.137437862926163
 95%|█████████▌| 95/100 [1:11:50<03:44, 44.84s/it]Time:  2.5803644580300897
 96%|█████████▌| 96/100 [1:12:33<02:56, 44.13s/it]Time:  3.706632468965836
 97%|█████████▋| 97/100 [1:13:19<02:14, 44.71s/it]Time:  3.666223476990126
 98%|█████████▊| 98/100 [1:14:05<01:30, 45.01s/it]Time:  2.8486824670108035
 99%|█████████▉| 99/100 [1:14:46<00:44, 44.03s/it]Time:  0.9889215700095519
100%|██████████| 100/100 [1:15:10<00:00, 37.98s/it]100%|██████████| 100/100 [1:15:10<00:00, 45.11s/it]
Post-training time: 4514.06 seconds
GPU memory allocated: 375.68 MB, peak: 999.49 MB
/n/fs/klips/anaconda3/envs/cos568/lib/python3.10/site-packages/numpy/_core/_methods.py:197: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/n/fs/klips/anaconda3/envs/cos568/lib/python3.10/site-packages/numpy/_core/_methods.py:208: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
Train results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy  test_time
Init.      0           NaN   2.417717          11.73          50.17   5.309250
Pre-Prune  0           NaN   2.417717          11.73          50.17   5.309250
Post-Prune 0           NaN   2.396131          10.05          51.06   3.224096
Final      100    0.077677   0.599166          88.16          98.86   0.988922
Prune results:
             module   param  sparsity     size             shape     flops    score mean  score variance     score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.998843     1728     (64, 3, 3, 3)   1769472  1.704298e+19             inf  2.945027e+22    1.704298e+19                 inf   2.945027e+22      True
1    layers.0.conv    bias  1.000000       64             (64,)     65536  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
2    layers.1.conv  weight  0.993490    36864    (64, 64, 3, 3)  37748736  7.988898e+17             inf  2.945027e+22    7.988898e+17                 inf   2.945027e+22      True
3    layers.1.conv    bias  1.000000       64             (64,)     65536  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
4    layers.3.conv  weight  0.987115    73728   (128, 64, 3, 3)  18874368  3.994448e+17             inf  2.945027e+22    3.994448e+17                 inf   2.945027e+22      True
5    layers.3.conv    bias  1.000000      128            (128,)     32768  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
6    layers.4.conv  weight  0.974752   147456  (128, 128, 3, 3)  37748736  1.997224e+17             inf  2.945027e+22    1.997224e+17                 inf   2.945027e+22      True
7    layers.4.conv    bias  1.000000      128            (128,)     32768  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
8    layers.6.conv  weight  0.950578   294912  (256, 128, 3, 3)  18874368  9.986122e+16             inf  2.945027e+22    9.986122e+16                 inf   2.945027e+22      True
9    layers.6.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
10   layers.7.conv  weight  0.900650   589824  (256, 256, 3, 3)  37748736  4.993060e+16             inf  2.945027e+22    4.993060e+16                 inf   2.945027e+22      True
11   layers.7.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
12   layers.8.conv  weight  0.900650   589824  (256, 256, 3, 3)  37748736  4.993061e+16             inf  2.945027e+22    4.993061e+16                 inf   2.945027e+22      True
13   layers.8.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
14  layers.10.conv  weight  0.799322  1179648  (512, 256, 3, 3)  18874368  2.496530e+16             inf  2.945027e+22    2.496530e+16                 inf   2.945027e+22      True
15  layers.10.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
16  layers.11.conv  weight  0.611696  2359296  (512, 512, 3, 3)  37748736  1.248265e+16    9.402646e+31  2.945026e+22    1.248265e+16        9.402646e+31   2.945026e+22      True
17  layers.11.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
18  layers.12.conv  weight  0.611219  2359296  (512, 512, 3, 3)  37748736  1.248265e+16    9.415211e+31  2.945027e+22    1.248265e+16        9.415211e+31   2.945027e+22      True
19  layers.12.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
20  layers.14.conv  weight  0.549993  2359296  (512, 512, 3, 3)   9437184  1.248265e+16             inf  2.945026e+22    1.248265e+16                 inf   2.945026e+22      True
21  layers.14.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
22  layers.15.conv  weight  0.550156  2359296  (512, 512, 3, 3)   9437184  1.248265e+16             inf  2.945026e+22    1.248265e+16                 inf   2.945026e+22      True
23  layers.15.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
24  layers.16.conv  weight  0.533391  2359296  (512, 512, 3, 3)   9437184  1.248265e+16             inf  2.945028e+22    1.248265e+16                 inf   2.945028e+22      True
25  layers.16.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
26              fc  weight  0.999219     5120         (10, 512)      5120  5.752007e+18             inf  2.945027e+22    5.752007e+18                 inf   2.945027e+22      True
27              fc    bias  1.000000       10             (10,)        10  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
Parameter Sparsity: 9289139/14719818 (0.6311)
FLOP Sparsity: 257583398/313478154 (0.8217)
Saving results.
