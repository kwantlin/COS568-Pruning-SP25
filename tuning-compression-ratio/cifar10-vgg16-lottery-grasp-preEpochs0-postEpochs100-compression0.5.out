Namespace(dataset='cifar10', model='vgg16', model_class='lottery', dense_classifier=False, pretrained=False, optimizer='adam', train_batch_size=256, test_batch_size=256, pre_epochs=0, post_epochs=100, lr=0.001, lr_drops=[], lr_drop_rate=0.1, weight_decay=0.0, pruner='grasp', compression=0.5, quantization=False, prune_epochs=1, compression_schedule='exponential', mask_scope='global', prune_dataset_ratio=10, prune_batch_size=256, prune_bias=False, prune_batchnorm=False, prune_residual=False, prune_train_mode=False, reinitialize=False, shuffle=False, invert=False, pruner_list=[], prune_epoch_list=[], compression_list=[], level_list=[], experiment='singleshot', expid='cifar10-vgg16-lottery-grasp-preEpochs0-postEpochs100-compression0.5', result_dir='Results/data', gpu=0, workers=4, no_cuda=False, seed=1, verbose=False)
Loading cifar10 dataset.
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with grasp for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:35<00:00, 35.65s/it]100%|██████████| 1/1 [00:35<00:00, 35.65s/it]
Post-Training for 100 epochs.
  0%|          | 0/100 [00:00<?, ?it/s]Time:  3.7488789781928062
  1%|          | 1/100 [00:55<1:30:59, 55.14s/it]Time:  3.6649004723876715
  2%|▏         | 2/100 [01:50<1:30:42, 55.54s/it]Time:  4.108449222519994
  3%|▎         | 3/100 [02:47<1:30:12, 55.80s/it]Time:  3.921445544809103
  4%|▍         | 4/100 [03:42<1:29:12, 55.76s/it]Time:  3.9845362920314074
  5%|▌         | 5/100 [04:38<1:28:02, 55.61s/it]Time:  3.745883106254041
  6%|▌         | 6/100 [05:32<1:26:42, 55.34s/it]Time:  3.9044064516201615
  7%|▋         | 7/100 [06:27<1:25:37, 55.24s/it]Time:  4.40385602042079
  8%|▊         | 8/100 [07:23<1:24:37, 55.19s/it]Time:  4.290525848977268
  9%|▉         | 9/100 [08:18<1:23:36, 55.13s/it]Time:  4.180175694637001
 10%|█         | 10/100 [09:12<1:22:23, 54.92s/it]Time:  4.270392889156938
 11%|█         | 11/100 [10:07<1:21:27, 54.91s/it]Time:  4.160630846396089
 12%|█▏        | 12/100 [11:01<1:20:20, 54.78s/it]Time:  4.34456220921129
 13%|█▎        | 13/100 [11:57<1:19:44, 55.00s/it]Time:  4.3090793546289206
 14%|█▍        | 14/100 [12:51<1:18:28, 54.75s/it]Time:  4.316308771260083
 15%|█▌        | 15/100 [13:46<1:17:33, 54.75s/it]Time:  4.2469518994912505
 16%|█▌        | 16/100 [14:41<1:16:54, 54.93s/it]Time:  4.3862916100770235
 17%|█▋        | 17/100 [15:36<1:16:09, 55.05s/it]Time:  4.113347965292633
 18%|█▊        | 18/100 [16:31<1:15:06, 54.95s/it]Time:  4.123141362331808
 19%|█▉        | 19/100 [17:28<1:14:55, 55.50s/it]Time:  4.129294945858419
 20%|██        | 20/100 [18:25<1:14:31, 55.90s/it]Time:  4.097584852017462
 21%|██        | 21/100 [19:20<1:13:09, 55.57s/it]Time:  4.038555784150958
 22%|██▏       | 22/100 [20:14<1:11:56, 55.34s/it]Time:  4.351798556745052
 23%|██▎       | 23/100 [21:10<1:11:10, 55.46s/it]Time:  4.0632448717951775
 24%|██▍       | 24/100 [22:05<1:10:06, 55.34s/it]Time:  4.405548887327313
 25%|██▌       | 25/100 [23:01<1:09:29, 55.59s/it]Time:  4.017588848248124
 26%|██▌       | 26/100 [23:57<1:08:24, 55.47s/it]Time:  3.923734510317445
 27%|██▋       | 27/100 [24:51<1:07:07, 55.18s/it]Time:  4.272983346134424
 28%|██▊       | 28/100 [25:48<1:06:48, 55.67s/it]Time:  4.642669264227152
 29%|██▉       | 29/100 [26:42<1:05:11, 55.09s/it]Time:  4.5118463998660445
 30%|███       | 30/100 [27:37<1:04:23, 55.20s/it]Time:  4.537416173145175
 31%|███       | 31/100 [28:32<1:03:32, 55.25s/it]Time:  4.601938992738724
 32%|███▏      | 32/100 [29:27<1:02:21, 55.02s/it]Time:  4.665578905493021
 33%|███▎      | 33/100 [30:22<1:01:25, 55.00s/it]Time:  4.706006668508053
 34%|███▍      | 34/100 [31:16<1:00:19, 54.84s/it]Time:  4.778153095394373
 35%|███▌      | 35/100 [32:11<59:18, 54.74s/it]  Time:  4.795014556497335
 36%|███▌      | 36/100 [33:05<58:20, 54.70s/it]Time:  4.926137433387339
 37%|███▋      | 37/100 [34:00<57:21, 54.62s/it]Time:  4.899602363817394
 38%|███▊      | 38/100 [34:54<56:23, 54.57s/it]Time:  4.912296921014786
 39%|███▉      | 39/100 [35:49<55:22, 54.47s/it]Time:  4.900484766811132
 40%|████      | 40/100 [36:43<54:26, 54.44s/it]Time:  4.880760897882283
 41%|████      | 41/100 [37:37<53:28, 54.39s/it]Time:  4.91143296007067
 42%|████▏     | 42/100 [38:32<52:35, 54.40s/it]Time:  4.918431079946458
 43%|████▎     | 43/100 [39:26<51:41, 54.40s/it]Time:  4.945622836239636
 44%|████▍     | 44/100 [40:21<50:48, 54.43s/it]Time:  4.929570583626628
 45%|████▌     | 45/100 [41:15<49:56, 54.47s/it]Time:  4.986450326628983
 46%|████▌     | 46/100 [42:09<48:54, 54.35s/it]Time:  4.955146566964686
 47%|████▋     | 47/100 [43:03<47:52, 54.20s/it]Time:  5.1045998595654964
 48%|████▊     | 48/100 [43:57<46:58, 54.21s/it]Time:  4.722401120699942
 49%|████▉     | 49/100 [44:52<46:07, 54.26s/it]Time:  4.677602428942919
 50%|█████     | 50/100 [45:46<45:12, 54.25s/it]Time:  4.72995594330132
 51%|█████     | 51/100 [46:40<44:21, 54.32s/it]Time:  4.645764918066561
 52%|█████▏    | 52/100 [47:35<43:25, 54.27s/it]Time:  4.666439232416451
 53%|█████▎    | 53/100 [48:29<42:36, 54.40s/it]Time:  4.680312811397016
 54%|█████▍    | 54/100 [49:24<41:42, 54.41s/it]Time:  4.766936969012022
 55%|█████▌    | 55/100 [50:19<40:55, 54.57s/it]Time:  4.739466251805425
 56%|█████▌    | 56/100 [51:13<40:02, 54.61s/it]Time:  4.797970455139875
 57%|█████▋    | 57/100 [52:08<39:11, 54.68s/it]Time:  4.712421401403844
 58%|█████▊    | 58/100 [53:03<38:21, 54.79s/it]Time:  4.672993999905884
 59%|█████▉    | 59/100 [53:58<37:23, 54.72s/it]Time:  4.722421092912555
 60%|██████    | 60/100 [54:52<36:27, 54.68s/it]Time:  4.788384013809264
 61%|██████    | 61/100 [55:47<35:30, 54.64s/it]Time:  4.87173802498728
 62%|██████▏   | 62/100 [56:41<34:28, 54.42s/it]Time:  4.842350256629288
 63%|██████▎   | 63/100 [57:35<33:35, 54.49s/it]Time:  4.751779534853995
 64%|██████▍   | 64/100 [58:30<32:41, 54.49s/it]Time:  4.822402782738209
 65%|██████▌   | 65/100 [59:24<31:43, 54.39s/it]Time:  4.8321675742045045
 66%|██████▌   | 66/100 [1:00:18<30:46, 54.30s/it]Time:  4.8166461596265435
 67%|██████▋   | 67/100 [1:01:13<29:54, 54.37s/it]Time:  4.743248305283487
 68%|██████▊   | 68/100 [1:02:07<28:58, 54.34s/it]Time:  4.780042947269976
 69%|██████▉   | 69/100 [1:03:01<28:03, 54.31s/it]Time:  4.747307297773659
 70%|███████   | 70/100 [1:03:56<27:11, 54.40s/it]Time:  4.794259830377996
 71%|███████   | 71/100 [1:04:51<26:24, 54.63s/it]Time:  4.8762880228459835
 72%|███████▏  | 72/100 [1:05:46<25:30, 54.67s/it]Time:  4.729915007948875
 73%|███████▎  | 73/100 [1:06:41<24:38, 54.76s/it]Time:  4.6558652222156525
 74%|███████▍  | 74/100 [1:07:35<23:40, 54.65s/it]Time:  4.715284317731857
 75%|███████▌  | 75/100 [1:08:30<22:45, 54.62s/it]Time:  4.744216714985669
 76%|███████▌  | 76/100 [1:09:24<21:50, 54.62s/it]Time:  4.685416725464165
 77%|███████▋  | 77/100 [1:10:19<20:56, 54.62s/it]Time:  4.787399430759251
 78%|███████▊  | 78/100 [1:11:13<20:00, 54.55s/it]Time:  4.734978813678026
 79%|███████▉  | 79/100 [1:12:08<19:05, 54.53s/it]Time:  4.755905715748668
 80%|████████  | 80/100 [1:13:02<18:10, 54.51s/it]Time:  5.128916101530194
 81%|████████  | 81/100 [1:13:57<17:18, 54.68s/it]Time:  4.9628893649205565
 82%|████████▏ | 82/100 [1:14:52<16:23, 54.63s/it]Time:  4.974385000765324
 83%|████████▎ | 83/100 [1:15:46<15:27, 54.54s/it]Time:  4.935436498373747
 84%|████████▍ | 84/100 [1:16:41<14:32, 54.51s/it]Time:  4.950211620889604
 85%|████████▌ | 85/100 [1:17:35<13:37, 54.52s/it]Time:  4.989178183488548
 86%|████████▌ | 86/100 [1:18:30<12:43, 54.50s/it]Time:  4.941558426246047
 87%|████████▋ | 87/100 [1:19:24<11:48, 54.49s/it]Time:  4.98046837002039
 88%|████████▊ | 88/100 [1:20:18<10:52, 54.42s/it]Time:  4.94703796505928
 89%|████████▉ | 89/100 [1:21:13<09:58, 54.38s/it]Time:  4.927230610512197
 90%|█████████ | 90/100 [1:22:07<09:03, 54.33s/it]Time:  4.980565975420177
 91%|█████████ | 91/100 [1:23:01<08:09, 54.38s/it]Time:  4.948857948184013
 92%|█████████▏| 92/100 [1:23:56<07:15, 54.39s/it]Time:  4.956065953709185
 93%|█████████▎| 93/100 [1:24:50<06:20, 54.37s/it]Time:  4.952793762087822
 94%|█████████▍| 94/100 [1:25:45<05:26, 54.44s/it]Time:  4.968714552000165
 95%|█████████▌| 95/100 [1:26:39<04:32, 54.44s/it]Time:  4.92719969060272
 96%|█████████▌| 96/100 [1:27:35<03:39, 54.83s/it]Time:  4.933577498421073
 97%|█████████▋| 97/100 [1:28:29<02:44, 54.67s/it]Time:  4.9613421289250255
 98%|█████████▊| 98/100 [1:29:24<01:49, 54.58s/it]Time:  4.117039152421057
 99%|█████████▉| 99/100 [1:30:16<00:53, 53.89s/it]Time:  3.308047229424119
100%|██████████| 100/100 [1:30:58<00:00, 50.45s/it]100%|██████████| 100/100 [1:30:58<00:00, 54.59s/it]
Post-training time: 5462.77 seconds
GPU memory allocated: 374.76 MB, peak: 999.48 MB
Train results:
                 train_loss     test_loss  top1_accuracy  top5_accuracy   test_time
Init.      0           NaN  2.417717e+00          11.73          50.17  126.396767
Pre-Prune  0           NaN  2.417717e+00          11.73          50.17  126.396767
Post-Prune 0           NaN  5.236259e+10          10.00          49.71    4.008521
Final      100    2.039581  1.982471e+00          28.59          78.40    3.308047
Prune results:
             module   param  sparsity     size             shape     flops    score mean  score variance  score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.555556     1728     (64, 3, 3, 3)   1769472  4.450376e-05    8.434828e-08   0.076902    1.499241e-04        6.385164e-08       0.259069      True
1    layers.0.conv    bias  1.000000       64             (64,)     65536  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
2    layers.1.conv  weight  0.484402    36864    (64, 64, 3, 3)  37748736  2.075447e-06    4.122780e-09   0.076509    2.485490e-05        3.509321e-09       0.916251      True
3    layers.1.conv    bias  1.000000       64             (64,)     65536  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
4    layers.3.conv  weight  0.452203    73728   (128, 64, 3, 3)  18874368  1.036265e-06    1.686676e-09   0.076402    1.571875e-05        1.440670e-09       1.158912      True
5    layers.3.conv    bias  1.000000      128            (128,)     32768  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
6    layers.4.conv  weight  0.406467   147456  (128, 128, 3, 3)  37748736  5.169903e-07    5.230480e-10   0.076233    7.786415e-06        4.626870e-10       1.148154      True
7    layers.4.conv    bias  1.000000      128            (128,)     32768  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
8    layers.6.conv  weight  0.382178   294912  (256, 128, 3, 3)  18874368  2.581017e-07    2.053402e-10   0.076117    4.942224e-06        1.809812e-10       1.457521      True
9    layers.6.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
10   layers.7.conv  weight  0.350525   589824  (256, 256, 3, 3)  37748736  1.255479e-07    5.263316e-11   0.074051    2.530798e-06        4.624397e-11       1.492726      True
11   layers.7.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
12   layers.8.conv  weight  0.375439   589824  (256, 256, 3, 3)  37748736  1.240259e-07    3.422637e-11   0.073153    2.217017e-06        2.932658e-11       1.307650      True
13   layers.8.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
14  layers.10.conv  weight  0.368191  1179648  (512, 256, 3, 3)  18874368  6.157175e-08    1.396250e-11   0.072633    1.547428e-06        1.157176e-11       1.825420      True
15  layers.10.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
16  layers.11.conv  weight  0.332319  2359296  (512, 512, 3, 3)  37748736  2.846881e-08    4.243502e-12   0.067166    8.836322e-07        3.463504e-12       2.084750      True
17  layers.11.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
18  layers.12.conv  weight  0.348520  2359296  (512, 512, 3, 3)  37748736  2.840790e-08    3.657214e-12   0.067023    8.970007e-07        2.853411e-12       2.116290      True
19  layers.12.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
20  layers.14.conv  weight  0.310052  2359296  (512, 512, 3, 3)   9437184  2.694332e-08    3.812698e-12   0.063567    8.725643e-07        3.052055e-12       2.058637      True
21  layers.14.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
22  layers.15.conv  weight  0.257506  2359296  (512, 512, 3, 3)   9437184  2.711095e-08    4.234987e-12   0.063963    8.358489e-07        3.537079e-12       1.972015      True
23  layers.15.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
24  layers.16.conv  weight  0.261944  2359296  (512, 512, 3, 3)   9437184  2.642919e-08    5.443436e-12   0.062354    9.434240e-07        4.554086e-12       2.225816      True
25  layers.16.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
26              fc  weight  0.548242     5120         (10, 512)      5120  1.443846e-05    5.200131e-09   0.073925    3.773056e-05        3.985005e-09       0.193180      True
27              fc    bias  1.000000       10             (10,)        10  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
Parameter Sparsity: 4657711/14719818 (0.3164)
FLOP Sparsity: 118522489/313478154 (0.3781)
Saving results.
