Namespace(dataset='cifar10', model='vgg16', model_class='lottery', dense_classifier=False, pretrained=False, optimizer='adam', train_batch_size=256, test_batch_size=256, pre_epochs=0, post_epochs=100, lr=0.001, lr_drops=[], lr_drop_rate=0.1, weight_decay=0.0, pruner='synflow', compression=2.0, quantization=False, prune_epochs=1, compression_schedule='exponential', mask_scope='global', prune_dataset_ratio=10, prune_batch_size=256, prune_bias=False, prune_batchnorm=False, prune_residual=False, prune_train_mode=False, reinitialize=False, shuffle=False, invert=False, pruner_list=[], prune_epoch_list=[], compression_list=[], level_list=[], experiment='singleshot', expid='cifar10-vgg16-lottery-synflow-preEpochs0-postEpochs100-compression2', result_dir='Results/data', gpu=0, workers=4, no_cuda=False, seed=1, verbose=False)
Loading cifar10 dataset.
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with synflow for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:01<00:00,  1.09s/it]100%|██████████| 1/1 [00:01<00:00,  1.09s/it]
Post-Training for 100 epochs.
  0%|          | 0/100 [00:00<?, ?it/s]Time:  2.994645682978444
  1%|          | 1/100 [00:44<1:12:37, 44.02s/it]Time:  3.6005839549470693
  2%|▏         | 2/100 [01:28<1:12:27, 44.36s/it]Time:  4.090876088943332
  3%|▎         | 3/100 [02:12<1:11:13, 44.06s/it]Time:  4.145390645018779
  4%|▍         | 4/100 [02:55<1:09:45, 43.60s/it]Time:  4.145938777946867
  5%|▌         | 5/100 [03:37<1:08:21, 43.18s/it]Time:  4.116086939000525
  6%|▌         | 6/100 [04:19<1:07:04, 42.81s/it]Time:  4.129138504038565
  7%|▋         | 7/100 [05:02<1:06:05, 42.64s/it]Time:  4.099926519906148
  8%|▊         | 8/100 [05:44<1:05:13, 42.53s/it]Time:  4.117643403005786
  9%|▉         | 9/100 [06:26<1:04:21, 42.43s/it]Time:  4.1151291920104995
 10%|█         | 10/100 [07:07<1:03:04, 42.05s/it]Time:  4.168077328009531
 11%|█         | 11/100 [07:50<1:02:32, 42.16s/it]Time:  4.319588811020367
 12%|█▏        | 12/100 [08:33<1:02:21, 42.52s/it]Time:  3.555183144984767
 13%|█▎        | 13/100 [09:18<1:02:43, 43.26s/it]Time:  2.2861009660409763
 14%|█▍        | 14/100 [10:02<1:02:20, 43.49s/it]Time:  2.901704464922659
 15%|█▌        | 15/100 [10:48<1:02:46, 44.32s/it]Time:  2.5462644890649244
 16%|█▌        | 16/100 [11:33<1:02:23, 44.56s/it]Time:  2.5298469710396603
 17%|█▋        | 17/100 [12:19<1:02:05, 44.88s/it]Time:  2.900466916966252
 18%|█▊        | 18/100 [13:04<1:01:31, 45.02s/it]Time:  3.0113736039493233
 19%|█▉        | 19/100 [13:49<1:00:41, 44.96s/it]Time:  2.3680448989616707
 20%|██        | 20/100 [14:34<59:53, 44.92s/it]  Time:  2.39436770696193
 21%|██        | 21/100 [15:19<59:06, 44.89s/it]Time:  3.1733533010119572
 22%|██▏       | 22/100 [16:05<58:44, 45.18s/it]Time:  3.3173401890089735
 23%|██▎       | 23/100 [16:51<58:19, 45.44s/it]Time:  3.480858798022382
 24%|██▍       | 24/100 [17:36<57:35, 45.46s/it]Time:  3.755831590970047
 25%|██▌       | 25/100 [18:22<56:51, 45.49s/it]Time:  3.834637694992125
 26%|██▌       | 26/100 [19:07<56:09, 45.54s/it]Time:  3.9026349070481956
 27%|██▋       | 27/100 [19:52<54:59, 45.19s/it]Time:  4.024018716998398
 28%|██▊       | 28/100 [20:37<54:04, 45.06s/it]Time:  4.161987417959608
 29%|██▉       | 29/100 [21:21<52:59, 44.79s/it]Time:  4.171382382977754
 30%|███       | 30/100 [22:05<52:13, 44.76s/it]Time:  4.1216705170227215
 31%|███       | 31/100 [22:50<51:18, 44.62s/it]Time:  4.117581931990571
 32%|███▏      | 32/100 [23:34<50:29, 44.56s/it]Time:  4.142988895997405
 33%|███▎      | 33/100 [24:19<49:42, 44.52s/it]Time:  4.123066261061467
 34%|███▍      | 34/100 [25:03<49:03, 44.59s/it]Time:  4.137925470015034
 35%|███▌      | 35/100 [25:48<48:14, 44.54s/it]Time:  4.127250027027912
 36%|███▌      | 36/100 [26:32<47:31, 44.55s/it]Time:  4.102372109889984
 37%|███▋      | 37/100 [27:17<46:44, 44.52s/it]Time:  4.120554781053215
 38%|███▊      | 38/100 [28:01<46:03, 44.57s/it]Time:  4.242341178003699
 39%|███▉      | 39/100 [28:46<45:23, 44.65s/it]Time:  4.122668924042955
 40%|████      | 40/100 [29:31<44:41, 44.69s/it]Time:  4.130313663976267
 41%|████      | 41/100 [30:16<43:55, 44.67s/it]Time:  6.153088717954233
 42%|████▏     | 42/100 [31:03<43:51, 45.37s/it]Time:  4.182231183978729
 43%|████▎     | 43/100 [31:48<42:58, 45.24s/it]Time:  4.162733270088211
 44%|████▍     | 44/100 [32:32<42:02, 45.05s/it]Time:  4.16565871599596
 45%|████▌     | 45/100 [33:17<41:08, 44.87s/it]Time:  4.154878014000133
 46%|████▌     | 46/100 [34:01<40:17, 44.77s/it]Time:  4.161851419019513
 47%|████▋     | 47/100 [34:46<39:31, 44.74s/it]Time:  4.166278816992417
 48%|████▊     | 48/100 [35:30<38:40, 44.62s/it]Time:  4.700279328972101
 49%|████▉     | 49/100 [36:17<38:23, 45.17s/it]Time:  3.785238091950305
 50%|█████     | 50/100 [37:01<37:27, 44.95s/it]Time:  3.8492372860200703
 51%|█████     | 51/100 [37:46<36:39, 44.89s/it]Time:  3.794192538014613
 52%|█████▏    | 52/100 [38:30<35:50, 44.81s/it]Time:  3.826692085014656
 53%|█████▎    | 53/100 [39:15<35:03, 44.75s/it]Time:  3.8551580759231
 54%|█████▍    | 54/100 [40:00<34:17, 44.74s/it]Time:  3.795320117031224
 55%|█████▌    | 55/100 [40:44<33:30, 44.68s/it]Time:  3.7918369660619646
 56%|█████▌    | 56/100 [41:29<32:42, 44.60s/it]Time:  3.96276195300743
 57%|█████▋    | 57/100 [42:16<32:27, 45.29s/it]Time:  3.898775439010933
 58%|█████▊    | 58/100 [43:01<31:40, 45.25s/it]Time:  3.9996596979908645
 59%|█████▉    | 59/100 [43:46<30:52, 45.19s/it]Time:  3.8913352160016075
 60%|██████    | 60/100 [44:31<30:03, 45.09s/it]Time:  3.8944312279345468
 61%|██████    | 61/100 [45:16<29:17, 45.06s/it]Time:  3.8927756659686565
 62%|██████▏   | 62/100 [46:01<28:34, 45.11s/it]Time:  3.9481471478939056
 63%|██████▎   | 63/100 [46:46<27:49, 45.12s/it]Time:  3.9440715089440346
 64%|██████▍   | 64/100 [47:31<27:05, 45.14s/it]Time:  3.9074173429980874
 65%|██████▌   | 65/100 [48:17<26:21, 45.18s/it]Time:  3.9373714139219373
 66%|██████▌   | 66/100 [49:02<25:35, 45.16s/it]Time:  3.9656381519744173
 67%|██████▋   | 67/100 [49:47<24:50, 45.16s/it]Time:  3.9471656819805503
 68%|██████▊   | 68/100 [50:32<24:09, 45.28s/it]Time:  3.900205074925907
 69%|██████▉   | 69/100 [51:18<23:25, 45.33s/it]Time:  3.9306508048903197
 70%|███████   | 70/100 [52:04<22:45, 45.53s/it]Time:  3.9479788009775802
 71%|███████   | 71/100 [52:50<22:03, 45.65s/it]Time:  3.9427590729901567
 72%|███████▏  | 72/100 [53:35<21:13, 45.47s/it]Time:  3.9460090389475226
 73%|███████▎  | 73/100 [54:20<20:24, 45.35s/it]Time:  3.922912001959048
 74%|███████▍  | 74/100 [55:05<19:39, 45.37s/it]Time:  5.249689433025196
 75%|███████▌  | 75/100 [55:51<19:00, 45.61s/it]Time:  3.9988702120026574
 76%|███████▌  | 76/100 [56:37<18:11, 45.46s/it]Time:  4.000759736052714
 77%|███████▋  | 77/100 [57:22<17:23, 45.38s/it]Time:  4.003241610946134
 78%|███████▊  | 78/100 [58:07<16:36, 45.32s/it]Time:  3.974783317069523
 79%|███████▉  | 79/100 [58:52<15:50, 45.29s/it]Time:  3.9341483550379053
 80%|████████  | 80/100 [59:37<15:05, 45.28s/it]Time:  3.9387412070063874
 81%|████████  | 81/100 [1:00:23<14:19, 45.24s/it]Time:  3.9502098319353536
 82%|████████▏ | 82/100 [1:01:08<13:32, 45.16s/it]Time:  3.923180515994318
 83%|████████▎ | 83/100 [1:01:53<12:49, 45.28s/it]Time:  3.7610088399378583
 84%|████████▍ | 84/100 [1:02:38<12:02, 45.15s/it]Time:  3.8045545700006187
 85%|████████▌ | 85/100 [1:03:23<11:16, 45.09s/it]Time:  3.7666800919687375
 86%|████████▌ | 86/100 [1:04:07<10:29, 44.94s/it]Time:  3.765233655110933
 87%|████████▋ | 87/100 [1:04:52<09:43, 44.85s/it]Time:  3.7814405830577016
 88%|████████▊ | 88/100 [1:05:37<08:57, 44.75s/it]Time:  3.7572857970371842
 89%|████████▉ | 89/100 [1:06:21<08:12, 44.79s/it]Time:  3.8127649578964338
 90%|█████████ | 90/100 [1:07:06<07:28, 44.81s/it]Time:  5.129879600019194
 91%|█████████ | 91/100 [1:07:52<06:46, 45.19s/it]Time:  3.7695972400251776
 92%|█████████▏| 92/100 [1:08:37<05:59, 44.94s/it]Time:  3.786161986994557
 93%|█████████▎| 93/100 [1:09:22<05:14, 44.93s/it]Time:  3.9245920699322596
 94%|█████████▍| 94/100 [1:10:07<04:29, 44.92s/it]Time:  3.9359425200382248
 95%|█████████▌| 95/100 [1:10:52<03:44, 44.97s/it]Time:  3.9480694899102673
 96%|█████████▌| 96/100 [1:11:38<03:01, 45.38s/it]Time:  3.7300107890041545
 97%|█████████▋| 97/100 [1:12:33<02:24, 48.22s/it]Time:  3.6447597549995407
 98%|█████████▊| 98/100 [1:13:19<01:34, 47.46s/it]Time:  3.6977592600742355
 99%|█████████▉| 99/100 [1:14:04<00:46, 46.90s/it]Time:  2.7245555120753124
100%|██████████| 100/100 [1:14:46<00:00, 45.36s/it]100%|██████████| 100/100 [1:14:46<00:00, 44.86s/it]
Post-training time: 4489.62 seconds
GPU memory allocated: 375.68 MB, peak: 999.49 MB
/n/fs/klips/anaconda3/envs/cos568/lib/python3.10/site-packages/numpy/_core/_methods.py:197: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/n/fs/klips/anaconda3/envs/cos568/lib/python3.10/site-packages/numpy/_core/_methods.py:208: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
Train results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy  test_time
Init.      0           NaN   2.417717          11.73          50.17   5.316629
Pre-Prune  0           NaN   2.417717          11.73          50.17   5.316629
Post-Prune 0           NaN   2.302585          10.00          50.00   3.199855
Final      100    2.302662   2.302589          10.00          50.00   2.724556
Prune results:
             module   param  sparsity     size             shape     flops    score mean  score variance     score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.988426     1728     (64, 3, 3, 3)   1769472  1.704298e+19             inf  2.945027e+22    1.704298e+19                 inf   2.945027e+22      True
1    layers.0.conv    bias  1.000000       64             (64,)     65536  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
2    layers.1.conv  weight  0.806396    36864    (64, 64, 3, 3)  37748736  7.988898e+17             inf  2.945027e+22    7.988898e+17                 inf   2.945027e+22      True
3    layers.1.conv    bias  1.000000       64             (64,)     65536  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
4    layers.3.conv  weight  0.626017    73728   (128, 64, 3, 3)  18874368  3.994448e+17             inf  2.945027e+22    3.994448e+17                 inf   2.945027e+22      True
5    layers.3.conv    bias  1.000000      128            (128,)     32768  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
6    layers.4.conv  weight  0.332425   147456  (128, 128, 3, 3)  37748736  1.997224e+17             inf  2.945027e+22    1.997224e+17                 inf   2.945027e+22      True
7    layers.4.conv    bias  1.000000      128            (128,)     32768  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
8    layers.6.conv  weight  0.052199   294912  (256, 128, 3, 3)  18874368  9.986122e+16             inf  2.945027e+22    9.986122e+16                 inf   2.945027e+22      True
9    layers.6.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
10   layers.7.conv  weight  0.000154   589824  (256, 256, 3, 3)  37748736  4.993060e+16             inf  2.945027e+22    4.993060e+16                 inf   2.945027e+22      True
11   layers.7.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
12   layers.8.conv  weight  0.000112   589824  (256, 256, 3, 3)  37748736  4.993061e+16             inf  2.945027e+22    4.993061e+16                 inf   2.945027e+22      True
13   layers.8.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
14  layers.10.conv  weight  0.000000  1179648  (512, 256, 3, 3)  18874368  2.496530e+16             inf  2.945027e+22    2.496530e+16                 inf   2.945027e+22      True
15  layers.10.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
16  layers.11.conv  weight  0.000000  2359296  (512, 512, 3, 3)  37748736  1.248265e+16    9.402646e+31  2.945026e+22    1.248265e+16        9.402646e+31   2.945026e+22      True
17  layers.11.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
18  layers.12.conv  weight  0.000000  2359296  (512, 512, 3, 3)  37748736  1.248265e+16    9.415211e+31  2.945027e+22    1.248265e+16        9.415211e+31   2.945027e+22      True
19  layers.12.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
20  layers.14.conv  weight  0.000000  2359296  (512, 512, 3, 3)   9437184  1.248265e+16             inf  2.945026e+22    1.248265e+16                 inf   2.945026e+22      True
21  layers.14.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
22  layers.15.conv  weight  0.000000  2359296  (512, 512, 3, 3)   9437184  1.248265e+16             inf  2.945026e+22    1.248265e+16                 inf   2.945026e+22      True
23  layers.15.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
24  layers.16.conv  weight  0.000000  2359296  (512, 512, 3, 3)   9437184  1.248265e+16             inf  2.945028e+22    1.248265e+16                 inf   2.945028e+22      True
25  layers.16.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
26              fc  weight  0.975977     5120         (10, 512)      5120  5.752007e+18             inf  2.945027e+22    5.752007e+18                 inf   2.945027e+22      True
27              fc    bias  1.000000       10             (10,)        10  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
Parameter Sparsity: 151390/14719818 (0.0103)
FLOP Sparsity: 57830479/313478154 (0.1845)
Saving results.
