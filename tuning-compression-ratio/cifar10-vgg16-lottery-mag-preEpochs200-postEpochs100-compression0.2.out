Namespace(dataset='cifar10', model='vgg16', model_class='lottery', dense_classifier=False, pretrained=False, optimizer='adam', train_batch_size=256, test_batch_size=256, pre_epochs=200, post_epochs=100, lr=0.001, lr_drops=[], lr_drop_rate=0.1, weight_decay=0.0, pruner='mag', compression=0.2, quantization=False, prune_epochs=1, compression_schedule='exponential', mask_scope='global', prune_dataset_ratio=10, prune_batch_size=256, prune_bias=False, prune_batchnorm=False, prune_residual=False, prune_train_mode=False, reinitialize=False, shuffle=False, invert=False, pruner_list=[], prune_epoch_list=[], compression_list=[], level_list=[], experiment='singleshot', expid='cifar10-vgg16-lottery-mag-preEpochs200-postEpochs100-compression0.2', result_dir='Results/data', gpu=0, workers=4, no_cuda=False, seed=1, verbose=False)
Loading cifar10 dataset.
Creating lottery-vgg16 model.
Pre-Train for 200 epochs.
  0%|          | 0/200 [00:00<?, ?it/s]Time:  0.9757759239291772
  0%|          | 1/200 [00:08<29:31,  8.90s/it]Time:  0.9798920970642939
  1%|          | 2/200 [00:17<29:28,  8.93s/it]Time:  1.0247160119470209
  2%|▏         | 3/200 [00:26<29:25,  8.96s/it]Time:  0.987768657039851
  2%|▏         | 4/200 [00:35<29:17,  8.97s/it]Time:  0.9147914110217243
  2%|▎         | 5/200 [00:44<29:03,  8.94s/it]Time:  0.9548268869984895
  3%|▎         | 6/200 [00:53<28:59,  8.97s/it]Time:  0.9980870930012316
  4%|▎         | 7/200 [01:02<28:56,  9.00s/it]Time:  1.0196291470201686
  4%|▍         | 8/200 [01:11<28:52,  9.02s/it]Time:  0.9318974050693214
  4%|▍         | 9/200 [01:21<28:59,  9.11s/it]Time:  1.6334413409931585
  5%|▌         | 10/200 [01:34<33:21, 10.53s/it]Time:  1.6914277409669012
  6%|▌         | 11/200 [01:52<40:08, 12.74s/it]Time:  1.6624046110082418
  6%|▌         | 12/200 [02:10<44:36, 14.24s/it]Time:  1.6596878280397505
  6%|▋         | 13/200 [02:28<47:43, 15.31s/it]Time:  1.6608229789417237
  7%|▋         | 14/200 [02:45<49:43, 16.04s/it]Time:  1.69092007400468
  8%|▊         | 15/200 [03:03<51:09, 16.59s/it]Time:  1.6519167630467564
  8%|▊         | 16/200 [03:21<51:55, 16.93s/it]Time:  1.682832738966681
  8%|▊         | 17/200 [03:38<52:11, 17.11s/it]Time:  2.2334649770054966
  9%|▉         | 18/200 [03:57<53:09, 17.52s/it]Time:  1.9490732139674947
 10%|▉         | 19/200 [04:23<1:00:44, 20.13s/it]Time:  2.102357115945779
 10%|█         | 20/200 [04:50<1:06:06, 22.04s/it]Time:  2.0187621250515804
 10%|█         | 21/200 [05:16<1:09:34, 23.32s/it]Time:  1.965718580991961
 11%|█         | 22/200 [05:42<1:11:45, 24.19s/it]Time:  2.0919357730308548
 12%|█▏        | 23/200 [06:09<1:13:44, 25.00s/it]Time:  2.1411664709448814
 12%|█▏        | 24/200 [06:36<1:14:45, 25.48s/it]Time:  2.1456503750523552
 12%|█▎        | 25/200 [07:02<1:15:11, 25.78s/it]Time:  2.318350210087374
 13%|█▎        | 26/200 [07:29<1:15:49, 26.15s/it]Time:  2.3240335979498923
 14%|█▎        | 27/200 [07:56<1:15:45, 26.27s/it]Time:  2.601465657004155
 14%|█▍        | 28/200 [08:23<1:16:00, 26.51s/it]Time:  2.7135607609525323
 14%|█▍        | 29/200 [08:49<1:15:39, 26.54s/it]Time:  2.5608058370416984
 15%|█▌        | 30/200 [09:16<1:15:04, 26.50s/it]Time:  2.8284141259500757
 16%|█▌        | 31/200 [09:42<1:14:44, 26.54s/it]Time:  2.7591602969914675
 16%|█▌        | 32/200 [10:09<1:14:14, 26.51s/it]Time:  2.494244043948129
 16%|█▋        | 33/200 [10:35<1:13:43, 26.49s/it]Time:  2.4754772420274094
 17%|█▋        | 34/200 [11:01<1:12:54, 26.35s/it]Time:  2.5161248280201107
 18%|█▊        | 35/200 [11:28<1:12:22, 26.32s/it]Time:  2.5462913749506697
 18%|█▊        | 36/200 [11:53<1:11:35, 26.19s/it]Time:  2.566297248005867
 18%|█▊        | 37/200 [12:20<1:11:36, 26.36s/it]Time:  102.79674583300948
 19%|█▉        | 38/200 [14:30<2:34:34, 57.25s/it]Time:  934.4788075389806
 20%|█▉        | 39/200 [1:11:27<47:38:16, 1065.20s/it]