Namespace(dataset='cifar10', model='vgg16', model_class='lottery', dense_classifier=False, pretrained=False, optimizer='adam', train_batch_size=256, test_batch_size=256, pre_epochs=0, post_epochs=100, lr=0.001, lr_drops=[], lr_drop_rate=0.1, weight_decay=0.0, pruner='synflow', compression=0.5, quantization=False, prune_epochs=1, compression_schedule='exponential', mask_scope='global', prune_dataset_ratio=10, prune_batch_size=256, prune_bias=False, prune_batchnorm=False, prune_residual=False, prune_train_mode=False, reinitialize=False, shuffle=False, invert=False, pruner_list=[], prune_epoch_list=[], compression_list=[], level_list=[], experiment='singleshot', expid='cifar10-vgg16-lottery-synflow-preEpochs0-postEpochs100-compression0.5', result_dir='Results/data', gpu=0, workers=4, no_cuda=False, seed=1, verbose=False)
Loading cifar10 dataset.
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with synflow for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:01<00:00,  1.13s/it]100%|██████████| 1/1 [00:01<00:00,  1.13s/it]
Post-Training for 100 epochs.
  0%|          | 0/100 [00:00<?, ?it/s]Time:  3.1559424019651487
  1%|          | 1/100 [00:45<1:14:40, 45.25s/it]Time:  3.0511935129761696
  2%|▏         | 2/100 [01:31<1:14:43, 45.76s/it]Time:  3.242902970057912
  3%|▎         | 3/100 [02:18<1:14:45, 46.24s/it]Time:  3.203198099974543
  4%|▍         | 4/100 [03:04<1:13:45, 46.10s/it]Time:  3.4337068198947236
  5%|▌         | 5/100 [03:50<1:13:12, 46.24s/it]Time:  3.401343595003709
  6%|▌         | 6/100 [04:36<1:12:25, 46.23s/it]Time:  3.4469517999095842
  7%|▋         | 7/100 [05:23<1:11:44, 46.28s/it]Time:  3.546031353995204
  8%|▊         | 8/100 [06:09<1:10:59, 46.30s/it]Time:  3.422346311970614
  9%|▉         | 9/100 [06:55<1:10:12, 46.29s/it]Time:  3.5017854740144685
 10%|█         | 10/100 [07:42<1:09:30, 46.33s/it]Time:  3.471425633993931
 11%|█         | 11/100 [08:28<1:08:41, 46.31s/it]Time:  4.475091947009787
 12%|█▏        | 12/100 [09:15<1:08:12, 46.51s/it]Time:  3.0837633199989796
 13%|█▎        | 13/100 [10:01<1:07:10, 46.33s/it]Time:  3.2022652390878648
 14%|█▍        | 14/100 [10:47<1:06:19, 46.27s/it]Time:  2.957529320032336
 15%|█▌        | 15/100 [11:32<1:05:12, 46.04s/it]Time:  2.8736452190205455
 16%|█▌        | 16/100 [12:19<1:04:33, 46.11s/it]Time:  2.4680720100877807
 17%|█▋        | 17/100 [13:04<1:03:18, 45.76s/it]Time:  2.0642431309679523
 18%|█▊        | 18/100 [13:48<1:01:56, 45.32s/it]Time:  2.8360840620007366
 19%|█▉        | 19/100 [14:33<1:01:02, 45.22s/it]Time:  2.886343537014909
 20%|██        | 20/100 [15:18<1:00:22, 45.28s/it]Time:  3.0753920159768313
 21%|██        | 21/100 [16:05<59:58, 45.55s/it]  Time:  3.3281158530153334
 22%|██▏       | 22/100 [16:51<59:26, 45.73s/it]Time:  3.8756755749927834
 23%|██▎       | 23/100 [17:37<58:51, 45.86s/it]Time:  3.6765655979979783
 24%|██▍       | 24/100 [18:23<58:07, 45.89s/it]Time:  3.497156514087692
 25%|██▌       | 25/100 [19:09<57:19, 45.86s/it]Time:  3.6116688220063224
 26%|██▌       | 26/100 [19:54<56:20, 45.69s/it]Time:  3.712633028975688
 27%|██▋       | 27/100 [20:40<55:39, 45.75s/it]Time:  4.051388642983511
 28%|██▊       | 28/100 [21:25<54:52, 45.73s/it]Time:  3.9634248729562387
 29%|██▉       | 29/100 [22:11<53:59, 45.62s/it]Time:  3.917120575089939
 30%|███       | 30/100 [22:56<53:05, 45.51s/it]Time:  3.899307406041771
 31%|███       | 31/100 [23:41<52:15, 45.45s/it]Time:  3.931754093966447
 32%|███▏      | 32/100 [24:27<51:26, 45.39s/it]Time:  3.9579340079799294
 33%|███▎      | 33/100 [25:12<50:33, 45.27s/it]Time:  3.9645684469724074
 34%|███▍      | 34/100 [25:57<49:46, 45.24s/it]Time:  3.926715879002586
 35%|███▌      | 35/100 [26:42<49:01, 45.25s/it]Time:  3.9432408339343965
 36%|███▌      | 36/100 [27:27<48:12, 45.20s/it]Time:  3.986764681059867
 37%|███▋      | 37/100 [28:12<47:22, 45.12s/it]Time:  3.9166098299901932
 38%|███▊      | 38/100 [28:57<46:35, 45.09s/it]Time:  3.972111538052559
 39%|███▉      | 39/100 [29:42<45:49, 45.07s/it]Time:  3.9796446420950815
 40%|████      | 40/100 [30:27<44:57, 44.97s/it]Time:  4.000062476028688
 41%|████      | 41/100 [31:13<44:36, 45.37s/it]Time:  4.082997757010162
 42%|████▏     | 42/100 [31:59<43:51, 45.36s/it]Time:  4.161350863985717
 43%|████▎     | 43/100 [32:44<43:03, 45.32s/it]Time:  4.163193055079319
 44%|████▍     | 44/100 [33:29<42:13, 45.24s/it]Time:  4.118378842016682
 45%|████▌     | 45/100 [34:14<41:26, 45.20s/it]Time:  4.091505118994974
 46%|████▌     | 46/100 [34:59<40:35, 45.10s/it]Time:  4.098980753915384
 47%|████▋     | 47/100 [35:44<39:45, 45.01s/it]Time:  4.165467530954629
 48%|████▊     | 48/100 [36:26<38:27, 44.37s/it]Time:  4.107181889936328
 49%|████▉     | 49/100 [37:11<37:49, 44.51s/it]Time:  4.110122903017327
 50%|█████     | 50/100 [37:56<37:08, 44.56s/it]Time:  4.135189629974775
 51%|█████     | 51/100 [38:41<36:23, 44.56s/it]Time:  4.1139574541011825
 52%|█████▏    | 52/100 [39:26<35:45, 44.69s/it]Time:  4.095432130037807
 53%|█████▎    | 53/100 [40:10<34:59, 44.67s/it]Time:  4.10397989791818
 54%|█████▍    | 54/100 [40:55<34:17, 44.72s/it]Time:  4.190762487938628
 55%|█████▌    | 55/100 [41:40<33:35, 44.79s/it]Time:  4.11522838100791
 56%|█████▌    | 56/100 [42:26<33:01, 45.04s/it]Time:  4.098144151037559
 57%|█████▋    | 57/100 [43:10<32:09, 44.86s/it]Time:  4.092410920071416
 58%|█████▊    | 58/100 [43:54<31:17, 44.70s/it]Time:  4.111553957918659
 59%|█████▉    | 59/100 [44:39<30:30, 44.64s/it]Time:  4.095582316047512
 60%|██████    | 60/100 [45:23<29:45, 44.63s/it]Time:  4.111714817001484
 61%|██████    | 61/100 [46:08<29:01, 44.66s/it]Time:  4.094135688035749
 62%|██████▏   | 62/100 [46:53<28:15, 44.62s/it]Time:  4.082763792946935
 63%|██████▎   | 63/100 [47:37<27:30, 44.61s/it]Time:  4.116454083006829
 64%|██████▍   | 64/100 [48:22<26:45, 44.59s/it]Time:  4.1920625671045855
 65%|██████▌   | 65/100 [49:07<26:05, 44.72s/it]Time:  4.191453889012337
 66%|██████▌   | 66/100 [49:52<25:23, 44.80s/it]Time:  4.125276430975646
 67%|██████▋   | 67/100 [50:37<24:42, 44.91s/it]Time:  4.161053983028978
 68%|██████▊   | 68/100 [51:22<24:02, 45.07s/it]Time:  5.3342086910270154
 69%|██████▉   | 69/100 [52:09<23:32, 45.57s/it]Time:  4.114625392016023
 70%|███████   | 70/100 [52:55<22:45, 45.52s/it]Time:  3.838772231945768
 71%|███████   | 71/100 [53:40<21:57, 45.43s/it]Time:  4.013628632994369
 72%|███████▏  | 72/100 [54:26<21:16, 45.59s/it]Time:  3.858257070998661
 73%|███████▎  | 73/100 [55:11<20:31, 45.60s/it]Time:  3.8919705109437928
 74%|███████▍  | 74/100 [55:57<19:45, 45.59s/it]Time:  3.8423956549959257
 75%|███████▌  | 75/100 [56:42<18:56, 45.47s/it]Time:  3.7943831119919196
 76%|███████▌  | 76/100 [57:27<18:09, 45.38s/it]Time:  3.852682898985222
 77%|███████▋  | 77/100 [58:13<17:22, 45.33s/it]Time:  3.844975801068358
 78%|███████▊  | 78/100 [58:57<16:34, 45.18s/it]Time:  3.9741488930303603
 79%|███████▉  | 79/100 [59:43<15:49, 45.21s/it]Time:  3.911544742062688
 80%|████████  | 80/100 [1:00:28<15:04, 45.24s/it]Time:  3.866641471046023
 81%|████████  | 81/100 [1:01:13<14:16, 45.10s/it]Time:  4.030298677971587
 82%|████████▏ | 82/100 [1:01:58<13:32, 45.13s/it]Time:  3.8597559389891103
 83%|████████▎ | 83/100 [1:02:43<12:48, 45.21s/it]Time:  3.81661628105212
 84%|████████▍ | 84/100 [1:03:29<12:04, 45.26s/it]Time:  3.8860407290048897
 85%|████████▌ | 85/100 [1:04:14<11:19, 45.31s/it]Time:  3.8892200370319188
 86%|████████▌ | 86/100 [1:04:59<10:34, 45.32s/it]Time:  3.825420545064844
 87%|████████▋ | 87/100 [1:05:45<09:48, 45.23s/it]Time:  3.787142887013033
 88%|████████▊ | 88/100 [1:06:29<09:01, 45.14s/it]Time:  3.8023385999258608
 89%|████████▉ | 89/100 [1:07:15<08:17, 45.19s/it]Time:  3.90014541905839
 90%|█████████ | 90/100 [1:08:00<07:33, 45.33s/it]Time:  3.8180544188944623
 91%|█████████ | 91/100 [1:08:46<06:47, 45.28s/it]Time:  3.8636156669817865
 92%|█████████▏| 92/100 [1:09:31<06:02, 45.25s/it]Time:  3.843562565976754
 93%|█████████▎| 93/100 [1:10:16<05:16, 45.23s/it]Time:  3.807715915958397
 94%|█████████▍| 94/100 [1:11:01<04:30, 45.13s/it]Time:  3.6963343599345535
 95%|█████████▌| 95/100 [1:11:45<03:44, 44.94s/it]Time:  1.9892395340139046
 96%|█████████▌| 96/100 [1:12:27<02:55, 43.97s/it]Time:  3.8056736750295386
 97%|█████████▋| 97/100 [1:13:07<02:07, 42.62s/it]Time:  3.78038657293655
 98%|█████████▊| 98/100 [1:13:51<01:26, 43.32s/it]Time:  3.5747738119680434
 99%|█████████▉| 99/100 [1:14:36<00:43, 43.67s/it]Time:  2.444214978022501
100%|██████████| 100/100 [1:15:05<00:00, 39.34s/it]100%|██████████| 100/100 [1:15:05<00:00, 45.06s/it]
Post-training time: 4508.91 seconds
GPU memory allocated: 375.68 MB, peak: 999.49 MB
/n/fs/klips/anaconda3/envs/cos568/lib/python3.10/site-packages/numpy/_core/_methods.py:197: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/n/fs/klips/anaconda3/envs/cos568/lib/python3.10/site-packages/numpy/_core/_methods.py:208: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
Train results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy  test_time
Init.      0           NaN   2.417717          11.73          50.17   5.320984
Pre-Prune  0           NaN   2.417717          11.73          50.17   5.320984
Post-Prune 0           NaN   2.329115           9.99          51.15   3.246017
Final      100    0.080172   0.547841          87.76          98.76   2.444215
Prune results:
             module   param  sparsity     size             shape     flops    score mean  score variance     score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.997685     1728     (64, 3, 3, 3)   1769472  1.704298e+19             inf  2.945027e+22    1.704298e+19                 inf   2.945027e+22      True
1    layers.0.conv    bias  1.000000       64             (64,)     65536  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
2    layers.1.conv  weight  0.983805    36864    (64, 64, 3, 3)  37748736  7.988898e+17             inf  2.945027e+22    7.988898e+17                 inf   2.945027e+22      True
3    layers.1.conv    bias  1.000000       64             (64,)     65536  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
4    layers.3.conv  weight  0.969618    73728   (128, 64, 3, 3)  18874368  3.994448e+17             inf  2.945027e+22    3.994448e+17                 inf   2.945027e+22      True
5    layers.3.conv    bias  1.000000      128            (128,)     32768  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
6    layers.4.conv  weight  0.940477   147456  (128, 128, 3, 3)  37748736  1.997224e+17             inf  2.945027e+22    1.997224e+17                 inf   2.945027e+22      True
7    layers.4.conv    bias  1.000000      128            (128,)     32768  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
8    layers.6.conv  weight  0.881911   294912  (256, 128, 3, 3)  18874368  9.986122e+16             inf  2.945027e+22    9.986122e+16                 inf   2.945027e+22      True
9    layers.6.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
10   layers.7.conv  weight  0.766754   589824  (256, 256, 3, 3)  37748736  4.993060e+16             inf  2.945027e+22    4.993060e+16                 inf   2.945027e+22      True
11   layers.7.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
12   layers.8.conv  weight  0.766447   589824  (256, 256, 3, 3)  37748736  4.993061e+16             inf  2.945027e+22    4.993061e+16                 inf   2.945027e+22      True
13   layers.8.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
14  layers.10.conv  weight  0.546590  1179648  (512, 256, 3, 3)  18874368  2.496530e+16             inf  2.945027e+22    2.496530e+16                 inf   2.945027e+22      True
15  layers.10.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
16  layers.11.conv  weight  0.232446  2359296  (512, 512, 3, 3)  37748736  1.248265e+16    9.402646e+31  2.945026e+22    1.248265e+16        9.402646e+31   2.945026e+22      True
17  layers.11.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
18  layers.12.conv  weight  0.232694  2359296  (512, 512, 3, 3)  37748736  1.248265e+16    9.415211e+31  2.945027e+22    1.248265e+16        9.415211e+31   2.945027e+22      True
19  layers.12.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
20  layers.14.conv  weight  0.211462  2359296  (512, 512, 3, 3)   9437184  1.248265e+16             inf  2.945026e+22    1.248265e+16                 inf   2.945026e+22      True
21  layers.14.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
22  layers.15.conv  weight  0.211558  2359296  (512, 512, 3, 3)   9437184  1.248265e+16             inf  2.945026e+22    1.248265e+16                 inf   2.945026e+22      True
23  layers.15.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
24  layers.16.conv  weight  0.210058  2359296  (512, 512, 3, 3)   9437184  1.248265e+16             inf  2.945028e+22    1.248265e+16                 inf   2.945028e+22      True
25  layers.16.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
26              fc  weight  0.997656     5120         (10, 512)      5120  5.752007e+18             inf  2.945027e+22    5.752007e+18                 inf   2.945027e+22      True
27              fc    bias  1.000000       10             (10,)        10  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
Parameter Sparsity: 4657710/14719818 (0.3164)
FLOP Sparsity: 201358519/313478154 (0.6423)
Saving results.
