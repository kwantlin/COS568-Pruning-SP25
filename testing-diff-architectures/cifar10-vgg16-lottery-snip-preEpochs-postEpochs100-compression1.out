Namespace(dataset='cifar10', model='vgg16', model_class='lottery', dense_classifier=False, pretrained=False, optimizer='adam', train_batch_size=256, test_batch_size=256, pre_epochs=0, post_epochs=100, lr=0.001, lr_drops=[], lr_drop_rate=0.1, weight_decay=0.0, pruner='snip', compression=1.0, quantization=False, prune_epochs=1, compression_schedule='exponential', mask_scope='global', prune_dataset_ratio=10, prune_batch_size=256, prune_bias=False, prune_batchnorm=False, prune_residual=False, prune_train_mode=False, reinitialize=False, shuffle=False, invert=False, pruner_list=[], prune_epoch_list=[], compression_list=[], level_list=[], experiment='singleshot', expid='cifar10-vgg16-lottery-snip-preEpochs-postEpochs100-compression1', result_dir='Results/data', gpu=0, workers=4, no_cuda=False, seed=1, verbose=False)
Loading cifar10 dataset.
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with snip for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  1.27it/s]100%|██████████| 1/1 [00:00<00:00,  1.27it/s]
Post-Training for 100 epochs.
  0%|          | 0/100 [00:00<?, ?it/s]Time:  3.6577084170130547
  1%|          | 1/100 [00:48<1:19:29, 48.17s/it]Time:  3.776010444998974
  2%|▏         | 2/100 [01:36<1:18:47, 48.24s/it]Time:  3.2745001369912643
  3%|▎         | 3/100 [02:23<1:17:18, 47.82s/it]Time:  3.37062126598903
  4%|▍         | 4/100 [03:11<1:16:11, 47.62s/it]Time:  3.5546581870003138
  5%|▌         | 5/100 [03:59<1:15:35, 47.74s/it]Time:  3.692438834987115
  6%|▌         | 6/100 [04:46<1:14:51, 47.78s/it]Time:  3.7394305340130813
  7%|▋         | 7/100 [05:34<1:14:04, 47.79s/it]Time:  4.101300505979452
  8%|▊         | 8/100 [06:22<1:13:14, 47.76s/it]Time:  4.1683418979810085
  9%|▉         | 9/100 [07:09<1:12:21, 47.70s/it]Time:  3.901264321990311
 10%|█         | 10/100 [07:57<1:11:36, 47.74s/it]Time:  3.9727829709881917
 11%|█         | 11/100 [08:45<1:10:54, 47.80s/it]Time:  3.917706269014161
 12%|█▏        | 12/100 [09:33<1:10:12, 47.87s/it]Time:  4.053735416004201
 13%|█▎        | 13/100 [10:21<1:09:25, 47.88s/it]Time:  3.7399375649983995
 14%|█▍        | 14/100 [11:09<1:08:29, 47.78s/it]Time:  3.8284851610078476
 15%|█▌        | 15/100 [11:56<1:07:38, 47.75s/it]Time:  3.8996489770070184
 16%|█▌        | 16/100 [12:44<1:06:43, 47.66s/it]Time:  4.203525972989155
 17%|█▋        | 17/100 [13:32<1:06:05, 47.77s/it]Time:  4.041670321981655
 18%|█▊        | 18/100 [14:20<1:05:15, 47.75s/it]Time:  4.009554190008203
 19%|█▉        | 19/100 [15:08<1:04:37, 47.87s/it]Time:  4.0991899500077125
 20%|██        | 20/100 [15:55<1:03:39, 47.75s/it]Time:  3.961506402993109
 21%|██        | 21/100 [16:43<1:02:45, 47.67s/it]Time:  3.9764825129823294
 22%|██▏       | 22/100 [17:30<1:01:55, 47.63s/it]Time:  3.840437781997025
 23%|██▎       | 23/100 [18:18<1:01:04, 47.59s/it]Time:  3.878694043989526
 24%|██▍       | 24/100 [19:05<1:00:17, 47.60s/it]Time:  4.041126288997475
 25%|██▌       | 25/100 [19:53<59:34, 47.66s/it]  Time:  4.12191893401905
 26%|██▌       | 26/100 [20:41<58:44, 47.63s/it]Time:  4.288508541998453
 27%|██▋       | 27/100 [21:29<58:01, 47.69s/it]Time:  4.263493102014763
 28%|██▊       | 28/100 [22:16<57:19, 47.77s/it]Time:  4.287474369019037
 29%|██▉       | 29/100 [23:04<56:27, 47.71s/it]Time:  4.447891880001407
 30%|███       | 30/100 [23:51<55:32, 47.60s/it]Time:  4.284069271001499
 31%|███       | 31/100 [24:39<54:40, 47.54s/it]Time:  4.523530539008789
 32%|███▏      | 32/100 [25:27<54:00, 47.65s/it]Time:  4.353086297982372
 33%|███▎      | 33/100 [26:14<53:08, 47.60s/it]Time:  4.306145591981476
 34%|███▍      | 34/100 [27:01<52:14, 47.49s/it]Time:  4.504859677021159
 35%|███▌      | 35/100 [27:48<51:16, 47.33s/it]Time:  4.340923918003682
 36%|███▌      | 36/100 [28:36<50:26, 47.29s/it]Time:  4.49078435098636
 37%|███▋      | 37/100 [29:23<49:34, 47.22s/it]Time:  4.490720132016577
 38%|███▊      | 38/100 [30:10<48:49, 47.24s/it]Time:  4.559409921988845
 39%|███▉      | 39/100 [30:57<48:02, 47.25s/it]Time:  4.495075545011787
 40%|████      | 40/100 [31:44<47:09, 47.16s/it]Time:  4.517506775009679
 41%|████      | 41/100 [32:31<46:11, 46.97s/it]Time:  4.555976534000365
 42%|████▏     | 42/100 [33:18<45:30, 47.08s/it]Time:  4.496523850975791
 43%|████▎     | 43/100 [34:05<44:40, 47.03s/it]Time:  4.608325298991986
 44%|████▍     | 44/100 [34:52<43:55, 47.06s/it]Time:  4.572262368019437
 45%|████▌     | 45/100 [35:39<43:01, 46.93s/it]Time:  4.530053638998652
 46%|████▌     | 46/100 [36:26<42:13, 46.92s/it]Time:  4.6147744090121705
 47%|████▋     | 47/100 [37:13<41:29, 46.97s/it]Time:  4.5196960660105105
 48%|████▊     | 48/100 [37:59<40:39, 46.92s/it]Time:  4.588589233986568
 49%|████▉     | 49/100 [38:46<39:53, 46.93s/it]Time:  4.458432793006068
 50%|█████     | 50/100 [39:33<39:06, 46.93s/it]Time:  4.529715576005401
 51%|█████     | 51/100 [40:20<38:20, 46.96s/it]Time:  4.58297911399859
 52%|█████▏    | 52/100 [41:07<37:29, 46.86s/it]Time:  4.5806829019857105
 53%|█████▎    | 53/100 [41:54<36:41, 46.84s/it]Time:  4.504880445019808
 54%|█████▍    | 54/100 [42:41<35:57, 46.91s/it]Time:  4.465600436000386
 55%|█████▌    | 55/100 [43:28<35:10, 46.90s/it]Time:  4.452782603999367
 56%|█████▌    | 56/100 [44:14<34:18, 46.80s/it]Time:  4.6011915120179765
 57%|█████▋    | 57/100 [45:01<33:36, 46.89s/it]Time:  4.505511896015378
 58%|█████▊    | 58/100 [45:48<32:49, 46.88s/it]Time:  4.590749811002752
 59%|█████▉    | 59/100 [46:35<32:05, 46.96s/it]Time:  4.570509962999495
 60%|██████    | 60/100 [47:23<31:23, 47.08s/it]Time:  4.666181966982549
 61%|██████    | 61/100 [48:10<30:39, 47.16s/it]Time:  4.503929645026801
 62%|██████▏   | 62/100 [48:57<29:53, 47.19s/it]Time:  4.532674036978278
 63%|██████▎   | 63/100 [49:44<29:04, 47.14s/it]Time:  4.502188915997976
 64%|██████▍   | 64/100 [50:31<28:13, 47.03s/it]Time:  4.544429889996536
 65%|██████▌   | 65/100 [51:18<27:20, 46.86s/it]Time:  4.691592981980648
 66%|██████▌   | 66/100 [52:05<26:37, 46.98s/it]Time:  4.475415532011539
 67%|██████▋   | 67/100 [52:51<25:45, 46.82s/it]Time:  4.585828869981924
 68%|██████▊   | 68/100 [53:39<25:03, 46.97s/it]Time:  4.524162653979147
 69%|██████▉   | 69/100 [54:26<24:17, 47.01s/it]Time:  4.59386520498083
 70%|███████   | 70/100 [55:12<23:24, 46.83s/it]Time:  4.618644478992792
 71%|███████   | 71/100 [55:59<22:37, 46.82s/it]Time:  4.663756816997193
 72%|███████▏  | 72/100 [56:46<21:49, 46.77s/it]Time:  4.638120813993737
 73%|███████▎  | 73/100 [57:33<21:04, 46.83s/it]Time:  4.566925083025126
 74%|███████▍  | 74/100 [58:20<20:20, 46.93s/it]Time:  4.607540158001939
 75%|███████▌  | 75/100 [59:07<19:33, 46.94s/it]Time:  4.525828764017206
 76%|███████▌  | 76/100 [59:53<18:44, 46.86s/it]Time:  4.577177688013762
 77%|███████▋  | 77/100 [1:00:41<18:00, 46.96s/it]Time:  4.577846154978033
 78%|███████▊  | 78/100 [1:01:28<17:14, 47.01s/it]Time:  4.604875416989671
 79%|███████▉  | 79/100 [1:02:15<16:30, 47.15s/it]Time:  4.498563077009749
 80%|████████  | 80/100 [1:03:02<15:42, 47.12s/it]Time:  4.605304419004824
 81%|████████  | 81/100 [1:03:49<14:55, 47.13s/it]Time:  4.503320433985209
 82%|████████▏ | 82/100 [1:04:36<14:05, 46.98s/it]Time:  4.575252670998452
 83%|████████▎ | 83/100 [1:05:23<13:16, 46.86s/it]Time:  4.4702198909944855
 84%|████████▍ | 84/100 [1:06:09<12:26, 46.69s/it]Time:  4.56353371302248
 85%|████████▌ | 85/100 [1:06:56<11:43, 46.90s/it]Time:  4.595687290013302
 86%|████████▌ | 86/100 [1:07:44<10:58, 47.04s/it]Time:  4.733267773990519
 87%|████████▋ | 87/100 [1:08:31<10:14, 47.27s/it]Time:  4.508372748998227
 88%|████████▊ | 88/100 [1:09:19<09:26, 47.23s/it]Time:  4.558622312993975
 89%|████████▉ | 89/100 [1:10:06<08:38, 47.17s/it]Time:  4.499184140004218
 90%|█████████ | 90/100 [1:10:53<07:51, 47.14s/it]Time:  4.60245542798657
 91%|█████████ | 91/100 [1:11:40<07:04, 47.14s/it]Time:  4.510092876007548
 92%|█████████▏| 92/100 [1:12:26<06:15, 46.95s/it]Time:  4.505666980985552
 93%|█████████▎| 93/100 [1:13:13<05:28, 46.94s/it]Time:  4.545997186010936
 94%|█████████▍| 94/100 [1:14:00<04:41, 46.88s/it]Time:  4.641362858004868
 95%|█████████▌| 95/100 [1:14:47<03:54, 46.95s/it]Time:  4.655378819996258
 96%|█████████▌| 96/100 [1:15:34<03:07, 46.93s/it]Time:  4.608953138027573
 97%|█████████▋| 97/100 [1:16:21<02:20, 46.95s/it]Time:  4.471906958002364
 98%|█████████▊| 98/100 [1:17:08<01:33, 46.89s/it]Time:  4.620897170010721
 99%|█████████▉| 99/100 [1:17:55<00:46, 46.92s/it]Time:  2.7265423399803694
100%|██████████| 100/100 [1:18:37<00:00, 45.64s/it]100%|██████████| 100/100 [1:18:37<00:00, 47.18s/it]
Post-training time: 4721.74 seconds
GPU memory allocated: 435.85 MB, peak: 1060.06 MB
Train results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy  test_time
Init.      0           NaN   2.417717          11.73          50.17   3.742645
Pre-Prune  0           NaN   2.417717          11.73          50.17   3.742645
Post-Prune 0           NaN   2.326856          10.00          50.00   3.833143
Final      100    0.113065   0.526210          87.51          99.26   2.726542
Prune results:
             module   param  sparsity     size             shape     flops    score mean  score variance  score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.874421     1728     (64, 3, 3, 3)   1769472  2.224966e-06    6.371801e-12   0.003845    2.224966e-06        6.371801e-12       0.003845      True
1    layers.0.conv    bias  1.000000       64             (64,)     65536  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
2    layers.1.conv  weight  0.477268    36864    (64, 64, 3, 3)  37748736  3.802294e-07    3.769638e-13   0.014017    3.802294e-07        3.769638e-13       0.014017      True
3    layers.1.conv    bias  1.000000       64             (64,)     65536  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
4    layers.3.conv  weight  0.378757    73728   (128, 64, 3, 3)  18874368  3.019000e-07    2.926917e-13   0.022258    3.019000e-07        2.926917e-13       0.022258      True
5    layers.3.conv    bias  1.000000      128            (128,)     32768  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
6    layers.4.conv  weight  0.248596   147456  (128, 128, 3, 3)  37748736  1.766793e-07    1.293368e-13   0.026052    1.766793e-07        1.293368e-13       0.026052      True
7    layers.4.conv    bias  1.000000      128            (128,)     32768  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
8    layers.6.conv  weight  0.198564   294912  (256, 128, 3, 3)  18874368  1.371369e-07    8.621578e-14   0.040443    1.371369e-07        8.621578e-14       0.040443      True
9    layers.6.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
10   layers.7.conv  weight  0.136907   589824  (256, 256, 3, 3)  37748736  9.237761e-08    4.426949e-14   0.054487    9.237761e-08        4.426949e-14       0.054487      True
11   layers.7.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
12   layers.8.conv  weight  0.151525   589824  (256, 256, 3, 3)  37748736  9.919984e-08    4.516633e-14   0.058510    9.919984e-08        4.516633e-14       0.058510      True
13   layers.8.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
14  layers.10.conv  weight  0.132495  1179648  (512, 256, 3, 3)  18874368  8.549011e-08    3.122918e-14   0.100848    8.549011e-08        3.122918e-14       0.100848      True
15  layers.10.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
16  layers.11.conv  weight  0.079919  2359296  (512, 512, 3, 3)  37748736  5.522888e-08    1.596758e-14   0.130301    5.522888e-08        1.596758e-14       0.130301      True
17  layers.11.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
18  layers.12.conv  weight  0.081543  2359296  (512, 512, 3, 3)  37748736  5.628561e-08    1.532595e-14   0.132794    5.628561e-08        1.532595e-14       0.132794      True
19  layers.12.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
20  layers.14.conv  weight  0.098683  2359296  (512, 512, 3, 3)   9437184  6.324614e-08    2.105084e-14   0.149216    6.324614e-08        2.105084e-14       0.149216      True
21  layers.14.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
22  layers.15.conv  weight  0.080533  2359296  (512, 512, 3, 3)   9437184  5.294718e-08    2.291931e-14   0.124918    5.294718e-08        2.291931e-14       0.124918      True
23  layers.15.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
24  layers.16.conv  weight  0.082787  2359296  (512, 512, 3, 3)   9437184  5.487940e-08    2.587618e-14   0.129477    5.487940e-08        2.587618e-14       0.129477      True
25  layers.16.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
26              fc  weight  0.744336     5120         (10, 512)      5120  2.506261e-06    1.778373e-11   0.012832    2.506261e-06        1.778373e-11       0.012832      True
27              fc    bias  1.000000       10             (10,)        10  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
Parameter Sparsity: 1475793/14719818 (0.1003)
FLOP Sparsity: 62080857/313478154 (0.1980)
Saving results.
