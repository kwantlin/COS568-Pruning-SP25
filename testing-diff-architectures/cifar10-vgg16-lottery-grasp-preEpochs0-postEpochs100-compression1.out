Namespace(dataset='cifar10', model='vgg16', model_class='lottery', dense_classifier=False, pretrained=False, optimizer='adam', train_batch_size=256, test_batch_size=256, pre_epochs=0, post_epochs=100, lr=0.001, lr_drops=[], lr_drop_rate=0.1, weight_decay=0.0, pruner='grasp', compression=1.0, quantization=False, prune_epochs=1, compression_schedule='exponential', mask_scope='global', prune_dataset_ratio=10, prune_batch_size=256, prune_bias=False, prune_batchnorm=False, prune_residual=False, prune_train_mode=False, reinitialize=False, shuffle=False, invert=False, pruner_list=[], prune_epoch_list=[], compression_list=[], level_list=[], experiment='singleshot', expid='cifar10-vgg16-lottery-grasp-preEpochs0-postEpochs100-compression1', result_dir='Results/data', gpu=0, workers=4, no_cuda=False, seed=1, verbose=False)
Loading cifar10 dataset.
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with grasp for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:35<00:00, 35.65s/it]100%|██████████| 1/1 [00:35<00:00, 35.65s/it]
Post-Training for 100 epochs.
  0%|          | 0/100 [00:00<?, ?it/s]Time:  3.7201610635966063
  1%|          | 1/100 [00:53<1:28:58, 53.92s/it]Time:  4.439390178769827
  2%|▏         | 2/100 [01:48<1:28:16, 54.05s/it]Time:  4.9667830020189285
  3%|▎         | 3/100 [02:40<1:26:09, 53.30s/it]Time:  4.926917614415288
  4%|▍         | 4/100 [03:32<1:24:24, 52.76s/it]Time:  4.986363961361349
  5%|▌         | 5/100 [04:25<1:23:55, 53.01s/it]Time:  4.954188534989953
  6%|▌         | 6/100 [05:18<1:23:05, 53.04s/it]Time:  4.975524111650884
  7%|▋         | 7/100 [06:12<1:22:21, 53.13s/it]Time:  4.965387251228094
  8%|▊         | 8/100 [07:07<1:22:24, 53.75s/it]Time:  4.953088126145303
  9%|▉         | 9/100 [08:02<1:22:03, 54.10s/it]Time:  4.967626791447401
 10%|█         | 10/100 [08:57<1:21:29, 54.32s/it]Time:  4.9949912475422025
 11%|█         | 11/100 [09:51<1:20:32, 54.30s/it]Time:  4.985110288485885
 12%|█▏        | 12/100 [10:45<1:19:43, 54.36s/it]Time:  4.931739104911685
 13%|█▎        | 13/100 [11:39<1:18:39, 54.24s/it]Time:  4.991726024076343
 14%|█▍        | 14/100 [12:34<1:17:53, 54.34s/it]Time:  4.957008463330567
 15%|█▌        | 15/100 [13:27<1:16:37, 54.09s/it]Time:  4.97807605843991
 16%|█▌        | 16/100 [14:22<1:15:54, 54.22s/it]Time:  4.967442424967885
 17%|█▋        | 17/100 [15:16<1:14:54, 54.15s/it]Time:  4.930412356741726
 18%|█▊        | 18/100 [16:10<1:14:03, 54.19s/it]Time:  4.966895665042102
 19%|█▉        | 19/100 [17:04<1:13:04, 54.12s/it]Time:  4.97894518263638
 20%|██        | 20/100 [17:55<1:10:59, 53.24s/it]Time:  4.976217830553651
 21%|██        | 21/100 [18:46<1:09:16, 52.61s/it]Time:  4.974589335732162
 22%|██▏       | 22/100 [19:40<1:08:36, 52.77s/it]Time:  4.980015547014773
 23%|██▎       | 23/100 [20:33<1:08:02, 53.02s/it]Time:  4.982214883901179
 24%|██▍       | 24/100 [21:27<1:07:35, 53.36s/it]Time:  4.980428282171488
 25%|██▌       | 25/100 [22:21<1:06:47, 53.44s/it]Time:  5.001787765882909
 26%|██▌       | 26/100 [23:21<1:08:14, 55.33s/it]Time:  4.804051238112152
 27%|██▋       | 27/100 [24:15<1:07:07, 55.17s/it]Time:  4.8203308349475265
 28%|██▊       | 28/100 [25:12<1:06:46, 55.65s/it]Time:  4.706660348922014
 29%|██▉       | 29/100 [26:05<1:04:48, 54.77s/it]Time:  4.631742015480995
 30%|███       | 30/100 [26:58<1:03:21, 54.31s/it]Time:  4.629913901910186
 31%|███       | 31/100 [27:52<1:02:24, 54.26s/it]Time:  4.541973378509283
 32%|███▏      | 32/100 [28:48<1:01:50, 54.57s/it]Time:  4.746107553131878
 33%|███▎      | 33/100 [29:42<1:00:57, 54.60s/it]Time:  5.177173685282469
 34%|███▍      | 34/100 [30:37<1:00:10, 54.70s/it]Time:  4.9568940456956625
 35%|███▌      | 35/100 [31:32<59:17, 54.74s/it]  Time:  4.984278070740402
 36%|███▌      | 36/100 [32:27<58:20, 54.69s/it]Time:  4.990645341575146
 37%|███▋      | 37/100 [33:21<57:12, 54.48s/it]Time:  4.920513737946749
 38%|███▊      | 38/100 [34:15<56:18, 54.49s/it]Time:  4.96145941875875
 39%|███▉      | 39/100 [35:10<55:36, 54.69s/it]Time:  4.975226813927293
 40%|████      | 40/100 [36:05<54:45, 54.77s/it]Time:  4.948758948594332
 41%|████      | 41/100 [37:00<53:52, 54.78s/it]Time:  4.937432269565761
 42%|████▏     | 42/100 [37:55<52:57, 54.79s/it]Time:  4.931547058746219
 43%|████▎     | 43/100 [38:50<52:08, 54.89s/it]Time:  4.96665264479816
 44%|████▍     | 44/100 [39:45<51:16, 54.94s/it]Time:  4.992473798803985
 45%|████▌     | 45/100 [40:40<50:21, 54.94s/it]Time:  4.949287720955908
 46%|████▌     | 46/100 [41:34<49:18, 54.79s/it]Time:  4.940912424586713
 47%|████▋     | 47/100 [42:29<48:25, 54.82s/it]Time:  4.971536160446703
 48%|████▊     | 48/100 [43:24<47:30, 54.82s/it]Time:  4.920004804618657
 49%|████▉     | 49/100 [44:19<46:33, 54.78s/it]Time:  4.9308361280709505
 50%|█████     | 50/100 [45:14<45:42, 54.85s/it]Time:  4.9476965861395
 51%|█████     | 51/100 [46:09<44:48, 54.86s/it]Time:  4.9485583901405334
 52%|█████▏    | 52/100 [47:04<43:55, 54.90s/it]Time:  4.972589218057692
 53%|█████▎    | 53/100 [47:58<42:56, 54.83s/it]Time:  4.9297716580331326
 54%|█████▍    | 54/100 [48:53<41:59, 54.77s/it]Time:  4.965643682517111
 55%|█████▌    | 55/100 [49:47<41:00, 54.68s/it]Time:  4.934482925571501
 56%|█████▌    | 56/100 [50:42<40:01, 54.58s/it]Time:  4.987581629306078
 57%|█████▋    | 57/100 [51:36<39:07, 54.59s/it]Time:  4.948355853557587
 58%|█████▊    | 58/100 [52:31<38:09, 54.51s/it]Time:  4.973523149266839
 59%|█████▉    | 59/100 [53:26<37:19, 54.62s/it]Time:  4.996978315524757
 60%|██████    | 60/100 [54:21<36:28, 54.70s/it]Time:  4.963358272798359
 61%|██████    | 61/100 [55:15<35:34, 54.74s/it]Time:  4.990613820962608
 62%|██████▏   | 62/100 [56:10<34:39, 54.72s/it]Time:  4.9704731199890375
 63%|██████▎   | 63/100 [57:05<33:46, 54.77s/it]Time:  5.005652896128595
 64%|██████▍   | 64/100 [58:00<32:52, 54.78s/it]Time:  4.966135240159929
 65%|██████▌   | 65/100 [58:55<32:00, 54.88s/it]Time:  4.941091483458877
 66%|██████▌   | 66/100 [59:50<31:06, 54.91s/it]Time:  4.948900113813579
 67%|██████▋   | 67/100 [1:00:45<30:11, 54.88s/it]Time:  4.962633077986538
 68%|██████▊   | 68/100 [1:01:39<29:14, 54.82s/it]Time:  4.938536048866808
 69%|██████▉   | 69/100 [1:02:34<28:17, 54.75s/it]Time:  4.940336944535375
 70%|███████   | 70/100 [1:03:28<27:20, 54.69s/it]Time:  4.945215332321823
 71%|███████   | 71/100 [1:04:23<26:21, 54.54s/it]Time:  4.963698404841125
 72%|███████▏  | 72/100 [1:05:17<25:25, 54.47s/it]Time:  4.966949268244207
 73%|███████▎  | 73/100 [1:06:12<24:33, 54.59s/it]Time:  4.932073813863099
 74%|███████▍  | 74/100 [1:07:07<23:42, 54.71s/it]Time:  4.965025142766535
 75%|███████▌  | 75/100 [1:08:02<22:49, 54.78s/it]Time:  4.972619145177305
 76%|███████▌  | 76/100 [1:08:57<21:55, 54.79s/it]Time:  4.953256160020828
 77%|███████▋  | 77/100 [1:09:51<20:59, 54.77s/it]Time:  4.941406959667802
 78%|███████▊  | 78/100 [1:10:46<20:04, 54.75s/it]Time:  4.9615023992955685
 79%|███████▉  | 79/100 [1:11:41<19:10, 54.78s/it]Time:  4.985555928200483
 80%|████████  | 80/100 [1:12:36<18:18, 54.92s/it]Time:  4.9782760962843895
 81%|████████  | 81/100 [1:13:31<17:22, 54.86s/it]Time:  4.951325955800712
 82%|████████▏ | 82/100 [1:14:25<16:26, 54.82s/it]Time:  4.980074341408908
 83%|████████▎ | 83/100 [1:15:21<15:33, 54.93s/it]Time:  4.949142369441688
 84%|████████▍ | 84/100 [1:16:16<14:39, 54.98s/it]Time:  4.965725843794644
 85%|████████▌ | 85/100 [1:17:11<13:44, 54.98s/it]Time:  4.895764508284628
 86%|████████▌ | 86/100 [1:18:06<12:49, 54.97s/it]Time:  4.9580060029402375
 87%|████████▋ | 87/100 [1:19:01<11:55, 55.02s/it]Time:  4.985573483631015
 88%|████████▊ | 88/100 [1:19:56<10:59, 54.98s/it]Time:  4.975097177550197
 89%|████████▉ | 89/100 [1:20:51<10:05, 55.01s/it]Time:  4.987684075720608
 90%|█████████ | 90/100 [1:21:46<09:09, 54.96s/it]Time:  4.982286768965423
 91%|█████████ | 91/100 [1:22:41<08:15, 55.01s/it]Time:  4.929615321569145
 92%|█████████▏| 92/100 [1:23:35<07:18, 54.87s/it]Time:  4.959516489878297
 93%|█████████▎| 93/100 [1:24:30<06:24, 54.94s/it]Time:  4.959295466542244
 94%|█████████▍| 94/100 [1:25:25<05:29, 54.86s/it]Time:  4.945712294429541
 95%|█████████▌| 95/100 [1:26:20<04:34, 54.80s/it]Time:  4.961849985644221
 96%|█████████▌| 96/100 [1:27:15<03:39, 54.84s/it]Time:  4.966062172316015
 97%|█████████▋| 97/100 [1:28:09<02:44, 54.79s/it]Time:  4.98155452311039
 98%|█████████▊| 98/100 [1:29:05<01:49, 54.91s/it]Time:  4.982452921569347
 99%|█████████▉| 99/100 [1:30:00<00:54, 54.93s/it]Time:  4.126085709780455
100%|██████████| 100/100 [1:30:46<00:00, 52.39s/it]100%|██████████| 100/100 [1:30:46<00:00, 54.47s/it]
Post-training time: 5450.42 seconds
GPU memory allocated: 374.76 MB, peak: 999.48 MB
Train results:
                 train_loss     test_loss  top1_accuracy  top5_accuracy   test_time
Init.      0           NaN  2.417717e+00          11.73          50.17  126.375624
Pre-Prune  0           NaN  2.417717e+00          11.73          50.17  126.375624
Post-Prune 0           NaN  7.414986e+09          10.00          49.98    3.910174
Final      100    2.295325  2.226940e+00          21.02          69.77    4.126086
Prune results:
             module   param  sparsity     size             shape     flops    score mean  score variance  score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.532986     1728     (64, 3, 3, 3)   1769472  4.450376e-05    8.434830e-08   0.076903    1.499241e-04        6.385165e-08       0.259069      True
1    layers.0.conv    bias  1.000000       64             (64,)     65536  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
2    layers.1.conv  weight  0.369385    36864    (64, 64, 3, 3)  37748736  2.075447e-06    4.122780e-09   0.076509    2.485490e-05        3.509322e-09       0.916251      True
3    layers.1.conv    bias  1.000000       64             (64,)     65536  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
4    layers.3.conv  weight  0.301229    73728   (128, 64, 3, 3)  18874368  1.036265e-06    1.686676e-09   0.076402    1.571875e-05        1.440670e-09       1.158912      True
5    layers.3.conv    bias  1.000000      128            (128,)     32768  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
6    layers.4.conv  weight  0.218770   147456  (128, 128, 3, 3)  37748736  5.169903e-07    5.230479e-10   0.076233    7.786414e-06        4.626869e-10       1.148154      True
7    layers.4.conv    bias  1.000000      128            (128,)     32768  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
8    layers.6.conv  weight  0.185632   294912  (256, 128, 3, 3)  18874368  2.581017e-07    2.053401e-10   0.076117    4.942223e-06        1.809812e-10       1.457521      True
9    layers.6.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
10   layers.7.conv  weight  0.142064   589824  (256, 256, 3, 3)  37748736  1.255479e-07    5.263315e-11   0.074051    2.530798e-06        4.624396e-11       1.492725      True
11   layers.7.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
12   layers.8.conv  weight  0.148042   589824  (256, 256, 3, 3)  37748736  1.240259e-07    3.422636e-11   0.073153    2.217017e-06        2.932658e-11       1.307650      True
13   layers.8.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
14  layers.10.conv  weight  0.127266  1179648  (512, 256, 3, 3)  18874368  6.157173e-08    1.396250e-11   0.072633    1.547428e-06        1.157176e-11       1.825420      True
15  layers.10.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
16  layers.11.conv  weight  0.082909  2359296  (512, 512, 3, 3)  37748736  2.846882e-08    4.243502e-12   0.067166    8.836322e-07        3.463506e-12       2.084750      True
17  layers.11.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
18  layers.12.conv  weight  0.088690  2359296  (512, 512, 3, 3)  37748736  2.840790e-08    3.657215e-12   0.067023    8.970009e-07        2.853413e-12       2.116291      True
19  layers.12.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
20  layers.14.conv  weight  0.087905  2359296  (512, 512, 3, 3)   9437184  2.694331e-08    3.812699e-12   0.063567    8.725643e-07        3.052056e-12       2.058637      True
21  layers.14.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
22  layers.15.conv  weight  0.083046  2359296  (512, 512, 3, 3)   9437184  2.711094e-08    4.234989e-12   0.063963    8.358491e-07        3.537080e-12       1.972015      True
23  layers.15.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
24  layers.16.conv  weight  0.091467  2359296  (512, 512, 3, 3)   9437184  2.642921e-08    5.443440e-12   0.062354    9.434240e-07        4.554088e-12       2.225816      True
25  layers.16.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
26              fc  weight  0.506055     5120         (10, 512)      5120  1.443846e-05    5.200128e-09   0.073925    3.773055e-05        3.985003e-09       0.193180      True
27              fc    bias  1.000000       10             (10,)        10  0.000000e+00    0.000000e+00   0.000000    0.000000e+00        0.000000e+00       0.000000     False
Parameter Sparsity: 1475793/14719818 (0.1003)
FLOP Sparsity: 54920876/313478154 (0.1752)
Saving results.
