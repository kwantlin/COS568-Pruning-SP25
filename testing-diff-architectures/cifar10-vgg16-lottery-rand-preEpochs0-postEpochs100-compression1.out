Namespace(dataset='cifar10', model='vgg16', model_class='lottery', dense_classifier=False, pretrained=False, optimizer='adam', train_batch_size=256, test_batch_size=256, pre_epochs=0, post_epochs=100, lr=0.001, lr_drops=[], lr_drop_rate=0.1, weight_decay=0.0, pruner='rand', compression=1.0, quantization=False, prune_epochs=1, compression_schedule='exponential', mask_scope='global', prune_dataset_ratio=10, prune_batch_size=256, prune_bias=False, prune_batchnorm=False, prune_residual=False, prune_train_mode=False, reinitialize=False, shuffle=False, invert=False, pruner_list=[], prune_epoch_list=[], compression_list=[], level_list=[], experiment='singleshot', expid='cifar10-vgg16-lottery-rand-preEpochs0-postEpochs100-compression1', result_dir='Results/data', gpu=0, workers=4, no_cuda=False, seed=1, verbose=False)
Loading cifar10 dataset.
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with rand for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  8.37it/s]100%|██████████| 1/1 [00:00<00:00,  8.36it/s]
Post-Training for 100 epochs.
  0%|          | 0/100 [00:00<?, ?it/s]Time:  3.8730604872107506
  1%|          | 1/100 [00:55<1:31:58, 55.75s/it]Time:  3.9020220525562763
  2%|▏         | 2/100 [01:50<1:30:00, 55.11s/it]Time:  3.7415781849995255
  3%|▎         | 3/100 [02:45<1:29:08, 55.14s/it]Time:  3.1762008434161544
  4%|▍         | 4/100 [03:40<1:27:49, 54.89s/it]Time:  4.300079293549061
  5%|▌         | 5/100 [04:33<1:26:18, 54.51s/it]Time:  4.5555911073461175
  6%|▌         | 6/100 [05:27<1:25:03, 54.30s/it]Time:  4.780886381864548
  7%|▋         | 7/100 [06:21<1:23:46, 54.05s/it]Time:  4.736993093043566
  8%|▊         | 8/100 [07:16<1:23:16, 54.31s/it]Time:  4.779255487024784
  9%|▉         | 9/100 [08:09<1:22:01, 54.08s/it]Time:  4.752662996761501
 10%|█         | 10/100 [09:03<1:21:00, 54.00s/it]Time:  4.764964704401791
 11%|█         | 11/100 [09:56<1:19:22, 53.52s/it]Time:  4.745303459465504
 12%|█▏        | 12/100 [10:48<1:18:13, 53.33s/it]Time:  4.7875360902398825
 13%|█▎        | 13/100 [11:42<1:17:32, 53.47s/it]Time:  4.724265598692
 14%|█▍        | 14/100 [12:35<1:16:26, 53.33s/it]Time:  4.760848900303245
 15%|█▌        | 15/100 [13:25<1:13:49, 52.11s/it]Time:  4.69636205304414
 16%|█▌        | 16/100 [14:18<1:13:32, 52.53s/it]Time:  4.687138766050339
 17%|█▋        | 17/100 [15:12<1:13:13, 52.94s/it]Time:  4.717175262980163
 18%|█▊        | 18/100 [16:05<1:12:27, 53.02s/it]Time:  4.703515904955566
 19%|█▉        | 19/100 [17:00<1:12:15, 53.52s/it]Time:  4.712038806639612
 20%|██        | 20/100 [17:54<1:11:29, 53.62s/it]Time:  4.704081603325903
 21%|██        | 21/100 [18:48<1:10:45, 53.74s/it]Time:  4.653931411914527
 22%|██▏       | 22/100 [19:41<1:09:45, 53.67s/it]Time:  4.692752065137029
 23%|██▎       | 23/100 [20:35<1:08:49, 53.63s/it]Time:  4.718910877592862
 24%|██▍       | 24/100 [21:28<1:07:55, 53.63s/it]Time:  4.699088015593588
 25%|██▌       | 25/100 [22:21<1:06:38, 53.31s/it]Time:  4.726071088574827
 26%|██▌       | 26/100 [23:16<1:06:25, 53.86s/it]Time:  4.748464792966843
 27%|██▋       | 27/100 [24:05<1:03:49, 52.46s/it]Time:  4.747358225286007
 28%|██▊       | 28/100 [24:56<1:02:21, 51.97s/it]Time:  3.962922249920666
 29%|██▉       | 29/100 [25:47<1:01:05, 51.62s/it]Time:  3.6710232477635145
 30%|███       | 30/100 [26:42<1:01:25, 52.65s/it]Time:  4.826251042075455
 31%|███       | 31/100 [27:36<1:01:03, 53.09s/it]Time:  4.689554633572698
 32%|███▏      | 32/100 [28:30<1:00:24, 53.29s/it]Time:  4.659398261457682
 33%|███▎      | 33/100 [29:24<59:42, 53.46s/it]  Time:  4.7735685762017965
 34%|███▍      | 34/100 [30:18<59:01, 53.65s/it]Time:  4.442823871970177
 35%|███▌      | 35/100 [31:11<58:02, 53.58s/it]Time:  4.529932094737887
 36%|███▌      | 36/100 [32:05<57:17, 53.72s/it]Time:  4.498223348520696
 37%|███▋      | 37/100 [32:58<56:10, 53.50s/it]Time:  4.567683272063732
 38%|███▊      | 38/100 [33:53<55:32, 53.74s/it]Time:  4.39297251123935
 39%|███▉      | 39/100 [34:47<54:46, 53.88s/it]Time:  4.5435271840542555
 40%|████      | 40/100 [35:41<54:00, 54.01s/it]Time:  4.5068193366751075
 41%|████      | 41/100 [36:35<52:58, 53.87s/it]Time:  4.351085781119764
 42%|████▏     | 42/100 [37:29<52:11, 53.99s/it]Time:  4.4558568531647325
 43%|████▎     | 43/100 [38:23<51:23, 54.10s/it]Time:  4.387793976813555
 44%|████▍     | 44/100 [39:17<50:27, 54.07s/it]Time:  4.322967364452779
 45%|████▌     | 45/100 [40:12<49:38, 54.15s/it]Time:  4.484599623829126
 46%|████▌     | 46/100 [41:06<48:49, 54.24s/it]Time:  4.428821872919798
 47%|████▋     | 47/100 [42:00<47:45, 54.07s/it]Time:  4.421211996115744
 48%|████▊     | 48/100 [42:54<46:54, 54.13s/it]Time:  4.505244451574981
 49%|████▉     | 49/100 [43:48<46:02, 54.16s/it]Time:  4.432475463487208
 50%|█████     | 50/100 [44:42<45:06, 54.13s/it]Time:  4.43907893076539
 51%|█████     | 51/100 [45:37<44:14, 54.18s/it]Time:  4.559371716342866
 52%|█████▏    | 52/100 [46:31<43:22, 54.22s/it]Time:  4.445485467091203
 53%|█████▎    | 53/100 [47:25<42:20, 54.06s/it]Time:  4.577561824582517
 54%|█████▍    | 54/100 [48:19<41:30, 54.14s/it]Time:  4.440190531313419
 55%|█████▌    | 55/100 [49:13<40:33, 54.07s/it]Time:  4.498892396688461
 56%|█████▌    | 56/100 [50:07<39:35, 53.99s/it]Time:  4.4959750808775425
 57%|█████▋    | 57/100 [51:01<38:47, 54.12s/it]Time:  4.524462084285915
 58%|█████▊    | 58/100 [51:55<37:49, 54.04s/it]Time:  4.513883606530726
 59%|█████▉    | 59/100 [52:49<36:57, 54.10s/it]Time:  4.4545985562726855
 60%|██████    | 60/100 [53:43<36:03, 54.09s/it]Time:  4.46974618639797
 61%|██████    | 61/100 [54:38<35:13, 54.19s/it]Time:  4.5921654822304845
 62%|██████▏   | 62/100 [55:32<34:18, 54.18s/it]Time:  4.528709026053548
 63%|██████▎   | 63/100 [56:26<33:24, 54.17s/it]Time:  4.500542417168617
 64%|██████▍   | 64/100 [57:20<32:31, 54.22s/it]Time:  4.435575684532523
 65%|██████▌   | 65/100 [58:14<31:34, 54.13s/it]Time:  4.431585599668324
 66%|██████▌   | 66/100 [59:08<30:37, 54.03s/it]Time:  4.401190126314759
 67%|██████▋   | 67/100 [1:00:01<29:37, 53.87s/it]Time:  4.437002458609641
 68%|██████▊   | 68/100 [1:00:55<28:45, 53.91s/it]Time:  4.515700068324804
 69%|██████▉   | 69/100 [1:01:49<27:50, 53.89s/it]Time:  4.312693606130779
 70%|███████   | 70/100 [1:02:43<26:57, 53.90s/it]Time:  4.521755958907306
 71%|███████   | 71/100 [1:03:38<26:06, 54.02s/it]Time:  4.353620193898678
 72%|███████▏  | 72/100 [1:04:32<25:15, 54.12s/it]Time:  4.358047912828624
 73%|███████▎  | 73/100 [1:05:26<24:22, 54.18s/it]Time:  4.474389279261231
 74%|███████▍  | 74/100 [1:06:20<23:29, 54.20s/it]Time:  4.460979668423533
 75%|███████▌  | 75/100 [1:07:14<22:33, 54.16s/it]Time:  4.389620395377278
 76%|███████▌  | 76/100 [1:08:09<21:38, 54.12s/it]Time:  4.342929153703153
 77%|███████▋  | 77/100 [1:09:03<20:47, 54.22s/it]Time:  4.437332012690604
 78%|███████▊  | 78/100 [1:09:57<19:53, 54.26s/it]Time:  4.360673732124269
 79%|███████▉  | 79/100 [1:10:52<18:58, 54.24s/it]Time:  4.391693168319762
 80%|████████  | 80/100 [1:11:46<18:04, 54.23s/it]Time:  4.451308626681566
 81%|████████  | 81/100 [1:12:40<17:09, 54.19s/it]Time:  4.343127818778157
 82%|████████▏ | 82/100 [1:13:34<16:13, 54.11s/it]Time:  4.333371226675808
 83%|████████▎ | 83/100 [1:14:28<15:22, 54.25s/it]Time:  4.374779331497848
 84%|████████▍ | 84/100 [1:15:23<14:27, 54.23s/it]Time:  4.478591036051512
 85%|████████▌ | 85/100 [1:16:17<13:34, 54.33s/it]Time:  4.479451154358685
 86%|████████▌ | 86/100 [1:17:11<12:39, 54.27s/it]Time:  4.5108950920403
 87%|████████▋ | 87/100 [1:18:06<11:46, 54.34s/it]Time:  4.520229329355061
 88%|████████▊ | 88/100 [1:19:00<10:51, 54.27s/it]Time:  4.366345979273319
 89%|████████▉ | 89/100 [1:19:54<09:56, 54.23s/it]Time:  4.435061424039304
 90%|█████████ | 90/100 [1:20:48<09:02, 54.24s/it]Time:  4.551123371347785
 91%|█████████ | 91/100 [1:21:42<08:08, 54.24s/it]Time:  4.55129614751786
 92%|█████████▏| 92/100 [1:22:37<07:13, 54.24s/it]Time:  4.2813388761132956
 93%|█████████▎| 93/100 [1:23:31<06:19, 54.25s/it]Time:  4.4983914801850915
 94%|█████████▍| 94/100 [1:24:25<05:25, 54.20s/it]Time:  4.569239944219589
 95%|█████████▌| 95/100 [1:25:20<04:31, 54.35s/it]Time:  4.563910119235516
 96%|█████████▌| 96/100 [1:26:14<03:36, 54.25s/it]Time:  4.634501378983259
 97%|█████████▋| 97/100 [1:27:08<02:42, 54.20s/it]Time:  4.595551148056984
 98%|█████████▊| 98/100 [1:28:02<01:48, 54.11s/it]Time:  4.62049413099885
 99%|█████████▉| 99/100 [1:28:56<00:54, 54.14s/it]Time:  3.7680972889065742
100%|██████████| 100/100 [1:29:43<00:00, 52.10s/it]100%|██████████| 100/100 [1:29:43<00:00, 53.84s/it]
Post-training time: 5384.74 seconds
GPU memory allocated: 375.42 MB, peak: 999.49 MB
Train results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy  test_time
Init.      0           NaN   2.417717          11.73          50.17   0.969520
Pre-Prune  0           NaN   2.417717          11.73          50.17   0.969520
Post-Prune 0           NaN   2.302585           9.99          50.04   0.912640
Final      100    2.302679   2.302588          10.00          50.00   3.768097
Prune results:
             module   param  sparsity     size             shape     flops  score mean  score variance    score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.100694     1728     (64, 3, 3, 3)   1769472   -0.027050        1.005513   -46.742134        0.805873            0.356814   1.392548e+03      True
1    layers.0.conv    bias  1.000000       64             (64,)     65536    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
2    layers.1.conv  weight  0.100423    36864    (64, 64, 3, 3)  37748736    0.000872        0.996963    32.139240        0.796522            0.362516   2.936300e+04      True
3    layers.1.conv    bias  1.000000       64             (64,)     65536    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
4    layers.3.conv  weight  0.099501    73728   (128, 64, 3, 3)  18874368    0.000958        0.993901    70.645149        0.796130            0.360079   5.869704e+04      True
5    layers.3.conv    bias  1.000000      128            (128,)     32768    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
6    layers.4.conv  weight  0.099413   147456  (128, 128, 3, 3)  37748736    0.001246        0.994699   183.733383        0.795026            0.362635   1.172313e+05      True
7    layers.4.conv    bias  1.000000      128            (128,)     32768    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
8    layers.6.conv  weight  0.099294   294912  (256, 128, 3, 3)  18874368   -0.001563        1.001676  -460.913849        0.798645            0.363844   2.355301e+05      True
9    layers.6.conv    bias  1.000000      256            (256,)     16384    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
10   layers.7.conv  weight  0.099864   589824  (256, 256, 3, 3)  37748736    0.000378        0.999599   222.715530        0.797776            0.363152   4.705474e+05      True
11   layers.7.conv    bias  1.000000      256            (256,)     16384    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
12   layers.8.conv  weight  0.099543   589824  (256, 256, 3, 3)  37748736    0.000403        0.998166   237.463287        0.796769            0.363324   4.699538e+05      True
13   layers.8.conv    bias  1.000000      256            (256,)     16384    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
14  layers.10.conv  weight  0.099947  1179648  (512, 256, 3, 3)  18874368    0.000511        0.999746   602.936768        0.797616            0.363555   9.409056e+05      True
15  layers.10.conv    bias  1.000000      512            (512,)      8192    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
16  layers.11.conv  weight  0.100168  2359296  (512, 512, 3, 3)  37748736   -0.000190        1.001348  -448.645966        0.798652            0.363503   1.884256e+06      True
17  layers.11.conv    bias  1.000000      512            (512,)      8192    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
18  layers.12.conv  weight  0.099864  2359296  (512, 512, 3, 3)  37748736    0.000249        0.999722   588.484009        0.797592            0.363569   1.881756e+06      True
19  layers.12.conv    bias  1.000000      512            (512,)      8192    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
20  layers.14.conv  weight  0.100023  2359296  (512, 512, 3, 3)   9437184   -0.000339        0.999254  -799.927002        0.797659            0.362994   1.881914e+06      True
21  layers.14.conv    bias  1.000000      512            (512,)      2048    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
22  layers.15.conv  weight  0.100098  2359296  (512, 512, 3, 3)   9437184    0.000708        1.000298  1670.380615        0.797886            0.363677   1.882448e+06      True
23  layers.15.conv    bias  1.000000      512            (512,)      2048    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
24  layers.16.conv  weight  0.100165  2359296  (512, 512, 3, 3)   9437184    0.000980        0.999300  2311.431885        0.797918            0.362629   1.882524e+06      True
25  layers.16.conv    bias  1.000000      512            (512,)      2048    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
26              fc  weight  0.096289     5120         (10, 512)      5120    0.007839        0.966788    40.134628        0.782462            0.354602   4.006208e+03      True
27              fc    bias  1.000000       10             (10,)        10    0.000000        0.000000     0.000000        0.000000            0.000000   0.000000e+00     False
Parameter Sparsity: 1475793/14719818 (0.1003)
FLOP Sparsity: 31549407/313478154 (0.1006)
Saving results.
