Namespace(dataset='cifar10', model='vgg16', model_class='lottery', dense_classifier=False, pretrained=False, optimizer='adam', train_batch_size=256, test_batch_size=256, pre_epochs=0, post_epochs=100, lr=0.001, lr_drops=[], lr_drop_rate=0.1, weight_decay=0.0, pruner='synflow', compression=1.0, quantization=False, prune_epochs=1, compression_schedule='exponential', mask_scope='global', prune_dataset_ratio=10, prune_batch_size=256, prune_bias=False, prune_batchnorm=False, prune_residual=False, prune_train_mode=False, reinitialize=False, shuffle=False, invert=False, pruner_list=[], prune_epoch_list=[], compression_list=[], level_list=[], experiment='singleshot', expid='cifar10-vgg16-lottery-synflow-preEpochs-postEpochs100-compression1', result_dir='Results/data', gpu=0, workers=4, no_cuda=False, seed=1, verbose=False)
Loading cifar10 dataset.
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with synflow for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  1.61it/s]100%|██████████| 1/1 [00:00<00:00,  1.61it/s]
Post-Training for 100 epochs.
  0%|          | 0/100 [00:00<?, ?it/s]Time:  3.6601515189977363
  1%|          | 1/100 [00:47<1:18:42, 47.70s/it]Time:  3.7191723870055284
  2%|▏         | 2/100 [01:35<1:17:41, 47.57s/it]Time:  3.6752107339852955
  3%|▎         | 3/100 [02:22<1:16:40, 47.43s/it]Time:  3.867338420008309
  4%|▍         | 4/100 [03:10<1:15:59, 47.50s/it]Time:  3.6763195249950513
  5%|▌         | 5/100 [03:57<1:15:26, 47.65s/it]Time:  3.749325949989725
  6%|▌         | 6/100 [04:45<1:14:40, 47.66s/it]Time:  3.8922348310006782
  7%|▋         | 7/100 [05:33<1:13:47, 47.61s/it]Time:  4.140315287018893
  8%|▊         | 8/100 [06:20<1:12:58, 47.59s/it]Time:  4.477654454996809
  9%|▉         | 9/100 [07:08<1:12:28, 47.79s/it]Time:  4.279248653008835
 10%|█         | 10/100 [07:56<1:11:46, 47.85s/it]Time:  4.181026242004009
 11%|█         | 11/100 [08:44<1:10:55, 47.81s/it]Time:  4.319533080997644
 12%|█▏        | 12/100 [09:32<1:10:08, 47.82s/it]Time:  4.216265130002284
 13%|█▎        | 13/100 [10:19<1:09:08, 47.68s/it]Time:  4.504995962022804
 14%|█▍        | 14/100 [11:07<1:08:24, 47.73s/it]Time:  4.327378172980389
 15%|█▌        | 15/100 [11:55<1:07:31, 47.67s/it]Time:  4.287037761998363
 16%|█▌        | 16/100 [12:42<1:06:43, 47.67s/it]Time:  4.332746175001375
 17%|█▋        | 17/100 [13:30<1:05:58, 47.69s/it]Time:  4.4061383140215185
 18%|█▊        | 18/100 [14:18<1:05:07, 47.66s/it]Time:  4.400112558010733
 19%|█▉        | 19/100 [15:05<1:04:22, 47.68s/it]Time:  4.608763355994597
 20%|██        | 20/100 [15:53<1:03:26, 47.59s/it]Time:  4.3567998459911905
 21%|██        | 21/100 [16:41<1:02:48, 47.71s/it]Time:  4.387037111009704
 22%|██▏       | 22/100 [17:29<1:02:03, 47.74s/it]Time:  4.378024010016816
 23%|██▎       | 23/100 [18:16<1:01:01, 47.55s/it]Time:  4.274892080022255
 24%|██▍       | 24/100 [19:03<1:00:18, 47.61s/it]Time:  4.451201373012736
 25%|██▌       | 25/100 [19:51<59:26, 47.55s/it]  Time:  4.498362120997626
 26%|██▌       | 26/100 [20:38<58:39, 47.57s/it]Time:  4.4476910950033925
 27%|██▋       | 27/100 [21:26<57:43, 47.45s/it]Time:  4.467349198996089
 28%|██▊       | 28/100 [22:13<56:55, 47.44s/it]Time:  4.6437306689913385
 29%|██▉       | 29/100 [23:00<55:59, 47.32s/it]Time:  4.504997595009627
 30%|███       | 30/100 [23:47<55:10, 47.29s/it]Time:  4.609880728006829
 31%|███       | 31/100 [24:35<54:25, 47.33s/it]Time:  4.620884831994772
 32%|███▏      | 32/100 [25:22<53:29, 47.19s/it]Time:  4.574370841990458
 33%|███▎      | 33/100 [26:08<52:26, 46.96s/it]Time:  4.514852298976621
 34%|███▍      | 34/100 [26:55<51:32, 46.85s/it]Time:  4.658024210977601
 35%|███▌      | 35/100 [27:41<50:40, 46.78s/it]Time:  4.553922552004224
 36%|███▌      | 36/100 [28:28<49:54, 46.79s/it]Time:  4.617856354016112
 37%|███▋      | 37/100 [29:15<49:07, 46.79s/it]Time:  4.595996549993288
 38%|███▊      | 38/100 [30:01<48:12, 46.65s/it]Time:  4.554434107994894
 39%|███▉      | 39/100 [30:48<47:29, 46.72s/it]Time:  4.571251749992371
 40%|████      | 40/100 [31:35<46:39, 46.65s/it]Time:  4.629717899020761
 41%|████      | 41/100 [32:22<46:02, 46.82s/it]Time:  4.6174211439793
 42%|████▏     | 42/100 [33:09<45:20, 46.91s/it]Time:  4.594262622005772
 43%|████▎     | 43/100 [33:56<44:41, 47.04s/it]Time:  4.640357794007286
 44%|████▍     | 44/100 [34:43<43:50, 46.97s/it]Time:  4.577683231997071
 45%|████▌     | 45/100 [35:30<43:04, 46.99s/it]Time:  4.572776865999913
 46%|████▌     | 46/100 [36:17<42:20, 47.06s/it]Time:  4.576131801994052
 47%|████▋     | 47/100 [37:05<41:36, 47.11s/it]Time:  4.612871119985357
 48%|████▊     | 48/100 [37:52<40:50, 47.13s/it]Time:  4.564929023996228
 49%|████▉     | 49/100 [38:39<40:02, 47.12s/it]Time:  4.56111812000745
 50%|█████     | 50/100 [39:26<39:11, 47.03s/it]Time:  4.585062393016415
 51%|█████     | 51/100 [40:13<38:22, 46.99s/it]Time:  4.545066785009112
 52%|█████▏    | 52/100 [40:59<37:35, 46.99s/it]Time:  4.617051637993427
 53%|█████▎    | 53/100 [41:47<36:49, 47.02s/it]Time:  4.589739266986726
 54%|█████▍    | 54/100 [42:33<35:57, 46.91s/it]Time:  4.486828263994539
 55%|█████▌    | 55/100 [43:20<35:09, 46.88s/it]Time:  4.5186782750242855
 56%|█████▌    | 56/100 [44:07<34:18, 46.77s/it]Time:  4.620060867018765
 57%|█████▋    | 57/100 [44:54<33:36, 46.90s/it]Time:  4.610832140024286
 58%|█████▊    | 58/100 [45:41<32:56, 47.05s/it]Time:  4.465434236975852
 59%|█████▉    | 59/100 [46:28<32:03, 46.92s/it]Time:  4.634069841005839
 60%|██████    | 60/100 [47:15<31:25, 47.14s/it]Time:  4.585178930981783
 61%|██████    | 61/100 [48:02<30:34, 47.04s/it]Time:  4.475867533998098
 62%|██████▏   | 62/100 [48:49<29:48, 47.07s/it]Time:  4.538845290982863
 63%|██████▎   | 63/100 [49:36<28:55, 46.90s/it]Time:  4.507666475983569
 64%|██████▍   | 64/100 [50:23<28:07, 46.87s/it]Time:  4.679856357019162
 65%|██████▌   | 65/100 [51:10<27:22, 46.93s/it]Time:  4.49523230700288
 66%|██████▌   | 66/100 [51:56<26:30, 46.77s/it]Time:  4.663875342987012
 67%|██████▋   | 67/100 [52:43<25:46, 46.87s/it]Time:  4.570447205973323
 68%|██████▊   | 68/100 [53:31<25:03, 46.99s/it]Time:  4.628254584007664
 69%|██████▉   | 69/100 [54:18<24:19, 47.08s/it]Time:  4.5859897379996255
 70%|███████   | 70/100 [55:05<23:32, 47.08s/it]Time:  4.5205392979842145
 71%|███████   | 71/100 [55:52<22:42, 46.99s/it]Time:  4.5278937649854925
 72%|███████▏  | 72/100 [56:38<21:51, 46.84s/it]Time:  4.619045579980593
 73%|███████▎  | 73/100 [57:25<21:03, 46.80s/it]Time:  4.566771559009794
 74%|███████▍  | 74/100 [58:12<20:15, 46.76s/it]Time:  4.504105297004571
 75%|███████▌  | 75/100 [58:58<19:28, 46.73s/it]Time:  4.612628381990362
 76%|███████▌  | 76/100 [59:45<18:41, 46.74s/it]Time:  4.530613963026553
 77%|███████▋  | 77/100 [1:00:32<17:55, 46.76s/it]Time:  4.471128120989306
 78%|███████▊  | 78/100 [1:01:19<17:08, 46.77s/it]Time:  4.64132923600846
 79%|███████▉  | 79/100 [1:02:06<16:25, 46.93s/it]Time:  4.583632433001185
 80%|████████  | 80/100 [1:02:53<15:39, 46.96s/it]Time:  4.627490282000508
 81%|████████  | 81/100 [1:03:40<14:52, 46.98s/it]Time:  4.503631459985627
 82%|████████▏ | 82/100 [1:04:27<14:05, 46.97s/it]Time:  4.695442613010528
 83%|████████▎ | 83/100 [1:05:14<13:19, 47.03s/it]Time:  4.601481657999102
 84%|████████▍ | 84/100 [1:06:01<12:32, 47.04s/it]Time:  4.636426297016442
 85%|████████▌ | 85/100 [1:06:48<11:46, 47.12s/it]Time:  4.719697633001488
 86%|████████▌ | 86/100 [1:07:35<10:59, 47.09s/it]Time:  4.663220243994147
 87%|████████▋ | 87/100 [1:08:23<10:13, 47.20s/it]Time:  4.586787185980938
 88%|████████▊ | 88/100 [1:09:10<09:25, 47.16s/it]Time:  4.536481996008661
 89%|████████▉ | 89/100 [1:09:57<08:38, 47.14s/it]Time:  4.53290582698537
 90%|█████████ | 90/100 [1:10:44<07:51, 47.12s/it]Time:  4.502230222977232
 91%|█████████ | 91/100 [1:11:31<07:03, 47.01s/it]Time:  4.5939482059911825
 92%|█████████▏| 92/100 [1:12:18<06:15, 46.99s/it]Time:  4.619189554010518
 93%|█████████▎| 93/100 [1:13:04<05:28, 46.89s/it]Time:  4.613693120016251
 94%|█████████▍| 94/100 [1:13:52<04:42, 47.02s/it]Time:  4.515414471999975
 95%|█████████▌| 95/100 [1:14:39<03:54, 46.98s/it]Time:  4.543049391999375
 96%|█████████▌| 96/100 [1:15:25<03:07, 46.87s/it]Time:  4.6349794839916285
 97%|█████████▋| 97/100 [1:16:12<02:20, 46.91s/it]Time:  4.620135173987364
 98%|█████████▊| 98/100 [1:16:59<01:33, 46.83s/it]Time:  4.524606357008452
 99%|█████████▉| 99/100 [1:17:46<00:46, 46.87s/it]Time:  3.724905143986689
100%|██████████| 100/100 [1:18:32<00:00, 46.57s/it]100%|██████████| 100/100 [1:18:32<00:00, 47.12s/it]
Post-training time: 4715.84 seconds
GPU memory allocated: 375.68 MB, peak: 999.49 MB
/n/fs/klips/anaconda3/envs/cos568/lib/python3.10/site-packages/numpy/_core/_methods.py:197: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/n/fs/klips/anaconda3/envs/cos568/lib/python3.10/site-packages/numpy/_core/_methods.py:208: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
Train results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy  test_time
Init.      0           NaN   2.417717          11.73          50.17   3.844990
Pre-Prune  0           NaN   2.417717          11.73          50.17   3.844990
Post-Prune 0           NaN   2.302601          10.00          49.95   3.520166
Final      100    0.046358   0.636345          88.20          99.28   3.724905
Prune results:
             module   param  sparsity     size             shape     flops    score mean  score variance     score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.997106     1728     (64, 3, 3, 3)   1769472  1.704298e+19             inf  2.945027e+22    1.704298e+19                 inf   2.945027e+22      True
1    layers.0.conv    bias  1.000000       64             (64,)     65536  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
2    layers.1.conv  weight  0.964790    36864    (64, 64, 3, 3)  37748736  7.988898e+17             inf  2.945027e+22    7.988898e+17                 inf   2.945027e+22      True
3    layers.1.conv    bias  1.000000       64             (64,)     65536  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
4    layers.3.conv  weight  0.928589    73728   (128, 64, 3, 3)  18874368  3.994448e+17             inf  2.945027e+22    3.994448e+17                 inf   2.945027e+22      True
5    layers.3.conv    bias  1.000000      128            (128,)     32768  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
6    layers.4.conv  weight  0.859741   147456  (128, 128, 3, 3)  37748736  1.997224e+17             inf  2.945027e+22    1.997224e+17                 inf   2.945027e+22      True
7    layers.4.conv    bias  1.000000      128            (128,)     32768  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
8    layers.6.conv  weight  0.726661   294912  (256, 128, 3, 3)  18874368  9.986122e+16             inf  2.945027e+22    9.986122e+16                 inf   2.945027e+22      True
9    layers.6.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
10   layers.7.conv  weight  0.484929   589824  (256, 256, 3, 3)  37748736  4.993060e+16             inf  2.945027e+22    4.993060e+16                 inf   2.945027e+22      True
11   layers.7.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
12   layers.8.conv  weight  0.484745   589824  (256, 256, 3, 3)  37748736  4.993061e+16             inf  2.945027e+22    4.993061e+16                 inf   2.945027e+22      True
13   layers.8.conv    bias  1.000000      256            (256,)     16384  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
14  layers.10.conv  weight  0.162064  1179648  (512, 256, 3, 3)  18874368  2.496530e+16             inf  2.945027e+22    2.496530e+16                 inf   2.945027e+22      True
15  layers.10.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
16  layers.11.conv  weight  0.007960  2359296  (512, 512, 3, 3)  37748736  1.248265e+16    9.402646e+31  2.945026e+22    1.248265e+16        9.402646e+31   2.945026e+22      True
17  layers.11.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
18  layers.12.conv  weight  0.007958  2359296  (512, 512, 3, 3)  37748736  1.248265e+16    9.415211e+31  2.945027e+22    1.248265e+16        9.415211e+31   2.945027e+22      True
19  layers.12.conv    bias  1.000000      512            (512,)      8192  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
20  layers.14.conv  weight  0.029585  2359296  (512, 512, 3, 3)   9437184  1.248265e+16             inf  2.945026e+22    1.248265e+16                 inf   2.945026e+22      True
21  layers.14.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
22  layers.15.conv  weight  0.029672  2359296  (512, 512, 3, 3)   9437184  1.248265e+16             inf  2.945026e+22    1.248265e+16                 inf   2.945026e+22      True
23  layers.15.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
24  layers.16.conv  weight  0.033553  2359296  (512, 512, 3, 3)   9437184  1.248265e+16             inf  2.945028e+22    1.248265e+16                 inf   2.945028e+22      True
25  layers.16.conv    bias  1.000000      512            (512,)      2048  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
26              fc  weight  0.994922     5120         (10, 512)      5120  5.752007e+18             inf  2.945027e+22    5.752007e+18                 inf   2.945027e+22      True
27              fc    bias  1.000000       10             (10,)        10  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
Parameter Sparsity: 1475792/14719818 (0.1003)
FLOP Sparsity: 143301021/313478154 (0.4571)
Saving results.
